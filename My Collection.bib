@article{Diakogiannis2020,
abstract = {Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes. The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9{\%} over all classes for our best model.},
author = {Diakogiannis, Foivos I and Waldner, Fran{\c{c}}ois and Caccetta, Peter and Wu, Chen},
doi = {10.1016/j.isprsjprs.2020.01.013},
file = {::},
keywords = {Architecture,Convolutional neural network,Data augmentation,Loss function,Very high spatial resolution},
title = {{ISPRS Journal of Photogrammetry and Remote Sensing ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data}},
url = {https://doi.org/10.1016/j.isprsjprs.2020.01.013},
year = {2020}
}
@techreport{Xie,
abstract = {We present a simple self-training method that achieves 88.4{\%} top-1 accuracy on ImageNet, which is 2.0{\%} better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0{\%} to 83.7{\%}, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.},
archivePrefix = {arXiv},
arxivId = {1911.04252v2},
author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
eprint = {1911.04252v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Xie et al. - Unknown - Self-training with Noisy Student improves ImageNet classification.pdf:pdf},
title = {{Self-training with Noisy Student improves ImageNet classification}}
}
@techreport{Liu,
abstract = {Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification. In this paper, we study NAS for semantic image segmentation. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our architecture searched specifically for semantic image segmentation, attains state-of-the-art performance without any ImageNet pretraining. 1},
archivePrefix = {arXiv},
arxivId = {1901.02985v2},
author = {Liu, Chenxi and Chen, Liang-Chieh and Schroff, Florian and Adam, Hartwig and Hua, Wei and Yuille, Alan and Fei-Fei, Li},
eprint = {1901.02985v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - Unknown - Auto-DeepLab Hierarchical Neural Architecture Search for Semantic Image Segmentation.pdf:pdf},
title = {{Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation}},
url = {https://github.com/}
}
@article{Lv2020,
abstract = {The extraction and evaluation of crop production units are important foundations for agricultural production and management in modern smallholder regions, which are very significant to the regulation and sustainable development of agriculture. Crop areas have been recognized efficiently and accurately via remote sensing (RS) and machine learning (ML), especially deep learning (DL), which are too rough for modern smallholder production. In this paper, a delimitation-grading method for actual crop production units (ACPUs) based on RS images was explored using a combination of a mask region-based convolutional neural network (Mask R-CNN), spatial analysis, comprehensive index evaluation, and cluster analysis. Da'an City, Jilin province, China, was chosen as the study region to satisfy the agro-production demands in modern smallholder areas. Firstly, the ACPUs were interpreted from perspectives such as production mode, spatial form, and actual productivity. Secondly, cultivated land plots (C-plots) were extracted by Mask R-CNN with high-resolution RS images, which were used to delineate contiguous cultivated land plots (CC-plots) on the basis of auxiliary data correction. Then, the refined delimitation-grading results of the ACPUs were obtained through comprehensive evaluation of spatial characteristics and real productivity clustering. For the conclusion, the effectiveness of the Mask R-CNN model in C-plot recognition (loss = 0.16, mean average precision (mAP) = 82.29{\%}) and a reasonable distance threshold (20 m) for CC-plot delimiting were verified. The spatial features were evaluated with the scale-shape dimensions of nine specific indicators. Real productivities were clustered by the incorporation of two-step cluster and K-Means cluster. Furthermore, most of the ACPUs in the study area were of a reasonable scale and an appropriate shape, holding real productivities at a medium level or above. The proposed method in this paper can be adjusted according to the changes of the study area with flexibility to assist agro-supervision in many modern smallholder regions.},
annote = {Mask rcnn melhor que kmeans
},
author = {Lv, Yahui and Zhang, Chao and Yun, Wenju and Gao, Lulu and Wang, Huan and Ma, Jiani and Li, Hongju and Zhu, Dehai},
doi = {10.3390/rs12071074},
file = {::},
keywords = {Mask R-CNN,RS,crop production unit delineation-grading,instance segmentation,modern smallholder},
title = {{Remote Sens}},
url = {www.mdpi.com/journal/remotesensingArticle},
volume = {12},
year = {2020}
}
@article{OrtegaAdarme2020,
abstract = {Deforestation is one of the major threats to natural ecosystems. This process has a substantial contribution to climate change and biodiversity reduction. Therefore, the monitoring and early detection of deforestation is an essential process for preservation. Techniques based on satellite images are among the most attractive options for this application. However, many approaches involve some human intervention or are dependent on a manually selected threshold to identify regions that suffer deforestation. Motivated by this scenario, the present work evaluates Deep Learning-based strategies for automatic deforestation detection, namely, Early Fusion (EF), Siamese Network (SN), and Convolutional Support Vector Machine (CSVM) as well as Support Vector Machine (SVM), used as the baseline. The target areas are two regions with different deforestation patterns: the Amazon and Cerrado biomes in Brazil. The experiments used two co-registered Landsat 8 images acquired at different dates. The strategies based on Deep Learning achieved the best performance in our analysis in comparison with the baseline, with SN and EF superior to CSVM and SVM. In the same way, a reduction of the salt-and-pepper effect in the generated probabilistic change maps was noticed as the number of training samples increased. Finally, the work assesses how the methods can reduce the time invested in the visual inspection of deforested areas.},
annote = {Artigo ruim, no entanto {\'{e}} refer{\^{e}}ncia comparando com m{\'{e}}todos cl{\'{a}}ssicos.},
author = {{Ortega Adarme}, Mabel and {Queiroz Feitosa}, Raul and {Nigri Happ}, Patrick Nigri and {Aparecido De Almeida}, Claudio and {Rodrigues Gomes}, Alessandra},
doi = {10.3390/rs12060910},
file = {::},
journal = {Remote Sensing},
month = {mar},
number = {6},
pages = {910},
publisher = {MDPI AG},
title = {{Evaluation of Deep Learning Techniques for Deforestation Detection in the Brazilian Amazon and Cerrado Biomes From Remote Sensing Imagery}},
volume = {12},
year = {2020}
}
@misc{Zhu2017,
abstract = {Central to the looming paradigm shift toward data-intensive science, machine-learning techniques are becoming increasingly important. In particular, deep learning has proven to be both a major breakthrough and an extremely powerful tool in many fields. Shall we embrace deep learning as the key to everything? Or should we resist a black-box solution? These are controversial issues within the remote-sensing community. In this article, we analyze the challenges of using deep learning for remote-sensing data analysis, review recent advances, and provide resources we hope will make deep learning in remote sensing seem ridiculously simple. More importantly, we encourage remote-sensing scientists to bring their expertise into deep learning and use it as an implicit general model to tackle unprecedented, large-scale, influential challenges, such as climate change and urbanization.},
author = {Zhu, Xiao Xiang and Tuia, Devis and Mou, Lichao and Xia, Gui Song and Zhang, Liangpei and Xu, Feng and Fraundorfer, Friedrich},
booktitle = {IEEE Geoscience and Remote Sensing Magazine},
doi = {10.1109/MGRS.2017.2762307},
issn = {21686831},
month = {dec},
number = {4},
pages = {8--36},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources}},
volume = {5},
year = {2017}
}
@article{Ibtehaz2020,
abstract = {In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation. In this regard, U-Net has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, through extensive experimentations on some challenging datasets, we demonstrate that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modifications to improve upon the already state-of-the-art U-Net model. Following these modifications, we develop a novel architecture, MultiResUNet, as the potential successor to the U-Net architecture. We have tested and compared MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Although only slight improvements in the cases of ideal images are noticed, remarkable gains in performance have been attained for the challenging ones. We have evaluated our model on five different datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15{\%}, 5.07{\%}, 2.63{\%}, 1.41{\%}, and 0.62{\%} respectively. We have also discussed and highlighted some qualitatively superior aspects of MultiResUNet over classical U-Net that are not really reflected in the quantitative measures.},
author = {Ibtehaz, Nabil and Rahman, M Sohel},
doi = {10.1016/j.neunet.2019.08.025},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Ibtehaz, Rahman - 2020 - MultiResUNet Rethinking the U-Net architecture for multimodal biomedical image segmentation.pdf:pdf},
journal = {Neural Networks},
keywords = {Convolutional neural networks,Medical imaging,Net,Semantic segmentation U-},
pages = {74--87},
title = {{MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation}},
url = {https://doi.org/10.1016/j.neunet.2019.08.025},
volume = {121},
year = {2020}
}
@article{Heipke2020,
abstract = {During the last few years, artificial intelligence based on deep learning, and particularly based on convolutional neural networks, has acted as a game changer in just about all tasks related to photogrammetry and remote sensing. Results have shown partly significant improvements in many projects all across the photogrammetric processing chain from image orientation to surface reconstruction, scene classification as well as change detection, object extraction and object tracking and recognition in image sequences. This paper summarizes the foundations of deep learning for photogrammetry and remote sensing before illustrating, by way of example, different projects being carried out at the Institute of Photogrammetry and GeoInformation, Leibniz University Hannover, in this exciting and fast moving field of research and development.},
author = {Heipke, Christian and Rottensteiner, Franz},
doi = {10.1080/10095020.2020.1718003},
file = {:Users/philipeborba/Downloads/Deep learning for geometric and semantic tasks in photogrammetry and remote sensing (1).pdf:pdf},
issn = {10095020},
journal = {Geo-Spatial Information Science},
keywords = {Deep learning,convolutional neural networks(CNN),example project from IPI,machine learning},
number = {00},
pages = {1--10},
publisher = {Taylor {\&} Francis},
title = {{Deep learning for geometric and semantic tasks in photogrammetry and remote sensing}},
url = {https://doi.org/10.1080/10095020.2020.1718003},
volume = {00},
year = {2020}
}
@article{Timilsina2019,
abstract = {Abstract. Urban trees offer significant benefits for improving the sustainability and liveability of cities, but its monitoring is a major challenge for urban planners. Remote-sensing based technologies can effectively detect, monitor and quantify urban tree coverage as an alternative to field-based measurements. Automatic extraction of urban land cover features with high accuracy is a challenging task and it demands artificial intelligence workflows for efficiency and thematic quality. In this context, the objective of this research is to map urban tree coverage per cadastral parcel of Sandy Bay, Hobart from very high-resolution aerial orthophoto and LiDAR data using an Object Based Convolution Neural Network (CNN) approach. Instead of manual preparation of a large number of required training samples, automatically classified Object based image analysis (OBIA) output is used as an input samples to train CNN method. Also, CNN output is further refined and segmented using OBIA to assess the accuracy. The result shows 93.2{\%} overall accuracy for refined CNN classification. Similarly, the overlay of improved CNN output with cadastral parcel layer shows that 21.5{\%} of the study area is covered by trees. This research demonstrates that the accuracy of image classification can be improved by using a combination of OBIA and CNN methods. Such a combined method can be used where manual preparation of training samples for CNN is not preferred. Also, our results indicate that the technique can be implemented to calculate parcel level statistics for urban tree coverage that provides meaningful metrics to guide urban planning and land management practices.},
annote = {compara cnn com obia},
author = {Timilsina, S. and Sharma, S. K. and Aryal, J.},
doi = {10.5194/isprs-annals-iv-5-w2-111-2019},
file = {:Users/philipeborba/Downloads/isprs-annals-IV-5-W2-111-2019.pdf:pdf},
issn = {2194-9050},
journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {cadastral parcel,convolutional neural network,geobia,machine learning,urban trees},
number = {December},
pages = {111--117},
title = {{Mapping Urban Trees Within Cadastral Parcels Using an Object-Based Convolutional Neural Network}},
volume = {IV-5/W2},
year = {2019}
}
@article{Pan2020,
author = {Pan, Bin and Xu, Xia and Shi, Zhenwei and Zhang, Ning and Luo, Huanlin and Lan, Xianchao},
doi = {10.1109/lgrs.2019.2960528},
file = {:Users/philipeborba/Downloads/08950204.pdf:pdf},
issn = {1545-598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
pages = {1--5},
publisher = {IEEE},
title = {{DSSNet: A Simple Dilated Semantic Segmentation Network for Hyperspectral Imagery Classification}},
year = {2020}
}
@article{Saraiva2020,
abstract = {The availability of freshwater is becoming a global concern. Because agricultural consumption has been increasing steadily, the mapping of irrigated areas is key for supporting the monitoring of land use and better management of available water resources. In this paper, we propose a method to automatically detect and map center pivot irrigation systems using U-Net, an image segmentation convolutional neural network architecture, applied to a constellation of PlanetScope images from the Cerrado biome of Brazil. Our objective is to provide a fast and accurate alternative to map center pivot irrigation systems with very high spatial and temporal resolution imagery. We implemented a modified U-Net architecture using the TensorFlow library and trained it on the Google cloud platform with a dataset built from more than 42,000 very high spatial resolution PlanetScope images acquired between August 2017 and November 2018. The U-Net implementation achieved a precision of 99{\%} and a recall of 88{\%} to detect and map center pivot irrigation systems in our study area. This method, proposed to detect and map center pivot irrigation systems, has the potential to be scaled to larger areas and improve the monitoring of freshwater use by agricultural activities.},
annote = {artigo xer{\'{e}}u, tem overfit. D{\'{a}} pra publicar nessa revista},
author = {Saraiva, Marciano and Protas, {\'{E}}glen and Salgado, Mois{\'{e}}s and Souza, Carlos},
doi = {10.3390/rs12030558},
file = {:Users/philipeborba/Downloads/remotesensing-12-00558-v2.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Artificial intelligence,Neural networks,Semantic segmentation,Water management},
number = {3},
pages = {1--14},
title = {{Automatic mapping of center pivot irrigation systems from satellite images using deep learning}},
volume = {12},
year = {2020}
}
@article{Dong2020,
abstract = {Recently, convolutional neural networks (CNNs) showed excellent performance in many tasks, such as computer vision and remote sensing semantic segmentation. Especially, the ability to learn high-representation features of CNN draws much attention. And random forest (RF) algorithm, on the other hand, is widely applied for variables selection, classification, and regression. Based on the previous fusion models that fused CNN with the other models, such as conditional random fields (CRFs), support vector machine (SVM), and RF, this article tested a method based on the fusion of an RF classifier and the CNN for a very high resolution remote sensing (VHRRS) based forests mapping. The study area is located in the south of China and the main purpose was to precisely distinguish Lei bamboo forests from the other subtropical forests. The main novelties of this article are as follows. First, a test was conducted to confirm if a fusion of CNN and RF make an improvement in the VHRRS information extraction. Second, based on RF, variables with high importance were selected. Then, a test was again conducted to confirm if the learning from the selected variables will further give better results.},
annote = {cnn {\textgreater} random forests},
author = {Dong, Luofan and Xing, Luqi and Liu, Tengyan and Du, Huaqiang and Mao, Fangjie and Han, Ning and Li, Xuejian and Zhou, Guomo and Zhu, Di'en and Zheng, Junlong and Zhang, Meng},
doi = {10.1109/JSTARS.2019.2953234},
file = {:Users/philipeborba/Downloads/08935521.pdf:pdf},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Classification,convolutional neural networks (CNNs),random forest (RF),subtropical forest,very high resolution remote sensing (VHRRS)},
pages = {113--128},
title = {{Very High Resolution Remote Sensing Imagery Classification Using a Fusion of Random Forest and Deep Learning Technique-Subtropical Area for Example}},
volume = {13},
year = {2020}
}
@article{Lopez2020,
abstract = {This work aims at addressing two issues simultaneously: data compression at input space and semantic segmentation. Semantic segmentation of remotely sensed multi-or hyperspectral images through deep learning (DL) artificial neural networks (ANN) delivers as output the corresponding matrix of pixels classified elementwise, achieving competitive performance metrics. With technological progress, current remote sensing (RS) sensors have more spectral bands and higher spatial resolution than before, which means a greater number of pixels in the same area. Nevertheless, the more spectral bands and the greater number of pixels, the higher the computational complexity and the longer the processing times. Therefore, without dimensionality reduction, the classification task is challenging, particularly if large areas have to be processed. To solve this problem, our approach maps an RS-image or third-order tensor into a core tensor, representative of our input image, with the same spatial domain but with a lower number of new tensor bands using a Tucker decomposition (TKD). Then, a new input space with reduced dimensionality is built. To find the core tensor, the higher-order orthogonal iteration (HOOI) algorithm is used. A fully convolutional network (FCN) is employed afterwards to classify at the pixel domain, each core tensor. The whole framework, called here HOOI-FCN, achieves high performance metrics competitive with some RS-multispectral images (MSI) semantic segmentation state-of-the-art methods, while significantly reducing computational complexity, and thereby, processing time. We used a Sentinel-2 image data set from Central Europe as a case study, for which our framework outperformed other methods (included the FCN itself) with average pixel accuracy (PA) of 90{\%} (computational time--90s) and nine spectral bands, achieving a higher average PA of 91.97{\%} (computational time {\~{}}36.5s), and average PA of 91.56{\%} (computational time {\~{}}9.5s) for seven and five new tensor bands, respectively.},
annote = {cnn method that outperforms classical methods},
author = {L{\'{o}}pez, Josu{\'{e}} and Torres, Deni and Santos, Stewart and Atzberger, Clement},
doi = {10.3390/rs12030517},
file = {:Users/philipeborba/Downloads/remotesensing-12-00517.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Fully convolutional network,Semantic segmentation,Spectral image,Tensor decomposition},
number = {3},
pages = {1--21},
title = {{Spectral imagery tensor decomposition for semantic segmentation of remote sensing data through fully convolutional networks}},
volume = {12},
year = {2020}
}
@techreport{Sebastian,
abstract = {Automatic building extraction from aerial imagery has several applications in urban planning, disaster management, and change detection. In recent years, several works have adopted deep convolutional neural networks (CNNs) for building extraction , since they produce rich features that are invariant against lighting conditions, shadows, etc. Although several advances have been made, building extraction from aerial imagery still presents multiple challenges. Most of the deep learning segmentation methods optimize the per-pixel loss with respect to the ground truth without knowledge of the context. This often leads to imperfect outputs that may lead to missing or unrefined regions. In this work, we propose a novel loss function combining both adversarial and cross-entropy losses that learns to understand both local and global contexts for semantic segmentation. The newly proposed loss function deployed on the DeepLab v3+ network obtains state-of-the-art results on the Massachusetts buildings dataset. The loss function improves the structure and refines the edges of buildings without requiring any of the commonly used post-processing methods, such as Conditional Random Fields. We also perform ablation studies to understand the impact of the adversarial loss. Finally, the proposed method achieves a relaxed F1 score of 95.59{\%} on the Massachusetts buildings dataset compared to the previous best F1 of 94.88{\%}.},
archivePrefix = {arXiv},
arxivId = {2001.04269v2},
author = {Sebastian, Clint and Imbriaco, Raffaele and Bondarev, Egor and {De With}, Peter H N},
eprint = {2001.04269v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Sebastian et al. - Unknown - Adversarial Loss for Semantic Segmentation of Aerial Imagery.pdf:pdf},
keywords = {Index Terms-building segmentation,adversarial loss,aerial imagery},
title = {{Adversarial Loss for Semantic Segmentation of Aerial Imagery}}
}
@article{Jiang2020,
abstract = {Semantic segmentation methods based on deep learning considerably improve the segmentation performance of remote sensing images. However, with the extensive application of high-resolution remote sensing images, additional details introduce considerable interference to the learning process for classification, thereby diminishing the accuracy of segmentation and resulting in blurry object boundaries. To address this problem, this study designed Random-Walk-SegNet (RWSNet), a semantic segmentation network based on SegNet combined with random walk. First, SegNet is used as the basic architecture with the sliding window strategy that optimizes the network output to improve the continuity and smoothness of segmentation. Second, seed regions of the random walk are selected in accordance with the classification output of SegNet. Third, the weights of the undirected graph edge are determined by fusing the gradient of the original image and probability map of SegNet. Finally, random walk is implemented on the entire image, thus reducing edge blur and realizing high-performance semantic segmentation of remote sensing images. In comparison with mainstream and other improved methods, the proposed network has lower complexity but better performance, and the algorithm is state-of-the-art and robust.},
author = {Jiang, Jie and Lyu, Chengjin and Liu, Siying and He, Yongqiang and Hao, Xuetao},
doi = {10.1080/01431161.2019.1643937},
file = {:Users/philipeborba/Downloads/RWSNet a semantic segmentation network based on SegNet combined with random walk for remote sensing.pdf:pdf},
issn = {13665901},
journal = {International Journal of Remote Sensing},
number = {2},
pages = {487--505},
publisher = {Taylor {\&} Francis},
title = {{RWSNet: a semantic segmentation network based on SegNet combined with random walk for remote sensing}},
url = {https://doi.org/10.1080/01431161.2019.1643937},
volume = {41},
year = {2020}
}
@article{Hwang2020,
abstract = {We present a weakly supervised instance segmentation algorithm based on deep community learning with multiple tasks. This task is formulated as a combination of weakly supervised object detection and semantic segmentation, where individual objects of the same class are identified and segmented separately. We address this problem by designing a unified deep neural network architecture, which has a positive feedback loop of object detection with bounding box regression, instance mask generation, instance segmentation, and feature extraction. Each component of the network makes active interactions with others to improve accuracy, and the end-to-end trainability of our model makes our results more robust and reproducible. The proposed algorithm achieves state-of-the-art performance in the weakly supervised setting without any additional training such as Fast R-CNN and Mask R-CNN on the standard benchmark dataset.},
archivePrefix = {arXiv},
arxivId = {2001.11207},
author = {Hwang, Jaedong and Kim, Seohyun and Son, Jeany and Han, Bohyung},
eprint = {2001.11207},
file = {:Users/philipeborba/Downloads/2001.11207.pdf:pdf},
title = {{Weakly Supervised Instance Segmentation by Deep Community Learning}},
url = {http://arxiv.org/abs/2001.11207},
year = {2020}
}
@article{Prakash2020,
abstract = {Mapping landslides using automated methods is a challenging task, which is still largely done using human efforts. Today, the availability of high-resolution EO data products is increasing exponentially, and one of the targets is to exploit this data source for the rapid generation of landslide inventory. Conventional methods like pixel-based and object-based machine learning strategies have been studied extensively in the last decade. In addition, recent advances in CNN (convolutional neural network), a type of deep-learning method, has been widely successful in extracting information from images and have outperformed other conventional learning methods. In the last few years, there have been only a few attempts to adapt CNN for landslide mapping. In this study, we introduce a modified U-Net model for semantic segmentation of landslides at a regional scale from EO data using ResNet34 blocks for feature extraction. We also compare this with conventional pixel-based and object-based methods. The experiment was done in Douglas County, a study area selected in the south of Portland in Oregon, USA, and landslide inventory extracted from SLIDO (Statewide Landslide Information Database of Oregon) was considered as the ground truth. Landslide mapping is an imbalanced learning problem with very limited availability of training data. Our network was trained on a combination of focal Tversky loss and cross-entropy loss functions using augmented image tiles sampled from a selected training area. The deep-learning method was observed to have a better performance than the conventional methods with an MCC (Matthews correlation coefficient) score of 0.495 and a POD (probability of detection) rate of 0.72.},
author = {Prakash, Nikhil and Manconi, Andrea and Loew, Simon},
doi = {10.3390/rs12030346},
file = {:Users/philipeborba/Downloads/remotesensing-12-00346-v2.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {ConvolutionalNeuralNetworks (CNN),Deep learning,Dice coefficient,Landslide,Landslide mapping,Machine learning,Object-based,Pixel-based,Remote sensing,Resnet,Semantic segmentation,U-Net},
number = {3},
title = {{Mapping landslides on EO data: Performance of deep learning models vs. Traditional machine learning models}},
volume = {12},
year = {2020}
}
@article{Scepanovic2019,
abstract = {Land cover mapping is essential for monitoring the environment and understanding the effects of human activities on it. The automatic approaches to land cover mapping (i.e., image segmentation) mostly used traditional machine learning. On the natural images, deep learning has outperformed traditional machine learning on a range of tasks, including the image segmentation. On remote sensing images, recent studies are demonstrating successful application of specific deep learning models or their adaptations to particular small-scale mapping tasks (e.g., to classify wetland complexes). However, it is not readily clear which of the existing models for natural images are the best candidates to be taken for the particular remote sensing task and data. In this study, we answer that question for mapping the fundamental land cover classes using the satellite imaging radar data. We took ESA Sentinel-1 C-band SAR images available at no cost to users as representative data. CORINE land cover map was used as a reference, and the models were trained to distinguish between the 5 Level-1 CORINE classes. We selected seven among the state-of-the-art semantic segmentation models so that they cover a diverse set of approaches. We used 14 ESA Sentinel-1 scenes acquired during the summer season in Finland, which are representative of the land cover in the country. Upon the benchmarking, all the models demonstrated solid performance. The best model, FC-DenseNet (Fully Convolutional DenseNets), achieved the overall accuracy of 90.7{\%}. Overall, our results indicate that the semantic segmentation models are suitable for efficient wide-area mapping using satellite SAR imagery. Our results also provide baseline accuracy against which the newly proposed models should be evaluated and suggest the DenseNet-based models are the first candidate for this task.},
archivePrefix = {arXiv},
arxivId = {1912.05067},
author = {{\v{S}}{\'{c}}epanovi{\'{c}}, Sanja and Antropov, Oleg and Laurila, Pekka and Ignatenko, Vladimir and Praks, Jaan},
eprint = {1912.05067},
file = {:Users/philipeborba/Downloads/1912.05067.pdf:pdf},
title = {{Wide-Area Land Cover Mapping with Sentinel-1 Imagery using Deep Learning Semantic Segmentation Models}},
url = {http://arxiv.org/abs/1912.05067},
year = {2019}
}
@article{Reina2020,
abstract = {Convolutional neural network (CNN) models obtain state of the art performance on image classification, localization, and segmentation tasks. Limitations in computer hardware, most notably memory size in deep learning accelerator cards, prevent relatively large images, such as those from medical and satellite imaging, from being processed as a whole in their original resolution. A fully convolutional topology, such as U-Net, is typically trained on down-sampled images and inferred on images of their original size and resolution, by simply dividing the larger image into smaller (typically overlapping) tiles, making predictions on these tiles, and stitching them back together as the prediction for the whole image. In this study, we show that this tiling technique combined with translationally-invariant nature of CNNs causes small, but relevant differences during inference that can be detrimental in the performance of the model. Here we quantify these variations in both medical (i.e., BraTS) and non-medical (i.e., satellite) images and show that training a 2D U-Net model on the whole image substantially improves the overall model performance. Finally, we compare 2D and 3D semantic segmentation models to show that providing CNN models with a wider context of the image in all three dimensions leads to more accurate and consistent predictions. Our results suggest that tiling the input to CNN models—while perhaps necessary to overcome the memory limitations in computer hardware—may lead to undesirable and unpredictable errors in the model's output that can only be adequately mitigated by increasing the input of the model to the largest possible tile size.},
author = {Reina, G. Anthony and Panchumarthy, Ravi and Thakur, Siddhesh Pravin and Bastidas, Alexei and Bakas, Spyridon},
doi = {10.3389/fnins.2020.00065},
file = {:Users/philipeborba/Downloads/fnins-14-00065.pdf:pdf},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {BraTS,CNN,brain tumor,deep learning,glioma,satellite imaging,segmentation,tiling},
number = {February},
pages = {1--14},
title = {{Systematic Evaluation of Image Tiling Adverse Effects on Deep Learning Semantic Segmentation}},
volume = {14},
year = {2020}
}
@article{Dong2020a,
abstract = {Despite significant successes achieved in knowledge discovery, traditional machine learning methods may fail to obtain satisfactory performances when dealing with complex data, such as imbalanced, high-dimensional, noisy data, etc. The reason behind is that it is difficult for these methods to capture multiple characteristics and underlying structure of data. In this context, it becomes an important topic in the data mining field that how to effectively construct an efficient knowledge discovery and mining model. Ensemble learning, as one research hot spot, aims to integrate data fusion, data modeling, and data mining into a unified framework. Specifically, ensemble learning firstly extracts a set of features with a variety of transformations. Based on these learned features, multiple learning algorithms are utilized to produce weak predictive results. Finally, ensemble learning fuses the informative knowledge from the above results obtained to achieve knowledge discovery and better predictive performance via voting schemes in an adaptive way. In this paper, we review the research progress of the mainstream approaches of ensemble learning and classify them based on different characteristics. In addition, we present challenges and possible research directions for each mainstream approach of ensemble learning, and we also give an extra introduction for the combination of ensemble learning with other machine learning hot spots such as deep learning, reinforcement learning, etc.},
author = {Dong, Xibin and Yu, Zhiwen and Cao, Wenming and Shi, Yifan and Ma, Qianli},
doi = {10.1007/s11704-019-8208-z},
file = {:Users/philipeborba/Downloads/Dong2020{\_}Article{\_}ASurveyOnEnsembleLearning.pdf:pdf},
issn = {20952236},
journal = {Frontiers of Computer Science},
keywords = {clustering ensemble,ensemble learning,semi-supervised clustering ensemble,semi-supervised ensemble classification,supervised ensemble classification},
number = {2},
pages = {241--258},
title = {{A survey on ensemble learning}},
volume = {14},
year = {2020}
}
@article{Ding2020,
author = {Ding, Lei and Zhang, Jing and Bruzzone, Lorenzo},
doi = {10.1109/tgrs.2020.2964675},
file = {:Users/philipeborba/Downloads/08970467.pdf:pdf},
issn = {0196-2892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
pages = {1--10},
title = {{Semantic Segmentation of Large-Size VHR Remote Sensing Images Using a Two-Stage Multiscale Training Architecture}},
volume = {1},
year = {2020}
}
@article{Wang2020,
abstract = {Extracting buildings automatically from high-resolution aerial images is a significant and fundamental task for various practical applications, such as land-use statistics and urban planning. Recently, various methods based on deep learning, especially the fully convolution networks, achieve impressive scores in this challenging semantic segmentation task. However, the lack of global contextual information and the careless upsampling method limit the further improvement of the performance for building extraction task. To simultaneously address these problems, we propose a novel network named Efficient Non-local Residual U-shape Network(ENRU-Net), which is composed of a well designed U-shape encoder-decoder structure and an improved non-local block named asymmetric pyramid non-local block (APNB). The encoder-decoder structure is adopted to extract and restore the feature maps carefully, and APNB could capture global contextual information by utilizing self-attention mechanism. We evaluate the proposed ENRU-Net and compare it with other state-of-the-art models on two widely-used public aerial building imagery datasets: the Massachusetts Buildings Dataset and the WHU Aerial Imagery Dataset. The experiments show that the accuracy of ENRU-Net on these datasets has remarkable improvement against previous state-of-the-art semantic segmentation models, including FCN-8s, U-Net, SegNet and Deeplab v3. The subsequent analysis also indicates that our ENRU-Net has advantages in efficiency for building extraction from high-resolution aerial images.},
author = {Wang, Shengsheng and Hou, Xiaowei and Zhao, Xin},
doi = {10.1109/ACCESS.2020.2964043},
file = {:Users/philipeborba/Downloads/08950134.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Deep learning,building extraction,fully convolution network,non-local method,semantic segmentation},
pages = {7313--7322},
publisher = {IEEE},
title = {{Automatic Building Extraction from High-Resolution Aerial Imagery via Fully Convolutional Encoder-Decoder Network with Non-Local Block}},
volume = {8},
year = {2020}
}
@article{Zhang2020,
abstract = {A good semantic segmentation method for visual scene understanding should consider both accuracy and efficiency. However, the existing networks tend to concentrate only on segmentation results but not on simplifying the network. As a result, a heavy network will be made and it is difficult to deploy such heavy network on some hardware with limited memory. To address this problem, we in this paper develop a novel architecture by involving the recursive block to reduce parameters and improve prediction, as recursive block can improve performance without introducing new parameters for additional convolutions. In detail, for the purpose of mitigating the difficulty of training recursive block, we have adopted a residual unit to give the data more choices to flow through and utilize concatenation layer to combine the output maps of the recursive convolution layers with same resolution but different field-of-views. As a result, richer semantic information can be included in the feature maps, which is good to achieve satisfying pixel-wise prediction. Meriting from the above strategy, we also extend it to enhance Mask-RCNN for instance segmentation. Extensive simulations based on different benchmark datasets, such as DeepFashion, Cityscapes and PASCAL VOC 2012, show that our method can improve segmentation results as well as reduce the parameters.},
author = {Zhang, Yue and Li, Xianrui and Lin, Mingquan and Chiu, Bernard and Zhao, Mingbo},
doi = {10.1007/s00521-020-04738-5},
file = {:Users/philipeborba/Downloads/Zhang2020{\_}Article{\_}Deep-recursiveResidualNetworkF.pdf:pdf},
isbn = {0052102004738},
issn = {14333058},
journal = {Neural Computing and Applications},
keywords = {Convolutional neural networks,Deep learning,Recursive residual network,Semantic segmentation},
publisher = {Springer London},
title = {{Deep-recursive residual network for image semantic segmentation}},
url = {https://doi.org/10.1007/s00521-020-04738-5},
volume = {5},
year = {2020}
}
@article{Minaee2020,
abstract = {Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.},
archivePrefix = {arXiv},
arxivId = {2001.05566},
author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
eprint = {2001.05566},
file = {:Users/philipeborba/Downloads/2001.05566.pdf:pdf},
pages = {1--23},
title = {{Image Segmentation Using Deep Learning: A Survey}},
url = {http://arxiv.org/abs/2001.05566},
year = {2020}
}
@article{Lin2020,
author = {Lin, Jinzhao and Wu, Jun and Zhang, Hui},
file = {:Users/philipeborba/Downloads/remotesensing-12-00762-v2.pdf:pdf},
keywords = {building detection,faster r-cnn,improved algorithm,remote sensing images},
number = {i},
title = {{An Optimized Faster R-CNN Method Based on DRNet and RoI Align for Building Detection in Remote Sensing Images}},
year = {2020}
}
@article{Hu2020a,
abstract = {Deep learning methods for semantic image segmentation can effectively extract geographical features from very high-resolution (VHR) remote sensing images. However, these methods experience over-segmentation in low-level features and a breakdown in the integrity of objects with fixed patch sizes due to the multi-scaled geographical features. In this study, a dual attention mechanism is introduced and embedded into densely connected convolutional networks (DenseNets) to form a dense-global-entropy network (DGEN) for the semantic segmentation of VHR remote sensing images. In the DGEN architecture, a global attention enhancement module is developed for context acquisition, and a local attention fusion module is designed for detail selection. This network presents the improved semantic segmentation performance of test ISPRS 2D datasets. The experimental results indicate an improvement in the overall accuracy (OA), F1, kappa coefficient and mean intersection over union (MIoU). Compared with the DeeplabV3+ and SegNet models, the OA improves by 2.79{\%} and 1.19{\%}; the mean F1 improves by 3.43{\%} and 0.88{\%}; the kappa coefficient improves by 4.04{\%} and 1.82{\%}; and the MIoU improves by 5.22{\%} and 1.47{\%}, respectively. The experiments showed that the dual attention mechanism presented in this study can improve segmentation and maintain object integrity during the encoding-decoding process.},
annote = {ler},
author = {Hu, Huanjun and Li, Zheng and Li, Lin and Yang, Hui and Zhu, Haihong},
doi = {10.1109/ACCESS.2020.2964760},
file = {:Users/philipeborba/Downloads/08952711.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Attention mechanism,DenseNet,semantic segmentation,very high-resolution remote sensing images},
pages = {14606--14619},
title = {{Classification of Very High-Resolution Remote Sensing Imagery Using a Fully Convolutional Network with Global and Local Context Information Enhancements}},
volume = {8},
year = {2020}
}
@article{Akhtar2020,
abstract = {The convolutional neural network architecture has different components like convolution and pooling. The pooling is crucial component placed after the convolution layer. It plays a vital role in visual recognition, detection and segmentation course to overcome the concerns like overfitting, computation time and recognition accuracy. The elementary pooling process involves down sampling of feature map by piercing into subregions. This piercing and down sampling is defined by the pooling hyperparameters, viz. stride and filter size. This down sampling process discards the irrelevant information and picks the defined global feature. The generally used global feature selection methods are average and max pooling. These methods decline, when the main element has higher or lesser intensity than the nonsignificant element. It also suffers with locus and order of nominated global feature, hence not suitable for every situation. The pooling variants are proposed by numerous researchers to overcome concern. This article presents the state of the art on selection of global feature for pooling process mainly based on four categories such as value, probability, rank and transformed domain. The value and probability-based methods use the criteria such as the way of down sampling, size of kernel, input output feature map, location of pooling, number stages and random selection based on probability value. The rank-based methods assign the rank and weight to activation; the feature is selected based on the defined criteria. The transformed domain pooling methods transform the image to other domains such as wavelet, frequency for pooling the feature.},
author = {Akhtar, Nadeem and Ragavendran, U.},
doi = {10.1007/s00521-019-04296-5},
file = {:Users/philipeborba/Downloads/Akhtar-Ragavendran2020{\_}Article{\_}InterpretationOfIntelligenceIn.pdf:pdf},
isbn = {0123456789},
issn = {14333058},
journal = {Neural Computing and Applications},
keywords = {Deep learning methodology,Hybrid machine learning tools,Pooling intelligence,Self-learning artificial neural network,Stable sampling,Supervised algorithms},
number = {3},
pages = {879--898},
publisher = {Springer London},
title = {{Interpretation of intelligence in CNN-pooling processes: a methodological survey}},
url = {https://doi.org/10.1007/s00521-019-04296-5},
volume = {32},
year = {2020}
}
@article{Cira2020,
abstract = {{\textless}p{\textgreater}Remote sensing imagery combined with deep learning strategies is often regarded as an ideal solution for interpreting scenes and monitoring infrastructures with remarkable performance levels. In addition, the road network plays an important part in transportation, and currently one of the main related challenges is detecting and monitoring the occurring changes in order to update the existent cartography. This task is challenging due to the nature of the object (continuous and often with no clearly defined borders) and the nature of remotely sensed images (noise, obstructions). In this paper, we propose a novel framework based on convolutional neural networks (CNNs) to classify secondary roads in high-resolution aerial orthoimages divided in tiles of 256 × 256 pixels. We will evaluate the framework's performance on unseen test data and compare the results with those obtained by other popular CNNs trained from scratch.{\textless}/p{\textgreater}},
author = {Cira, Calimanut-Ionut and Alcarria, Ramon and Manso-Callejo, Miguel-{\'{A}}ngel and Serradilla, Francisco},
doi = {10.3390/rs12050765},
file = {:Users/philipeborba/Downloads/remotesensing-12-00765-v2.pdf:pdf},
issn = {2072-4292},
journal = {Remote Sensing},
keywords = {convolutional neural networks,deep learning,image analysis,remote sensing,road classification,secondary transport routes},
number = {5},
pages = {765},
title = {{A Framework Based on Nesting of Convolutional Neural Networks to Classify Secondary Roads in High Resolution Aerial Orthoimages}},
url = {https://www.mdpi.com/2072-4292/12/5/765},
volume = {12},
year = {2020}
}
@article{Long2017,
abstract = {In this paper, we focus on tackling the problem of automatic accurate localization of detected objects in high-resolution remote sensing images. The two major problems for object localization in remote sensing images caused by the complex context information such images contain are achieving generalizability of the features used to describe objects and achieving accurate object locations. To address these challenges, we propose a new object localization framework, which can be divided into three processes: region proposal, classification, and accurate object localization process. First, a region proposal method is used to generate candidate regions with the aim of detecting all objects of interest within these images. Then, generic image features from a local image corresponding to each region proposal are extracted by a combination model of 2-D reduction convolutional neural networks (CNNs). Finally, to improve the location accuracy, we propose an unsupervised score-based bounding box regression (USB-BBR) algorithm, combined with a nonmaximum suppression algorithm to optimize the bounding boxes of regions that detected as objects. Experiments show that the dimension-reduction model performs better than the retrained and fine-tuned models and the detection precision of the combined CNN model is much higher than that of any single model. Also our proposed USB-BBR algorithm can more accurately locate objects within an image. Compared with traditional features extraction methods, such as elliptic Fourier transform-based histogram of oriented gradients and local binary pattern histogram Fourier, our proposed localization framework shows robustness when dealing with different complex backgrounds.},
author = {Long, Yang and Gong, Yiping and Xiao, Zhifeng and Liu, Qing},
doi = {10.1109/TGRS.2016.2645610},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Long et al. - 2017 - Accurate object localization in remote sensing images based on convolutional neural networks.pdf:pdf},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Convolutional neural network (CNN),object localization,remote sensing images,unsupervised score-based bounding box regression (USB-BBR)},
month = {may},
number = {5},
pages = {2486--2498},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Accurate object localization in remote sensing images based on convolutional neural networks}},
volume = {55},
year = {2017}
}
@article{Tayara2018,
abstract = {Object detection in very high-resolution (VHR) aerial images is an essential step for a wide range of applications such as military applications, urban planning, and environmental management. Still, it is a challenging task due to the different scales and appearances of the objects. On the other hand, object detection task in VHR aerial images has improved remarkably in recent years due to the achieved advances in convolution neural networks (CNN). Most of the proposed methods depend on a two-stage approach, namely: a region proposal stage and a classification stage such as Faster R-CNN. Even though two-stage approaches outperform the traditional methods, their optimization is not easy and they are not suitable for real-time applications. In this paper, a uniform one-stage model for object detection in VHR aerial images has been proposed. In order to tackle the challenge of different scales, a densely connected feature pyramid network has been proposed by which high-level multi-scale semantic feature maps with high-quality information are prepared for object detection. This work has been evaluated on two publicly available datasets and outperformed the current state-of-the-art results on both in terms of mean average precision (mAP) and computation time.},
author = {Tayara, Hilal and Chong, Kil To},
doi = {10.3390/s18103341},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Tayara, Chong - 2018 - Object detection in very high-resolution aerial images using one-stage densely connected feature pyramid network.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Aerial images,Convolution neural network (cnn),Deep learning,Feature pyramid network,Focal loss,Object detection},
month = {oct},
number = {10},
publisher = {MDPI AG},
title = {{Object detection in very high-resolution aerial images using one-stage densely connected feature pyramid network}},
volume = {18},
year = {2018}
}
@article{Chen2019a,
abstract = {Object detection has attracted increasing attention in the field of remote sensing image analysis. Complex backgrounds, vertical views, and variations in target kind and size in remote sensing images make object detection a challenging task. In this work, considering that the types of objects are often closely related to the scene in which they are located, we propose a convolutional neural network (CNN) by combining scene-contextual information for object detection. Specifically, we put forward the scene-contextual feature pyramid network (SCFPN), which aims to strengthen the relationship between the target and the scene and solve problems resulting from variations in target size. Additionally, to improve the capability of feature extraction, the network is constructed by repeating a building aggregated residual block. This block increases the receptive field, which can extract richer information for targets and achieve excellent performance with respect to small object detection. Moreover, to improve the proposed model performance, we use group normalization, which divides the channels into groups and computes the mean and variance for normalization within each group, to solve the limitation of the batch normalization. The proposed method is validated on a public and challenging dataset. The experimental results demonstrate that our proposed method outperforms other state-of-the-art object detection models.},
author = {Chen, Chaoyue and Gong, Weiguo and Chen, Yongliang and Li, Weihong},
doi = {10.3390/rs11030339},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2019 - Object detection in remote sensing images based on a scene-contextual feature pyramid network.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Convolutional neural network (CNN),Object detection,Remote sensing images,Scenecontextual feature pyramid network (SCFPN)},
month = {feb},
number = {3},
publisher = {MDPI AG},
title = {{Object detection in remote sensing images based on a scene-contextual feature pyramid network}},
volume = {11},
year = {2019}
}
@article{Zhang2019,
abstract = {Abstract: How to efficiently utilize vast amounts of easily accessed aerial imageries is a critical challenge for researchers with the proliferation of high-resolution remote sensing sensors and platforms. Recently, the rapid development of deep neural networks (DNN) has been a focus in remote sensing, and the networks have achieved remarkable progress in image classification and segmentation tasks. However, the current DNN models inevitably lose the local cues during the downsampling operation. Additionally, even with skip connections, the upsampling methods cannot properly recover the structural information, such as the edge intersections, parallelism, and symmetry. In this paper, we propose the Web-Net, which is a nested network architecture with hierarchical dense connections, to handle these issues. We design the Ultra-Hierarchical Sampling (UHS) block to absorb and fuse the inter-level feature maps to propagate the feature maps among different levels. The position-wise downsampling/upsampling methods in the UHS iteratively change the shape of the inputs while preserving the number of their parameters, so that the low-level local cues and high-level semantic cues are properly preserved. We verify the effectiveness of the proposedWeb-Net in the Inria Aerial Dataset and WHU Dataset. The results of the proposedWeb-Net achieve an overall accuracy of 96.97{\%} and an IoU (Intersection over Union) of 80.10{\%} on the Inria Aerial Dataset, which surpasses the state-of-the-art SegNet 1.8{\%} and 9.96{\%}, respectively; the results on the WHU Dataset also support the effectiveness of the proposedWeb-Net. Additionally, benefitting from the nested network architecture and the UHS block, the extracted buildings on the prediction maps are obviously sharper and more accurately identified, and even the building areas that are covered by shadows can also be correctly extracted. The verified results indicate that the proposedWeb-Net is both effective and efficient for building extraction from high-resolution remote sensing images.},
author = {Zhang, Yan and Gong, Weiguo and Sun, Jingxi and Li, Weihong},
doi = {10.3390/rs11161897},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2019 - Web-Net A novel nest networks with ultra-hierarchical sampling for building extraction from aerial imageries.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Building extraction,Deep learning,Remote sensing,Ultra-hierarchical sampling,Web-net},
number = {16},
publisher = {MDPI AG},
title = {{Web-Net: A novel nest networks with ultra-hierarchical sampling for building extraction from aerial imageries}},
volume = {11},
year = {2019}
}
@article{Yan2019a,
abstract = {Recently, methods based on Faster region-based convolutional neural network (R-CNN) have been popular in multi-class object detection in remote sensing images due to their outstanding detection performance. The methods generally propose candidate region of interests (ROIs) through a region propose network (RPN), and the regions with high enough intersection-over-union (IoU) values against ground truth are treated as positive samples for training. In this paper, we find that the detection result of such methods is sensitive to the adaption of different IoU thresholds. Specially, detection performance of small objects is poor when choosing a normal higher threshold, while a lower threshold will result in poor location accuracy caused by a large quantity of false positives. To address the above issues, we propose a novel IoU-Adaptive Deformable R-CNN framework for multi-class object detection. Specially, by analyzing the different roles that IoU can play in different parts of the network, we propose an IoU-guided detection framework to reduce the loss of small object information during training. Besides, the IoU-based weighted loss is designed, which can learn the IoU information of positive ROIs to improve the detection accuracy effectively. Finally, the class aspect ratio constrained non-maximum suppression (CARC-NMS) is proposed, which further improves the precision of the results. Extensive experiments validate the effectiveness of our approach and we achieve state-of-the-art detection performance on the DOTA dataset.},
author = {Yan, Jiangqiao and Wang, Hongqi and Yan, Menglong and Diao, Wenhui and Sun, Xian and Li, Hao},
doi = {10.3390/rs11030286},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Yan et al. - 2019 - IoU-adaptive deformable R-CNN Make full use of IoU for multi-class object detection in remote sensing imagery.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Anchor matching,Cascade R-CNN,IoU-based weighted loss,Non-maximum suppression,Remote sensing imagery},
month = {feb},
number = {3},
publisher = {MDPI AG},
title = {{IoU-adaptive deformable R-CNN: Make full use of IoU for multi-class object detection in remote sensing imagery}},
volume = {11},
year = {2019}
}
@article{Li2018,
abstract = {Recent CNN based object detectors, no matter one-stage methods like YOLO, SSD, and RetinaNe or two-stage detectors like Faster R-CNN, R-FCN and FPN are usually trying to directly finetune from ImageNet pre-trained models designed for image classification. There has been little work discussing on the backbone feature extractor specifically designed for the object detection. More importantly, there are several differences between the tasks of image classification and object detection. 1. Recent object detectors like FPN and RetinaNet usually involve extra stages against the task of image classification to handle the objects with various scales. 2. Object detection not only needs to recognize the category of the object instances but also spatially locate the position. Large downsampling factor brings large valid receptive field, which is good for image classification but compromises the object location ability. Due to the gap between the image classification and object detection, we propose DetNet in this paper, which is a novel backbone network specifically designed for object detection. Moreover, DetNet includes the extra stages against traditional backbone network for image classification, while maintains high spatial resolution in deeper layers. Without any bells and whistles, state-of-the-art results have been obtained for both object detection and instance segmentation on the MSCOCO benchmark based on our DetNet{\~{}}(4.8G FLOPs) backbone. The code will be released for the reproduction.},
archivePrefix = {arXiv},
arxivId = {1804.06215},
author = {Li, Zeming and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Deng, Yangdong and Sun, Jian},
eprint = {1804.06215},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2018 - DetNet A Backbone network for Object Detection.pdf:pdf},
month = {apr},
title = {{DetNet: A Backbone network for Object Detection}},
url = {http://arxiv.org/abs/1804.06215},
year = {2018}
}
@article{Guo2020,
abstract = {{\textless}p{\textgreater}Multi-scale object detection is a basic challenge in computer vision. Although many advanced methods based on convolutional neural networks have succeeded in natural images, the progress in aerial images has been relatively slow mainly due to the considerably huge scale variations of objects and many densely distributed small objects. In this paper, considering that the semantic information of the small objects may be weakened or even disappear in the deeper layers of neural network, we propose a new detection framework called Extended Feature Pyramid Network (EFPN) for strengthening the information extraction ability of the neural network. In the EFPN, we first design the multi-branched dilated bottleneck (MBDB) module in the lateral connections to capture much more semantic information. Then, we further devise an attention pathway for better locating the objects. Finally, an augmented bottom-up pathway is conducted for making shallow layer information easier to spread and further improving performance. Moreover, we present an adaptive scale training strategy to enable the network to better recognize multi-scale objects. Meanwhile, we present a novel clustering method to achieve adaptive anchors and make the neural network better learn data features. Experiments on the public aerial datasets indicate that the presented method obtain state-of-the-art performance.{\textless}/p{\textgreater}},
author = {Guo, Wei and Li, Weihong and Gong, Weiguo and Cui, Jinkai},
doi = {10.3390/rs12050784},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - 2020 - Extended Feature Pyramid Network with Adaptive Scale Training Strategy and Anchors for Object Detection in Aerial(2).pdf:pdf},
issn = {2072-4292},
journal = {Remote Sensing},
keywords = {adaptive anchors,adaptive scale training strategy,aerial images,extended feature pyramid network (EFPN),object detection},
month = {mar},
number = {5},
pages = {784},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Extended Feature Pyramid Network with Adaptive Scale Training Strategy and Anchors for Object Detection in Aerial Images}},
url = {https://www.mdpi.com/2072-4292/12/5/784},
volume = {12},
year = {2020}
}
@article{Qiu2019,
abstract = {Object detection is a significant and challenging problem in the study area of remote sensing and image analysis. However, most existing methods are easy to miss or incorrectly locate objects due to the various sizes and aspect ratios of objects. In this paper, we propose a novel end-to-end Adaptively Aspect Ratio Multi-Scale Network (A2RMNet) to solve this problem. On the one hand, we design a multi-scale feature gate fusion network to adaptively integrate the multi-scale features of objects. This network is composed of gate fusion modules, refine blocks and region proposal networks. On the other hand, an aspect ratio attention network is leveraged to preserve the aspect ratios of objects, which alleviates the excessive shape distortions of objects caused by aspect ratio changes during training. Experiments show that the proposed A2RMNet significantly outperforms the previous state of the arts on the DOTA dataset, NWPU VHR-10 dataset, RSOD dataset and UCAS-AOD dataset by 5.73{\%}, 7.06{\%}, 3.27{\%} and 2.24{\%}, respectively.},
author = {Qiu, Heqian and Li, Hongliang and Wu, Qingbo and Meng, Fanman and Ngan, King Ngi and Shi, Hengcan},
doi = {10.3390/rs11131594},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Qiu et al. - 2019 - A2RMNet Adaptively Aspect Ratio Multi-Scale Network for object detection in remote sensing images.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {A multi-scale feature gate fusion network,Adaptively Aspect Ratio Multi-Scale Network (A2RMNet),An aspect ratio attention network,Object detection,Remote sensing and image analysis},
month = {jul},
number = {13},
publisher = {MDPI AG},
title = {{A2RMNet: Adaptively Aspect Ratio Multi-Scale Network for object detection in remote sensing images}},
volume = {11},
year = {2019}
}
@article{Wang2020a,
abstract = {{\textless}p{\textgreater}The accurate acquisition of water information from remote sensing images has become important in water resources monitoring and protections, and flooding disaster assessment. However, there are significant limitations in the traditionally used index for water body identification. In this study, we have proposed a deep convolutional neural network (CNN), based on the multidimensional densely connected convolutional neural network (DenseNet), for identifying water in the Poyang Lake area. The results from DenseNet were compared with the classical convolutional neural networks (CNNs): ResNet, VGG, SegNet and DeepLab v3+, and also compared with the Normalized Difference Water Index (NDWI). Results have indicated that CNNs are superior to the water index method. Among the five CNNs, the proposed DenseNet requires the shortest training time for model convergence, besides DeepLab v3+. The identification accuracies are evaluated through several error metrics. It is shown that the DenseNet performs much better than the other CNNs and the NDWI method considering the precision of identification results; among those, the NDWI performance is by far the poorest. It is suggested that the DenseNet is much better in distinguishing water from clouds and mountain shadows than other CNNs.{\textless}/p{\textgreater}},
author = {Wang, Guojie and Wu, Mengjuan and Wei, Xikun and Song, Huihui},
doi = {10.3390/rs12050795},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2020 - Water Identification from High-Resolution Remote Sensing Images Based on Multidimensional Densely Connected Convolu.pdf:pdf},
issn = {2072-4292},
journal = {Remote Sensing},
keywords = {convolutional neural network,water identification,water index},
month = {mar},
number = {5},
pages = {795},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Water Identification from High-Resolution Remote Sensing Images Based on Multidimensional Densely Connected Convolutional Neural Networks}},
url = {https://www.mdpi.com/2072-4292/12/5/795},
volume = {12},
year = {2020}
}
@article{Li2020,
author = {Li, Kun and Hu, Xiangyun and Jiang, Huiwei and Shu, Zhen and Zhang, Mi},
file = {:Users/philipeborba/Downloads/remotesensing-12-00789-v2.pdf:pdf},
keywords = {attention guidance,deep learning,high-resolution satellite imagery,interactive extraction,multi-scale,segmentation network},
title = {{Attention-Guided Multi-Scale Segmentation Neural Network for Interactive Extraction of Region Objects from High-Resolution Satellite Imagery}},
year = {2020}
}
@article{Liua,
author = {Liu, Peng and Wei, Yongming and Wang, Qinjun and Chen, Yu},
file = {:Users/philipeborba/Downloads/remotesensing-12-00894-v3.pdf:pdf},
pages = {1--13},
title = {{Research on Post-Earthquake Landslide Extraction Algorithm Based on Improved U-Net Model}}
}
@article{Oude2020,
author = {Oude, Sander and Yang, Zhishuang},
doi = {10.3390/rs12050877},
file = {:Users/philipeborba/Downloads/remotesensing-12-00877-v2.pdf:pdf},
keywords = {als point clouds,automatic training samples generation,convolutional neural network,graph,unsupervised segmentation},
pages = {1--18},
title = {{Using Training Samples Retrieved From a Topographic Map for the Classification of Airborne Laser Scanner Data}},
year = {2020}
}
@article{Wang2020b,
author = {Wang, Guojie and Wu, Mengjuan and Wei, Xikun and Song, Huihui},
file = {:Users/philipeborba/Downloads/remotesensing-12-00795.pdf:pdf},
keywords = {convolutional neural network,water identification,water index},
title = {{Water Identification from High-Resolution Remote Sensing Images Based on Multidimensional Densely Connected Convolutional Neural Networks}},
year = {2020}
}
@article{Shang2020,
author = {Shang, Ronghua and Zhang, Jiyu and Jiao, Licheng and Li, Yangyang and Marturi, Naresh},
doi = {10.3390/rs12050872},
file = {:Users/philipeborba/Downloads/remotesensing-12-00872-v2.pdf:pdf},
keywords = {adaptive fusion,cnn,multi-scale context,remote sensing image,semantic segmentation},
pages = {1--20},
title = {{Multi-scale Adaptive Feature Fusion Network for Semantic Segmentation in Remote Sensing Images}},
year = {2020}
}
@article{Liub,
author = {Liu, Sicong and Hu, Qing and Tong, Xiaohua and Xia, Junshi and Du, Qian and Samat, Alim},
file = {:Users/philipeborba/Downloads/remotesensing-12-00862-v2.pdf:pdf},
keywords = {classification,feature selection,gf,guided filter,multi-scale features,superpixel segmentation,vhr remote sensing images},
title = {{A Multi-Scale Superpixel-Guided Filter Feature Extraction and Selection Approach for Classification of Very-High-Resolution Remotely Sensed Imagery}}
}
@article{Baur2020,
abstract = {Recent advances in unmanned-aerial-vehicle- (UAV-) based remote sensing utilizing lightweight multispectral and thermal infrared sensors allow for rapid wide-area landmine contamination detection and mapping surveys. We present results of a study focused on developing and testing an automated technique of remote landmine detection and identification of scatterable antipersonnel landmines in wide-area surveys. Our methodology is calibrated for the detection of scatterable plastic landmines which utilize a liquid explosive encapsulated in a polyethylene or plastic body in their design. We base our findings on analysis of multispectral and thermal datasets collected by an automated UAV-survey system featuring scattered PFM-1-type landmines as test objects and present results of an effort to automate landmine detection, relying on supervised learning algorithms using a Faster Regional-Convolutional Neural Network (Faster R-CNN). The RGB visible light Faster R-CNN demo yielded a 99.3{\%} testing accuracy for a partially withheld testing set and 71.5{\%} testing accuracy for a completely withheld testing set. Across multiple test environments, using centimeter scale accurate georeferenced datasets paired with Faster R-CNN, allowed for accurate automated detection of test PFM-1 landmines. This method can be calibrated to other types of scatterable antipersonnel mines in future trials to aid humanitarian demining initiatives. With millions of remnant PFM-1 and similar scatterable plastic mines across post-conflict regions and considerable stockpiles of these landmines posing long-term humanitarian and economic threats to impacted communities, our methodology could considerably aid in efforts to demine impacted regions.},
author = {Baur, Baur and Steinberg, Steinberg and Nikulin, Nikulin and Chiu, Chiu and de Smet},
doi = {10.3390/rs12050859},
file = {:Users/philipeborba/Downloads/remotesensing-12-00859-v2.pdf:pdf},
journal = {Remote Sensing},
keywords = {cnn,landmines,neural networks,uav,uxo},
pages = {1--16},
title = {{Applying Deep Learning to Automate UAV-Based Detection of Scatterable Landmines}},
url = {https://www.mdpi.com/2072-4292/12/5/859?utm{\_}source=researcher{\_}app{\&}utm{\_}medium=referral{\&}utm{\_}campaign=RESR{\_}MRKT{\_}Researcher{\_}inbound},
year = {2020}
}
@article{Song2020,
abstract = {{\textless}p{\textgreater}Remote sensing images having high spatial resolution are acquired, and large amounts of data are extracted from their region of interest. For processing these images, objects of various sizes, from very small neighborhoods to large regions composed of thousands of pixels, should be considered. To this end, this study proposes change detection method using transfer learning and recurrent fully convolutional networks with multiscale three-dimensional (3D) filters. The initial convolutional layer of the change detection network with multiscale 3D filters was designed to extract spatial and spectral features of materials having different sizes; the layer exploits pre-trained weights and biases of semantic segmentation network trained on an open benchmark dataset. The 3D filter sizes were defined in a specialized way to extract spatial and spectral information, and the optimal size of the filter was determined using highly accurate semantic segmentation results. To demonstrate the effectiveness of the proposed method, binary change detection was performed on images obtained from multi-temporal Korea multipurpose satellite-3A. Results revealed that the proposed method outperformed the traditional deep learning-based change detection methods and the change detection accuracy improved using multiscale 3D filters and transfer learning.{\textless}/p{\textgreater}},
author = {Song, Ahram and Choi, Jaewan},
doi = {10.3390/rs12050799},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Song, Choi - 2020 - Fully Convolutional Networks with Multiscale 3D Filters and Transfer Learning for Change Detection in High Spatial R.pdf:pdf},
issn = {2072-4292},
journal = {Remote Sensing},
keywords = {change detection,convolutional long short-term memory,fully convolutional network,high spatial,multiscale three-dimensional filters,resolution satellite image,transfer learning},
month = {mar},
number = {5},
pages = {799},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Fully Convolutional Networks with Multiscale 3D Filters and Transfer Learning for Change Detection in High Spatial Resolution Satellite Images}},
url = {https://www.mdpi.com/2072-4292/12/5/799},
volume = {12},
year = {2020}
}
@article{Su2019a,
abstract = {In object-based image analysis (OBIA), it is often difficult to select the most useful features from a large number of segment-based information. The problem of choosing superpixel-based features is also very challenging. In order to solve this issue, this paper proposes a principal component analysis (PCA)-based method for superpixel-based classification of high resolution remote sensing imagery. This technique transforms the spectral features of superpixels, and the resulted feature variables are used to train a support vector machine classifier. Experiments based on 4 high resolution multispectral images indicated that although the performance is sensitive to the two parameters, the proposed method can increase classification accuracy effectively.},
author = {Su, Tengfei},
doi = {10.1007/s11042-019-08224-6},
file = {:Users/philipeborba/Downloads/Su2019{\_}Article{\_}Superpixel-basedPrincipalCompo.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Feature transform,Image classification,Principal component analysis,Superpixel},
number = {23},
pages = {34173--34191},
publisher = {Multimedia Tools and Applications},
title = {{Superpixel-based principal component analysis for high resolution remote sensing image classification}},
volume = {78},
year = {2019}
}
@article{Guo2018,
abstract = {During the long history of computer vision, one of the grand challenges has been semantic segmentation which is the ability to segment an unknown image into different parts and objects (e.g., beach, ocean, sun, dog, swimmer). Furthermore, segmentation is even deeper than object recognition because recognition is not necessary for segmentation. Specifically, humans can perform image segmentation without even knowing what the objects are (for example, in satellite imagery or medical X-ray scans, there may be several objects which are unknown, but they can still be segmented within the image typically for further investigation). Performing segmentation without knowing the exact identity of all objects in the scene is an important part of our visual understanding process which can give us a powerful model to understand the world and also be used to improve or augment existing computer vision techniques. Herein this work, we review the field of semantic segmentation as pertaining to deep convolutional neural networks. We provide comprehensive coverage of the top approaches and summarize the strengths, weaknesses and major challenges.},
author = {Guo, Yanming and Liu, Yu and Georgiou, Theodoros and Lew, Michael S.},
doi = {10.1007/s13735-017-0141-z},
file = {:Users/philipeborba/Downloads/Guo2018{\_}Article{\_}AReviewOfSemanticSegmentationU.pdf:pdf},
issn = {2192662X},
journal = {International Journal of Multimedia Information Retrieval},
keywords = {Computer vision,Convolutional neural networks,Deep learning,Image segmentation,Machine learning},
number = {2},
pages = {87--93},
publisher = {Springer London},
title = {{A review of semantic segmentation using deep neural networks}},
url = {https://doi.org/10.1007/s13735-017-0141-z},
volume = {7},
year = {2018}
}
@article{Guo2016,
abstract = {Deep learning algorithms are a subset of the machine learning algorithms, which aim at discovering multiple levels of distributed representations. Recently, numerous deep learning algorithms have been proposed to solve traditional artificial intelligence problems. This work aims to review the state-of-the-art in deep learning algorithms in computer vision by highlighting the contributions and challenges from over 210 recent research papers. It first gives an overview of various deep learning approaches and their recent developments, and then briefly describes their applications in diverse vision tasks, such as image classification, object detection, image retrieval, semantic segmentation and human pose estimation. Finally, the paper summarizes the future trends and challenges in designing and training deep neural networks.},
author = {Guo, Yanming and Liu, Yu and Oerlemans, Ard and Lao, Songyang and Wu, Song and Lew, Michael S.},
doi = {10.1016/j.neucom.2015.09.116},
file = {:Users/philipeborba/Downloads/1-s2.0-S0925231215017634-main (1).pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Applications,Challenges,Computer vision,Deep learning,Developments,Trends},
pages = {27--48},
title = {{Deep learning for visual understanding: A review}},
volume = {187},
year = {2016}
}
@article{Mi2020,
abstract = {Semantic segmentation plays an important role in remote sensing image understanding. Great progress has been made in this area with the development of Deep Convolutional Neural Networks (DCNNs). However, due to the complexity of ground objects' spectrum, DCNNs with simple classifier have difficulties in distinguishing ground object categories even though they can represent image features effectively. Additionally, DCNN-based semantic segmentation methods learn to accumulate contextual information over large receptive fields that causes blur on object boundaries. In this work, a novel approach named Superpixel-enhanced Deep Neural Forest (SDNF) is proposed to target the aforementioned problems. To improve the classification ability, we introduce Deep Neural Forest (DNF), where the representation learning of deep neural network is conducted by a completely differentiable decision forest. Therefore, better classification accuracy is achieved by combining DCNNs with decision forests in an end-to-end manner. In addition, considering the homogeneity within superpixels and heterogeneity between superpixels, a Superpixel-enhanced Region Module (SRM) is proposed to further alleviate the noises and strengthen edges of ground objects. Experimental results on the ISPRS 2D semantic labeling benchmark demonstrate that our model significantly outperforms state-of-the-art methods thus validate the efficiency of our proposed SDNF.},
author = {Mi, Li and Chen, Zhenzhong},
doi = {10.1016/j.isprsjprs.2019.11.006},
file = {:Users/philipeborba/Downloads/1-s2.0-S0924271619302606-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Neural forest,Remote sensing imagery,Semantic segmentation,Superpixel},
number = {June 2019},
pages = {140--152},
title = {{Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation}},
volume = {159},
year = {2020}
}
@article{Zhu2020,
author = {Zhu, Panpan and Tan, Yumin and Zhang, Liqiang and Wang, Yuebin and Mei, Jie and Liu, Hao and Wu, Mengfan},
doi = {10.1109/tgrs.2019.2960466},
file = {:Users/philipeborba/Downloads/08955981.pdf:pdf},
issn = {0196-2892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
pages = {1--14},
publisher = {IEEE},
title = {{Deep Learning for Multilabel Remote Sensing Image Annotation With Dual-Level Semantic Concepts}},
year = {2020}
}
@article{Zhang2020a,
abstract = {Residential-area segmentation is one of the most fundamental tasks in the field of remote sensing. Recently, fully supervised convolutional neural network (CNN)-based methods have shown superiority in the field of semantic segmentation. However, a serious problem for those CNN-based methods is that pixel-level annotations are expensive and laborious. In this study, a novel hierarchical weakly supervised learning (HWSL) method is proposed to realize pixel-level semantic segmentation in remote sensing images. First, a weakly supervised hierarchical saliency analysis is proposed to capture a sequence of class-specific hierarchical saliency maps by computing the gradient maps with respect to the middle layers of the CNN. Then, superpixels and low-rank matrix recovery are introduced to highlight the common salient areas and fuse class-specific saliency maps with adaptive weights. Finally, a subtraction operation between class-specific saliency maps is conducted to generate hierarchical residual saliency maps and fulfill residential-area segmentation. Comprehensive evaluations with two remote sensing data sets and comparison with seven methods validate the superiority of the proposed HWSL model.},
author = {Zhang, Libao and Ma, Jie and Lv, Xinran and Chen, Donghui},
doi = {10.1109/LGRS.2019.2914490},
file = {:Users/philipeborba/Downloads/08720000.pdf:pdf},
issn = {15580571},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Deep learning,remote sensing,saliency analysis,semantic segmentation,weakly supervised},
number = {1},
pages = {117--121},
publisher = {IEEE},
title = {{Hierarchical Weakly Supervised Learning for Residential Area Semantic Segmentation in Remote Sensing Images}},
volume = {17},
year = {2020}
}
@article{Wang2020c,
abstract = {Accurate automated segmentation of remote sensing data could benefit applications from land cover mapping and agricultural monitoring to urban development surveyal and disaster damage assessment. While convolutional neural networks (CNNs) achieve state-of-the-art accuracy when segmenting natural images with huge labeled datasets, their successful translation to remote sensing tasks has been limited by low quantities of ground truth labels, especially fully segmented ones, in the remote sensing domain. In this work, we perform cropland segmentation using two types of labels commonly found in remote sensing datasets that can be considered sources of “weak supervision”: (1) labels comprised of single geotagged points and (2) image-level labels. We demonstrate that (1) a U-Net trained on a single labeled pixel per image and (2) a U-Net image classifier transferred to segmentation can outperform pixel-level algorithms such as logistic regression, support vector machine, and random forest. While the high performance of neural networks is well-established for large datasets, our experiments indicate that U-Nets trained on weak labels outperform baseline methods with as few as 100 labels. Neural networks, therefore, can combine superior classification performance with efficient label usage, and allow pixel-level labels to be obtained from image labels.},
annote = {compara cnn, com svm, random forests e logistic regression},
author = {Wang, Sherrie and Chen, William and Xie, Sang Michael and Azzari, George and Lobell, David B.},
doi = {10.3390/rs12020207},
file = {:Users/philipeborba/Downloads/remotesensing-12-00207.pdf:pdf},
issn = {2072-4292},
journal = {Remote Sensing},
keywords = {agriculture,cover classification,deep learning,image segmentation,land,landsat,weak supervision},
number = {2},
pages = {207},
title = {{Weakly Supervised Deep Learning for Segmentation of Remote Sensing Imagery}},
volume = {12},
year = {2020}
}
@article{Li2020a,
abstract = {Remote sensing image scene classification has a high application value in the agricultural, military, as well as other fields. A large amount of remote sensing data is obtained every day. After learning the new batch data, scene classification algorithms based on deep learning face the problem of catastrophic forgetting, that is, they cannot maintain the performance of the old batch data. Therefore, it has become more and more important to ensure that the scene classification model has the ability of continual learning, that is, to learn new batch data without forgetting the performance of the old batch data. However, the existing remote sensing image scene classification datasets all use static benchmarks and lack the standard to divide the datasets into a number of sequential learning training batches, which largely limits the development of continual learning in remote sensing image scene classification. First, this study gives the criteria for training batches that have been partitioned into three continual learning scenarios, and proposes a large-scale remote sensing image scene classification database called the Continual Learning Benchmark for Remote Sensing (CLRS). The goal of CLRS is to help develop state-of-the-art continual learning algorithms in the field of remote sensing image scene classification. In addition, in this paper, a new method of constructing a large-scale remote sensing image classification database based on the target detection pretrained model is proposed, which can effectively reduce manual annotations. Finally, several mainstream continual learning methods are tested and analyzed under three continual learning scenarios, and the results can be used as a baseline for future work.},
author = {Li, Haifeng and Jiang, Hao and Gu, Xin and Peng, Jian and Li, Wenbo and Hong, Liang and Tao, Chao},
doi = {10.3390/s20041226},
file = {:Users/philipeborba/Downloads/sensors-20-01226.pdf:pdf},
isbn = {8615973184170},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {CLRS,Continual learning,Remote sensing dataset,Scene classification},
number = {4},
pages = {1--18},
title = {{CLRS: Continual learning benchmark for remote sensing image scene classification}},
volume = {20},
year = {2020}
}
@article{Heipke2020a,
abstract = {During the last few years, artificial intelligence based on deep learning, and particularly based on convolutional neural networks, has acted as a game changer in just about all tasks related to photogrammetry and remote sensing. Results have shown partly significant improvements in many projects all across the photogrammetric processing chain from image orientation to surface reconstruction, scene classification as well as change detection, object extraction and object tracking and recognition in image sequences. This paper summarizes the foundations of deep learning for photogrammetry and remote sensing before illustrating, by way of example, different projects being carried out at the Institute of Photogrammetry and GeoInformation, Leibniz University Hannover, in this exciting and fast moving field of research and development.},
author = {Heipke, Christian and Rottensteiner, Franz},
doi = {10.1080/10095020.2020.1718003},
file = {:Users/philipeborba/Downloads/Deep learning for geometric and semantic tasks in photogrammetry and remote sensing.pdf:pdf},
issn = {10095020},
journal = {Geo-Spatial Information Science},
keywords = {Deep learning,convolutional neural networks(CNN),example project from IPI,machine learning},
number = {00},
pages = {1--10},
publisher = {Taylor {\&} Francis},
title = {{Deep learning for geometric and semantic tasks in photogrammetry and remote sensing}},
url = {https://doi.org/10.1080/10095020.2020.1718003},
volume = {00},
year = {2020}
}
@article{Xu2020,
author = {Xu, Xinying and Xue, Yujing and Han, Xiaoxia and Zhang, Zhe and Xie, Jun},
file = {:Users/philipeborba/Downloads/applsci-10-01679-v3.pdf:pdf},
keywords = {conditional random fields,crf,image semantic segmentation,iss,weakly supervised},
title = {{applied sciences Weakly Supervised Conditional Random Fields Model for Semantic Segmentation with Image Patches}},
year = {2020}
}
@article{Cai2020,
abstract = {Abstract. With the deepening research and cross-fusion in the modern remote sensing image area, the classification of high spatial resolution remote sensing images has captured the attention of the researchers in the field of remote sensing. However, due to the serious phenomenon of “same object, different spectrum” and “same spectrum, different object” of high-resolution remote sensing image, the traditional classification strategy is hard to handle this challenge. In this paper, a remote sensing image scene classification model based on SENet and Inception-V3 is proposed by utilizing the deep learning method and transfer learning strategy. The model first adds a dropout layer before the full connection layer of the original Inception-V3 model to avoid over-fitting. Then we embed the SENet module into the Inception-V3 model for optimizing the network performance. In this paper, global average pooling is used as squeeze operation, and then two fully connected layers are used to construct a bottleneck structure. The model proposed in this paper is more non-linear, can better fit the complex correlation between channels, and greatly reduces the amount of parameters and computation. In the training process, this paper adopts the transfer learning strategy, makes full use of existing models and knowledge, improves training efficiency, and finally obtains scene classification results. The experimental results based on AID high-score remote sensing scene images show that SE-Inception has faster convergence speed and more stable training effect than the original Inception-V3 training. Compared with other traditional methods and deep learning networks, the improved model proposed in this paper has greater accuracy improvement.},
author = {Cai, Z. L. and Weng, Q. and Ye, S. Z.},
doi = {10.5194/isprs-archives-xlii-3-w10-539-2020},
file = {:Users/philipeborba/Downloads/isprs-archives-XLII-3-W10-539-2020.pdf:pdf},
issn = {2194-9034},
journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {abstract,and cross-fusion in the,area,convolutional neural networks inception-v3,deep learning,due to the,has captured the attention,high spatial resolution,however,modern remote sensing image,of the researchers in,remote sensing image classification,remote sensing images,resolution remote sensing images,senet,sensing,spatial,the classification of high,the field of remote,transfer learning,with the deepening research},
number = {November 2019},
pages = {539--545},
title = {{Research on Se-Inception in High-Resolution Remote Sensing Image Classification}},
volume = {XLII-3/W10},
year = {2020}
}
@article{R-cnn2020,
annote = {svm vs deep learning (compara{\c{c}}{\~{a}}o que o prof pediu)},
author = {R-cnn, Improved Mask},
file = {:Users/philipeborba/Downloads/sensors-20-01465-v2.pdf:pdf},
keywords = {building extraction,convolutional neural networks,high-resolution remote,mask r-cnn},
pages = {1--13},
title = {{sensors An E ffi cient Building Extraction Method from High}},
year = {2020}
}
@article{Hang2020,
abstract = {Abstract. The detection and reconstruction of building have attracted more attention in the community of remote sensing and computer vision. Light detection and ranging (LiDAR) has been proved to be a good way to extract building roofs, while we have to face the problem of data shortage for most of the time. In this paper, we tried to extract the building roofs from very high resolution (VHR) images of Chinese satellite Gaofen-2 by employing convolutional neural network (CNN). It has been proved that the CNN is of a higher capability of recognizing detailed features which may not be classified out by object-based classification approach. Several major steps are concerned in this study, such as generation of training dataset, model training, image segmentation and building roofs recognition. First, urban objects such as trees, roads, squares and buildings were classified based on random forest algorithm by an object-oriented classification approach, the building regions were separated from other classes at the aid of visually interpretation and correction; Next, different types of building roofs mainly categorized by color and size information were trained using the trained CNN. Finally, the industrial and residential building roofs have been recognized individually and the results have been validated individually. The assessment results prove effectiveness of the proposed method with approximately 91{\%} and 88{\%} of quality rates in detection industrial and residential building roofs, respectively. Which means that the CNN approach is prospecting in detecting buildings with a very higher accuracy.},
author = {Hang, L. and Cai, G. Y.},
doi = {10.5194/isprs-archives-xlii-3-w10-187-2020},
file = {:Users/philipeborba/Downloads/isprs-archives-XLII-3-W10-187-2020.pdf:pdf},
issn = {2194-9034},
journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {building roofs,cnn,vhr images},
number = {November 2019},
pages = {187--192},
title = {{Cnn Based Detection of Building Roofs From High Resolution Satellite Images}},
volume = {XLII-3/W10},
year = {2020}
}
@article{End-to-end2020,
author = {End-to-end, An and Method, Localized Post-processing and Images, Classification Result},
file = {:Users/philipeborba/Downloads/remotesensing-12-00852.pdf:pdf},
keywords = {conditional random field,crf,high-resolution remote sensing image,pixel-wise classification,result correction,semantic segmentation},
title = {{An End-to-End and Localized Post-Processing Method for Correcting High-Resolution Remote Sensing Classification Result Images}},
year = {2020}
}
@article{Wang2019a,
abstract = {Semantic segmentation can be considered as a per-pixel localization and classification problem, which gives a meaningful label to each pixel in an input image. Deep convolutional neural networks have made extremely successful in semantic segmentation in recent years. However, some challenges still exist. The first challenge task is that most current networks are complex and it is hard to deploy these models on mobile devices because of the limitation of computational cost and memory. Getting more contextual information from downsampled feature maps is another challenging task. To this end, we propose an asymmetric depthwise separable convolution network (ADSCNet) which is a lightweight neural network for real-time semantic segmentation. To facilitating information propagation, Dense Dilated Convolution Connections (DDCC), which connects a set of dilated convolutional layers in a dense way, is introduced in the network. Pooling operation is inserted before ADSCNet unit to cover more contextual information in prediction. Extensive experimental results validate the superior performance of our proposed method compared with other network architectures. Our approach achieves mean intersection over union (mIOU) of 67.5{\%} on Cityscapes dataset at 76.9 frames per second.},
author = {Wang, Jiawei and Xiong, Hongyun and Wang, Haibo and Nian, Xiaohong},
doi = {10.1007/s10489-019-01587-1},
file = {:Users/philipeborba/Downloads/Wang2020{\_}Article{\_}ADSCNetAsymmetricDepthwiseSepa.pdf:pdf},
issn = {15737497},
journal = {Applied Intelligence},
keywords = {Dense connection,Depthwise separable convolution,Real-time,Semantic segmentation},
publisher = {Applied Intelligence},
title = {{ADSCNet: asymmetric depthwise separable convolution for semantic segmentation in real-time}},
year = {2019}
}
@article{Zhang2020b,
abstract = {The semantic segmentation of remote sensing images (RSIs) is important in a variety of applications. Conventional encoder-decoder-based convolutional neural networks (CNNs) use cascade pooling operations to aggregate the semantic information, which results in a loss of localization accuracy and in the preservation of spatial details. To overcome these limitations, we introduce the use of the high-resolution network (HRNet) to produce high-resolution features without the decoding stage. Moreover, we enhance the low-to-high features extracted from different branches separately to strengthen the embedding of scale-related contextual information. The low-resolution features contain more semantic information and have a small spatial size; thus, they are utilized to model the long-term spatial correlations. The high-resolution branches are enhanced by introducing an adaptive spatial pooling (ASP) module to aggregate more local contexts. By combining these context aggregation designs across different levels, the resulting architecture is capable of exploiting spatial context at both global and local levels. The experimental results obtained on two RSI datasets show that our approach significantly improves the accuracy with respect to the commonly used CNNs and achieves state-of-the-art performance.},
author = {Zhang, Jing and Lin, Shaofu and Ding, Lei and Bruzzone, Lorenzo},
doi = {10.3390/rs12040701},
file = {:Users/philipeborba/Downloads/remotesensing-12-00701.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Convolutional neural network,Deep learning,Image analysis,Remote sensing,Semantic segmentation},
number = {4},
pages = {1--16},
title = {{Multi-scale context aggregation for semantic segmentation of remote sensing images}},
volume = {12},
year = {2020}
}
@article{Audebert2019,
abstract = {In recent years, deep-learning techniques revolutionized the way remote sensing data are processed. The classification of hyperspectral data is no exception to the rule, but it has intrinsic specificities that make the application of deep learning less straightforward than with other optical data. This article presents the state of the art of previous machine-learning approaches, reviews the various deeplearning approaches currently proposed for hyperspectral classification, and identifies the problems and difficulties that arise in the implementation of deep neural networks for this task. In particular, the issues of spatial and spectral resolution, data volume, and transfer of models from multimedia images to hyperspectral data are addressed. Additionally, a comparative study of various families of network architectures is provided, and a software toolbox is publicly released to allow experimenting with these methods (https://github.com/nshaud/DeepHyperX). This article is intended for both data scientists with interest in hyperspectral data and remote sensing experts eager to apply deeplearning techniques to their own data set.},
archivePrefix = {arXiv},
arxivId = {1904.10674},
author = {Audebert, Nicolas and {Le Saux}, Bertrand and Lefevre, Sebastien},
doi = {10.1109/MGRS.2019.2912563},
eprint = {1904.10674},
file = {:Users/philipeborba/Downloads/08738045.pdf:pdf},
issn = {21686831},
journal = {IEEE Geoscience and Remote Sensing Magazine},
number = {2},
pages = {159--173},
title = {{Deep learning for classification of hyperspectral data: A comparative review}},
volume = {7},
year = {2019}
}
@article{Zhu2017b,
abstract = {Central to the looming paradigm shift toward data-intensive science, machine-learning techniques are becoming increasingly important. In particular, deep learning has proven to be both a major breakthrough and an extremely powerful tool in many fields. Shall we embrace deep learning as the key to everything? Or should we resist a black-box solution? These are controversial issues within the remote-sensing community. In this article, we analyze the challenges of using deep learning for remote-sensing data analysis, review recent advances, and provide resources we hope will make deep learning in remote sensing seem ridiculously simple. More importantly, we encourage remote-sensing scientists to bring their expertise into deep learning and use it as an implicit general model to tackle unprecedented, large-scale, influential challenges, such as climate change and urbanization.},
annote = {great article},
archivePrefix = {arXiv},
arxivId = {1710.03959},
author = {Zhu, Xiao Xiang and Tuia, Devis and Mou, Lichao and Xia, Gui-Song and Zhang, Liangpei and Xu, Feng and Fraundorfer, Friedrich},
doi = {10.1109/MGRS.2017.2762307},
eprint = {1710.03959},
file = {:Users/philipeborba/Downloads/08113128.pdf:pdf},
journal = {IEEE Geoscience and Remote Sensing Magazine},
keywords = {Computer vision,Feature extraction,Hyperspectral imaging,Machine learning,Remote sensing,Tutorials},
number = {December},
pages = {8--36},
title = {{Deep learning in remote sensing: a comprehensive review and list of resources}},
url = {https://ieeexplore.ieee.org/document/8113128},
volume = {5},
year = {2017}
}
@techreport{Ganguli,
abstract = {Automatically generating maps from satellite images is an important task. There is a body of literature which tries to address this challenge. We created a more expansive survey of the task by experimenting with different models and adding new loss functions to improve results. We created a database of pairs of satellite images and the corresponding map of the area. Our model translates the satellite image to the corresponding standard layer map image using three main model architectures: (i) a conditional Generative Adversarial Network (GAN) which compresses the images down to a learned embedding, (ii) a generator which is trained as a normalizing flow (RealNVP) model, and (iii) a conditional GAN where the generator translates via a series of convolutions to the standard layer of a map and the discriminator input is the concatenation of the real/generated map and the satellite image. Model (iii) was by far the most promising of three models. To improve the results we also added a reconstruction loss and style transfer loss in addition to the GAN losses. The third model architecture produced the best quality of sampled images. In contrast to the other generative model where evaluation of the model is a challenging problem. since we have access to the real map for a given satellite image, we are able to assign a quantitative metric to the quality of the generated images in addition to inspecting them visually. While we are continuing to work on increasing the accuracy of the model, one challenge has been the coarse resolution of the data which upper-bounds the quality of the results of our model. Nevertheless, as will be seen in the results, the generated map is more accurate in the features it produces since the generator architecture demands a pixel-wise image translation/pixel-wise coloring. A video presentation summarizing this paper is available at: https://youtu.be/Ur0flOX-Ji0},
archivePrefix = {arXiv},
arxivId = {1902.05611v1},
author = {Ganguli, Swetava and Garzon, Pedro and Glaser, Noa},
eprint = {1902.05611v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Ganguli, Garzon, Glaser - Unknown - GeoGAN A Conditional GAN with Reconstruction and Style Loss to Generate Standard Layer of Maps from.pdf:pdf},
title = {{GeoGAN: A Conditional GAN with Reconstruction and Style Loss to Generate Standard Layer of Maps from Satellite Images}},
url = {https://youtu.be/Ur0flOX-Ji0}
}
@article{Liu2020,
abstract = {Land cover classification of remote sensing images is a challenging task due to limited amounts of annotated data, highly imbalanced classes, frequent incorrect pixel-level annotations, and an inherent complexity in the semantic segmentation task. In this article, we propose a novel architecture called the dense dilated convolutions' merging network (DDCM-Net) to address this task. The proposed DDCM-Net consists of dense dilated image convolutions merged with varying dilation rates. This effectively utilizes rich combinations of dilated convolutions that enlarge the network's receptive fields with fewer parameters and features compared with the state-of-the-art approaches in the remote sensing domain. Importantly, DDCM-Net obtains fused local- and global-context information, in effect incorporating surrounding discriminative capability for multiscale and complex-shaped objects with similar color and textures in very high-resolution aerial imagery. We demonstrate the effectiveness, robustness, and flexibility of the proposed DDCM-Net on the publicly available ISPRS Potsdam and Vaihingen data sets, as well as the DeepGlobe land cover data set. Our single model, trained on three-band Potsdam and Vaihingen data sets, achieves better accuracy in terms of both mean intersection over union (mIoU) and F1-score compared with other published models trained with more than three-band data. We further validate our model on the DeepGlobe data set, achieving state-of-the-art result 56.2{\%} mIoU with much fewer parameters and at a lower computational cost compared with related recent work. Code available at https://github.com/samleoqh/DDCM-Semantic-Segmentation-PyTorch},
archivePrefix = {arXiv},
arxivId = {2003.04027},
author = {Liu, Qinghui and Kampffmeyer, Michael and Jessen, Robert and Salberg, Arnt-B{\o}rre},
doi = {10.1109/TGRS.2020.2976658},
eprint = {2003.04027},
file = {:Users/philipeborba/Downloads/Dense{\_}Dilated{\_}Convolutions{\_}Merging{\_}Network{\_}for{\_}Lan.pdf:pdf},
number = {March},
title = {{Dense Dilated Convolutions Merging Network for Land Cover Classification}},
url = {http://arxiv.org/abs/2003.04027{\%}0Ahttp://dx.doi.org/10.1109/TGRS.2020.2976658},
year = {2020}
}
@article{Kang2019a,
abstract = {We present a random forest framework that learns the weights, shapes, and sparsities of feature representations for real-time semantic segmentation. Typical filters (kernels) have predetermined shapes and sparsities and learn only weights. A few feature extraction methods fix weights and learn only shapes and sparsities. These predetermined constraints restrict learning and extracting optimal features. To overcome this limitation, we propose an unconstrained representation that is able to extract optimal features by learning weights, shapes, and sparsities. We then present the random forest framework that learns the flexible filters using an iterative optimization algorithm and segments input images using the learned representations. We demonstrate the effectiveness of the proposed method using a hand segmentation dataset for hand-object interaction and using two semantic segmentation datasets. The results show that the proposed method achieves real-time semantic segmentation using limited computational and memory resources.},
archivePrefix = {arXiv},
arxivId = {1901.07828},
author = {Kang, Byeongkeun and Nguyen, Truong Q.},
doi = {10.1109/TIP.2019.2905081},
eprint = {1901.07828},
file = {:Users/philipeborba/Downloads/Random Forest With Learned Representations for Semantic Segmentation.pdf:pdf},
issn = {19410042},
journal = {IEEE Transactions on Image Processing},
keywords = {Semantic segmentation,feature extraction,object segmentation,random forest,real-time systems},
number = {7},
pages = {3542--3555},
title = {{Random Forest with Learned Representations for Semantic Segmentation}},
volume = {28},
year = {2019}
}
@article{Xia2019,
abstract = {In recent years, deep convolutional neural networks have gradually become the preferred method for image processing. After the development of Classification, Detection and Segmentation, a large variety of state-of-the-art models and algorithms have emerged in the field. However, for some specific data sets or tasks, not all methods are applicable, which is inconvenient to researchers. This paper took the data set provided in the airbus ship detection challenge in Kaggle as an example to explore an easy and effective method for segmentation tasks of data sets with class imbalance. This paper used U-Net with a pre-trained ResNets model, and tried different methods to explore the feature of the set. In the process of training ResNets, this paper proposed a new convolutional block structure which is inspired by Fibonacci sequence, but the effect is not good. In the end, the mF2 values of the models this paper trained achieved good results, which is better than the model of the combined training of ResNets and ordinary U-Net34. Moreover, the training parameters are less than that. This paper believe that this simple and effective training method will bring convenience to researchers in related fields.},
author = {Xia, Xiaoling and Lu, Qinyang and Gu, Xin},
doi = {10.1088/1742-6596/1213/2/022003},
file = {:Users/philipeborba/Downloads/Exploring An Easy Way for Imbalanced Data Sets in Semantic Image Segmentation.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
number = {2},
title = {{Exploring An Easy Way for Imbalanced Data Sets in Semantic Image Segmentation}},
volume = {1213},
year = {2019}
}
@article{Zhang2019a,
abstract = {The study investigates land use/cover classification and change detection of urban areas from very high resolution (VHR) remote sensing images using deep learning-based methods. Firstly, we introduce a fully Atrous convolutional neural network (FACNN) to learn the land cover classification. In the FACNN an encoder, consisting of full Atrous convolution layers, is proposed for extracting scale robust features from VHR images. Then, a pixel-based change map is produced based on the classification map of current images and an outdated land cover geographical information system (GIS) map. Both polygon-based and object-based change detection accuracy is investigated, where a polygon is the unit of the GIS map and an object consists of those adjacent changed pixels on the pixel-based change map. The test data covers a rapidly developing city of Wuhan (8000 km2), China, consisting of 0.5 m ground resolution aerial images acquired in 2014, and 1 m ground resolution Beijing-2 satellite images in 2017, and their land cover GIS maps. Testing results showed that our FACNN greatly exceeded several recent convolutional neural networks in land cover classification. Second, the object-based change detection could achieve much better results than a pixel-based method, and provide accurate change maps to facilitate manual urban land cover updating.},
author = {Zhang, Chi and Wei, Shiqing and Ji, Shunping and Lu, Meng},
doi = {10.3390/ijgi8040189},
file = {:Users/philipeborba/Downloads/Detecting{\_}Large-Scale{\_}Urban{\_}Land{\_}Cover{\_}Changes{\_}fro.pdf:pdf},
issn = {22209964},
journal = {ISPRS International Journal of Geo-Information},
keywords = {Atrous convolution,Change detection,Classification,Convolutional neural networks,Very-high-resolution remote sensing images},
number = {4},
title = {{Detecting large-scale urban land cover changes from very high resolution remote sensing images using CNN-based classification}},
volume = {8},
year = {2019}
}
@techreport{Geng,
abstract = {Semantic segmentation is a challenging task that needs to handle large scale variations, deformations and different viewpoints. In this paper, we develop a novel network named Gated Path Selection Network (GPSNet), which aims to learn adaptive receptive fields. In GPSNet, we first design a two-dimensional multi-scale network-SuperNet, which densely incorporates features from growing receptive fields. To dynamically select desirable semantic context, a gate prediction module is further introduced. In contrast to previous works that focus on optimizing sample positions on the regular grids, GPSNet can adaptively capture free form dense semantic contexts. The derived adaptive receptive fields are data-dependent, and are flexible that can model different object geometric transformations. On two representative semantic segmentation datasets, i.e., Cityscapes, and ADE20K, we show that the proposed approach consistently outperforms previous methods and achieves competitive performance without bells and whistles.},
archivePrefix = {arXiv},
arxivId = {2001.06819v1},
author = {Geng, Qichuan and Zhang, Hong and Qi, Xiaojuan and Yang, Ruigang and Zhou, Zhong and Huang, Gao},
eprint = {2001.06819v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Geng et al. - Unknown - Gated Path Selection Network for Semantic Segmentation.pdf:pdf},
title = {{Gated Path Selection Network for Semantic Segmentation}}
}
@techreport{HajizadehSaffar,
abstract = {This paper gives an overview on semantic segmentation consists of an explanation of this field, it's status and relation with other vision fundamental tasks, different datasets and common evaluation parameters that have been used by researchers. This survey also includes an overall review on a variety of recent approaches (RDF, MRF, CRF, etc.) and their advantages and challenges and shows the superiority of CNN-based semantic segmentation systems on CamVid and NYUDv2 datasets. In addition, some areas that is ideal for future work have mentioned.},
author = {{Hajizadeh Saffar}, Mohammad and Fayyaz, Mohsen and Sabokrou, Mohammad and Fathy, Mahmood},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Hajizadeh Saffar et al. - Unknown - Semantic Video Segmentation A Review on Recent Approaches.pdf:pdf},
keywords = {CNN,CRF,Convolutional Networks,Deep Learning,MRF,RDF,SVM,Semantic Segmentation,Unsupervised Graph Modeling,Video Segmentation},
title = {{Semantic Video Segmentation: A Review on Recent Approaches}}
}
@techreport{MajidAzimi,
abstract = {Understanding the complex urban infrastructure with centimeter-level accuracy is essential for many applications from autonomous driving to mapping, infrastructure monitoring , and urban management. Aerial images provide valuable information over a large area instantaneously; nevertheless, no current dataset captures the complexity of aerial scenes at the level of granularity required by real-world applications. To address this, we introduce SkyScapes, an aerial image dataset with highly-accurate, fine-grained annotations for pixel-level semantic labeling. SkyScapes provides annotations for 31 semantic categories ranging from large structures, such as buildings, roads and vegetation, to fine details, such as 12 (sub-)categories of lane markings. We have defined two main tasks on this dataset: dense semantic segmentation and multi-class lane-marking prediction. We carry out extensive experiments to evaluate state-of-the-art segmentation methods on SkyScapes. Existing methods struggle to deal with the wide range of classes, object sizes, scales, and fine details present. We therefore propose a novel multi-task model, which incorporates semantic edge detection and is better tuned for feature extraction from a wide range of scales. This model achieves notable improvements over the base-lines in region outlines and level of detail on both tasks.},
author = {{Majid Azimi}, Seyed and Henry, Corentin and Sommer, Lars and Schumann, Arne and Vig, Eleonora},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Majid Azimi et al. - Unknown - SkyScapes-Fine-Grained Semantic Understanding of Aerial Scenes.pdf:pdf},
title = {{SkyScapes-Fine-Grained Semantic Understanding of Aerial Scenes}},
url = {https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760Aerialimagewithoverlaidannotation:dense}
}
@techreport{Valada,
abstract = {Learning to reliably perceive and understand the scene is an integral enabler for robots to operate in the real-world. This problem is inherently challenging due to the multitude of object types as well as appearance changes caused by varying illumination and weather conditions. Leveraging complementary modalities can enable learning of semantically richer representations that are resilient to such perturbations. Despite the tremendous progress in recent years, most multimodal convolutional neural network approaches directly concatenate feature maps from individual modality streams rendering the model incapable of focusing only on the relevant complementary information for fusion. To address this limitation, we propose a mutimodal semantic segmentation framework that dynamically adapts the fusion of modality-specific features while being sensitive to the object category, spatial location and scene context in a self-supervised manner. Specifically, we propose an architecture consisting of two modality-specific encoder streams that fuse intermediate encoder representations into a single decoder using our proposed self-supervised model adaptation fusion mechanism which optimally combines complementary features. As intermediate representations are not aligned across modalities , we introduce an attention scheme for better correlation. In addition, we propose a computationally efficient unim-odal segmentation architecture termed AdapNet++ that incorporates a new encoder with multiscale residual units and an efficient atrous spatial pyramid pooling that has a lar-Abhinav Valada ger effective receptive field with more than 10× fewer parameters , complemented with a strong decoder with a multi-resolution supervision scheme that recovers high-resolution details. Comprehensive empirical evaluations on Cityscapes, Synthia, SUN RGB-D, ScanNet and Freiburg Forest benchmarks demonstrate that both our unimodal and multimodal architectures achieve state-of-the-art performance while simultaneously being efficient in terms of parameters and inference time as well as demonstrating substantial robustness in adverse perceptual conditions.},
archivePrefix = {arXiv},
arxivId = {1808.03833v3},
author = {Valada, Abhinav and Mohan, Rohit and Burgard, Wolfram},
eprint = {1808.03833v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Valada, Mohan, Burgard - Unknown - Self-Supervised Model Adaptation for Multimodal Semantic Segmentation.pdf:pdf},
keywords = {Adaptation {\textperiodcentered},Deep,Fusion {\textperiodcentered},Learning,Model,Multimodal,Scene,Segmentation {\textperiodcentered},Semantic,Understanding {\textperiodcentered}},
title = {{Self-Supervised Model Adaptation for Multimodal Semantic Segmentation}}
}
@techreport{Zhang,
abstract = {Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution. We find that introducing semantic information into low-level features and high-resolution details into high-level features is more effective for the later fusion. Based on this observation, we propose a new framework, named ExFuse, to bridge the gap between low-level and high-level features thus significantly improve the segmentation quality by 4.0{\%} in total. Furthermore , we evaluate our approach on the challenging PASCAL VOC 2012 segmentation benchmark and achieve 87.9{\%} mean IoU, which outper-forms the previous state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1804.03821v1},
author = {Zhang, Zhenli and Zhang, Xiangyu and Peng, Chao and Cheng, Dazhi and Sun, Jian},
eprint = {1804.03821v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - ExFuse Enhancing Feature Fusion for Semantic Segmentation.pdf:pdf},
keywords = {Convolutional Neural Networks,Semantic Segmentation},
title = {{ExFuse: Enhancing Feature Fusion for Semantic Segmentation}}
}
@techreport{Li,
abstract = {Labeling pixel-level masks for fine-grained semantic seg-mentation tasks, e.g. human parsing, remains a challenging task. The ambiguous boundary between different semantic parts and those categories with similar appearance usually are confusing, leading to unexpected noises in ground truth masks. To tackle the problem of learning with label noises, this work introduces a purification strategy, called Self-Correction for Human Parsing (SCHP), to progressively promote the reliability of the supervised labels as well as the learned models. In particular, starting from a model trained with inaccurate annotations as initialization, we design a cyclically learning scheduler to infer more reliable pseudo-masks by iteratively aggregating the current learned model with the former optimal one in an online manner. Besides, those correspondingly corrected labels can in turn to further boost the model performance. In this way, the models and the labels will reciprocally become more robust and accurate during the self-correction learning cycles. Benefiting from the superiority of SCHP, we achieve the best performance on two popular single-person human parsing benchmarks , including LIP and Pascal-Person-Part datasets. Our overall system ranks 1st in CVPR2019 LIP Challenge. Code is available at this url.},
archivePrefix = {arXiv},
arxivId = {1910.09777v1},
author = {Li, Peike and Xu, Yunqiu and Wei, Yunchao and Yang, Yi},
eprint = {1910.09777v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - Self-Correction for Human Parsing.pdf:pdf},
title = {{Self-Correction for Human Parsing}}
}
@techreport{Hu,
abstract = {We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200× faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmenta-tion on two large-scale benchmarks Semantic3D and Se-manticKITTI.},
archivePrefix = {arXiv},
arxivId = {1911.11236v1},
author = {Hu, Qingyong and Yang, Bo and Xie, Linhai and Rosa, Stefano and Guo, Yulan and Wang, Zhihua and Trigoni, Niki and Markham, Andrew},
eprint = {1911.11236v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - Unknown - RandLA-Net Efficient Semantic Segmentation of Large-Scale Point Clouds.pdf:pdf},
title = {{RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds}},
url = {https://github.com/}
}
@techreport{He,
abstract = {Many successful learning targets such as minimizing dice loss and cross-entropy loss have enabled unprecedented breakthroughs in segmentation tasks. Beyond these semantic metrics, this paper aims to introduce location supervision into semantic segmentation. Based on this idea, we present a Location-aware Upsampling (LaU) that adaptively refines the interpolating coordinates with trainable offsets. Then, location-aware losses are established by encouraging pixels to move towards well-classified locations. An LaU is offset prediction coupled with interpolation, which is trained end-to-end to generate confidence score at each position from coarse to fine. Guided by location-aware losses, the new module can replace its plain counterpart (e.g., bilinear upsam-pling) in a plug-and-play manner to further boost the leading encoder-decoder approaches. Extensive experiments validate the consistent improvement over the state-of-the-art methods on benchmark datasets. Our code is available at https://github.com/HolmesShuan/Location-aware-Upsampling-for-Semantic-Segmentation.},
archivePrefix = {arXiv},
arxivId = {1911.05250v2},
author = {He, Xiangyu and Mo, Zitao and Chen, Qiang and Cheng, Anda and Wang, Peisong and Cheng, Jian},
eprint = {1911.05250v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Location-aware Upsampling for Semantic Segmentation(2).pdf:pdf},
title = {{Location-aware Upsampling for Semantic Segmentation}},
url = {https://github.com/HolmesShuan/Location-aware-}
}
@techreport{Cheng,
abstract = {In this work, we introduce Panoptic-DeepLab, a simple, strong, and fast system for panoptic segmentation, aiming to establish a solid baseline for bottom-up methods that can achieve comparable performance of two-stage methods while yielding fast inference speed. In particular, Panoptic-DeepLab adopts the dual-ASPP and dual-decoder structures specific to semantic, and instance segmentation, respectively. The semantic segmentation branch is the same as the typical design of any semantic segmentation model (e.g., DeepLab), while the instance segmentation branch is class-agnostic, involving a simple instance center regression. As a result, our single Panoptic-DeepLab simultaneously ranks first at all three Cityscapes benchmarks, setting the new state-of-art of 84.2{\%} mIoU, 39.0{\%} AP, and 65.5{\%} PQ on test set. Additionally, equipped with MobileNetV3, Panoptic-DeepLab runs nearly in real-time with a single 1025 × 2049 image (15.8 frames per second), while achieving a competitive performance on Cityscapes (54.1 PQ{\%} on test set). On Mapillary Vistas test set, our ensemble of six models attains 42.7{\%} PQ, outperforming the challenge winner in 2018 by a healthy margin of 1.5{\%}. Finally, our Panoptic-DeepLab also performs on par with several top-down approaches on the challenging COCO dataset. For the first time, we demonstrate a bottom-up approach could deliver state-of-the-art results on panoptic segmentation.},
archivePrefix = {arXiv},
arxivId = {1911.10194v3},
author = {Cheng, Bowen and Collins, Maxwell D and Zhu, Yukun and Liu, Ting and Huang, Thomas S and Adam, Hartwig and Chen, Liang-Chieh},
eprint = {1911.10194v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Cheng et al. - Unknown - Panoptic-DeepLab A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation.pdf:pdf},
isbn = {1911.10194v3},
title = {{Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation}}
}
@techreport{Yuan,
abstract = {In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second , we compute the object region representation by aggre-gating the representations of the pixels lying in the object region. Last, we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff.},
archivePrefix = {arXiv},
arxivId = {1909.11065v2},
author = {Yuan, Yuhui and Chen, Xilin and Wang, Jingdong},
eprint = {1909.11065v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Yuan, Chen, Wang - Unknown - Object-Contextual Representations for Semantic Segmentation(2).pdf:pdf},
title = {{Object-Contextual Representations for Semantic Segmentation}}
}
@techreport{Fu,
abstract = {Recent works attempt to improve scene parsing performance by exploring different levels of contexts, and typically train a well-designed convolutional network to exploit useful contexts across all pixels equally. However, in this paper, we find that the context demands are varying from different pixels or regions in each image. Based on this observation , we propose an Adaptive Context Network (AC-Net) to capture the pixel-aware contexts by a competitive fusion of global context and local context according to different per-pixel demands. Specifically, when given a pixel, the global context demand is measured by the similarity between the global feature and its local feature, whose reverse value can be used to measure the local context demand. We model the two demand measurements by the proposed global context module and local context module, respectively , to generate adaptive contextual features. Furthermore , we import multiple such modules to build several adaptive context blocks in different levels of network to obtain a coarse-to-fine result. Finally, comprehensive experimental evaluations demonstrate the effectiveness of the proposed ACNet, and new state-of-the-arts performances are achieved on all four public datasets, i.e. Cityscapes, ADE20K, PASCAL Context, and COCO Stuff.},
author = {Fu, Jun and Liu, Jing and Wang, Yuhang and Li, Yong and Bao, Yongjun and Tang, Jinhui and Lu, Hanqing},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Fu et al. - Unknown - Adaptive Context Network for Scene Parsing.pdf:pdf},
title = {{Adaptive Context Network for Scene Parsing}}
}
@techreport{Chen,
abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolu-tional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed 'DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
archivePrefix = {arXiv},
arxivId = {1706.05587v3},
author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
eprint = {1706.05587v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - Rethinking Atrous Convolution for Semantic Image Segmentation(2).pdf:pdf},
title = {{Rethinking Atrous Convolution for Semantic Image Segmentation}}
}
@techreport{Yu,
abstract = {Most existing methods of semantic segmentation still suffer from two aspects of challenges: intra-class inconsistency and inter-class indistinction. To tackle these two problems , we propose a Discriminative Feature Network (DFN), which contains two sub-networks: Smooth Network and Border Network. Specifically, to handle the intra-class inconsistency problem, we specially design a Smooth Network with Channel Attention Block and global average pooling to select the more discriminative features. Furthermore, we propose a Border Network to make the bilateral features of boundary distinguishable with deep semantic boundary supervision. Based on our proposed DFN, we achieve state-of-the-art performance 86.2{\%} mean IOU on PASCAL VOC 2012 and 80.3{\%} mean IOU on Cityscapes dataset.},
archivePrefix = {arXiv},
arxivId = {1804.09337v1},
author = {Yu, Changqian and Wang, Jingbo and Peng, Chao and Gao, Changxin and Yu, Gang and Sang, Nong},
eprint = {1804.09337v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - Unknown - Learning a Discriminative Feature Network for Semantic Segmentation.pdf:pdf},
title = {{Learning a Discriminative Feature Network for Semantic Segmentation}}
}
@techreport{Zhong,
abstract = {The recent integration of attention mechanisms into seg-mentation networks improves their representational capabilities through a great emphasis on more informative features. However, these attention mechanisms ignore an implicit sub-task of semantic segmentation and are constrained by the grid structure of convolution kernels. In this paper, we propose a novel squeeze-and-attention network (SANet) architecture that leverages an effective squeeze-and-attention (SA) module to account for two distinctive characteristics of segmentation: i) pixel-group attention, and ii) pixel-wise prediction. Specifically, the proposed SA modules impose pixel-group attention on conventional convolution by introducing an 'attention' convolutional channel, thus taking into account spatial-channel inter-dependencies in an efficient manner. The final segmentation results are produced by merging outputs from four hierarchical stages of a SANet to integrate multi-scale contexts for obtaining an enhanced pixel-wise prediction. Empirical experiments on two challenging public datasets validate the effectiveness of the proposed SANets, which achieves 83.2{\%} mIoU (without COCO pre-training) on PASCAL VOC and a state-of-the-art mIoU of 54.4{\%} on PASCAL Context.},
archivePrefix = {arXiv},
arxivId = {1909.03402v3},
author = {Zhong, Zilong and Lin, Zhong Qiu and Bidart, Rene and Hu, Xiaodan and Daya, Ibrahim Ben and Li, Zhifeng and Zheng, Wei-Shi and Li, Jonathan and Wong, Alexander},
eprint = {1909.03402v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Zhong et al. - Unknown - Squeeze-and-Attention Networks for Semantic Segmentation.pdf:pdf},
title = {{Squeeze-and-Attention Networks for Semantic Segmentation}}
}
@techreport{Zhanga,
abstract = {Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Con-volutional Network (FCN) framework by employing Di-lated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmen-tation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7{\%} mIoU on PASCAL-Context, 85.9{\%} mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpasses the winning entry of COCO-Place Challenge 2017. In addition , we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45{\%}, which is comparable with state-of-the-art approaches with over 10× more layers. The source code for the complete system are publicly available 1 .},
archivePrefix = {arXiv},
arxivId = {1803.08904v1},
author = {Zhang, Hang and Dana, Kristin and Shi, Jianping and Zhang, Zhongyue and Wang, Xiaogang and Tyagi, Ambrish and Agrawal, Amit},
eprint = {1803.08904v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Context Encoding for Semantic Segmentation(2).pdf:pdf},
title = {{Context Encoding for Semantic Segmentation}},
url = {http://hangzh.com/}
}
@techreport{Chena,
abstract = {The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part seg-mentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architec-tures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7{\%} on Cityscapes (street scene parsing), 71.3{\%} on PASCAL-Person-Part (person-part segmentation), and 87.9{\%} on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.},
archivePrefix = {arXiv},
arxivId = {1809.04184v1},
author = {Chen, Liang-Chieh and Collins, Maxwell D and Zhu, Yukun and Papandreou, George and Zoph, Barret and Schroff, Florian and Adam, Hartwig and Shlens, Jonathon},
eprint = {1809.04184v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - Searching for Efficient Multi-Scale Architectures for Dense Image Prediction.pdf:pdf},
title = {{Searching for Efficient Multi-Scale Architectures for Dense Image Prediction}},
url = {https://github.com/}
}
@techreport{Wu,
abstract = {We propose an approach to instance-level image segmentation that is built on top of category-level segmentation. Specifically, for each pixel in a semantic category mask, its corresponding instance bounding box is predicted using a deep fully convolutional regression network. Thus it follows a different pipeline to the popular detect-then-segment approaches that first predict instances' bounding boxes, which are the current state-of-the-art in instance segmentation. We show that, by leveraging the strength of our state-of-the-art semantic segmentation models, the proposed method can achieve comparable or even better results to detect-then-segment approaches. We make the following contributions. (i) First, we propose a simple yet effective approach to semantic instance segmentation. (ii) Second, we propose an online bootstrapping method during training, which is critically important for achieving good performance for both semantic category segmentation and instance-level segmentation. (iii) As the performance of semantic category segmentation has a significant impact on the instance-level segmentation, which is the second step of our approach, we train fully convolutional residual networks to achieve the best semantic category segmentation accuracy. On the PASCAL VOC 2012 dataset, we obtain the currently best mean intersection-over-union score of 79.1{\%}. (iv) We also achieve state-of-the-art results for instance-level segmentation.},
archivePrefix = {arXiv},
arxivId = {1605.06885v1},
author = {Wu, Zifeng and Shen, Chunhua and {Van Den Hengel}, Anton},
eprint = {1605.06885v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Shen, Van Den Hengel - Unknown - Bridging Category-level and Instance-level Semantic Image Segmentation.pdf:pdf},
title = {{Bridging Category-level and Instance-level Semantic Image Segmentation *}},
url = {http://host.robots.ox.ac.uk:8080/anonymous/MZVIPW.html}
}
@techreport{Dai,
abstract = {Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called "BoxSup", produces competitive results (e.g., 62.0{\%} mAP for validation) supervised by boxes only, on par with strong base-lines (e.g., 63.8{\%} mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further unleashes the power of deep convo-lutional networks and yields state-of-the-art results on PAS-CAL VOC 2012 and PASCAL-CONTEXT [24].},
archivePrefix = {arXiv},
arxivId = {1503.01640v2},
author = {Dai, Jifeng and He, Kaiming and Sun, Jian},
eprint = {1503.01640v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Dai, He, Sun - Unknown - BoxSup Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation.pdf:pdf},
title = {{BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation}}
}
@techreport{Liuc,
abstract = {We present a technique for adding global context to fully convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN Long et al. (2014)). When we add our proposed global feature, and a technique for learning normalization parameters , accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn .},
archivePrefix = {arXiv},
arxivId = {1506.04579v2},
author = {Liu, Wei and Rabinovich, Andrew and Berg, Alexander C},
eprint = {1506.04579v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Rabinovich, Berg - Unknown - PARSENET LOOKING WIDER TO SEE BETTER.pdf:pdf},
title = {{PARSENET: LOOKING WIDER TO SEE BETTER}},
url = {https://github.com/weiliu89/caffe/tree/fcn}
}
@techreport{Zheng,
abstract = {Pixel-level labelling tasks, such as semantic segmenta-tion, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to de-lineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.},
archivePrefix = {arXiv},
arxivId = {1502.03240v3},
author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H S},
eprint = {1502.03240v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Zheng et al. - Unknown - Conditional Random Fields as Recurrent Neural Networks.pdf:pdf},
title = {{Conditional Random Fields as Recurrent Neural Networks}},
url = {https://github.com/torrvision/crfasrnn}
}
@techreport{Long,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolu-tional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmen-tation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolu-tional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolu-tional networks and transfer their learned representations by fine-tuning [4] to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038v2},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
eprint = {1411.4038v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - Unknown - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
title = {{Fully Convolutional Networks for Semantic Segmentation}}
}
@techreport{Wua,
abstract = {The trend towards increasingly deep neural networks has been driven by a general observation that increasing depth increases the performance of a network. Recently, however, evidence has been amassing that simply increasing depth may not be the best way to increase performance, particularly given other limitations. Investigations into deep residual networks have also suggested that they may not in fact be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. We examine these issues, and in doing so arrive at a new interpretation of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. As a result, we are able to derive a new, shallower , architecture of residual networks which significantly outperforms much deeper models such as ResNet-200 on the ImageNet classification dataset. We also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which out-performs the state-of-the-art by a remarkable margin on datasets including PASCAL VOC, PASCAL Context, and Cityscapes. The architecture that we propose thus outper-forms its comparators, including very deep ResNets, and yet is more efficient in memory use and sometimes also in training time. The code and models are available at https://github.com/itijyou/ademxapp.},
archivePrefix = {arXiv},
arxivId = {1611.10080v1},
author = {Wu, Zifeng and Shen, Chunhua and {Van Den Hengel}, Anton},
eprint = {1611.10080v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Shen, Van Den Hengel - Unknown - Wider or Deeper Revisiting the ResNet Model for Visual Recognition.pdf:pdf},
isbn = {1611.10080v1},
title = {{Wider or Deeper: Revisiting the ResNet Model for Visual Recognition *}},
url = {https://github.com/itijyou/ademxapp.}
}
@techreport{Ding,
abstract = {Scene segmentation is a challenging task as it need label every pixel in the image. It is crucial to exploit discriminative context and aggregate multi-scale features to achieve better segmentation. In this paper, we first propose a novel context contrasted local feature that not only leverages the informative context but also spotlights the local information in contrast to the context. The proposed context contrasted local feature greatly improves the parsing performance, especially for inconspicuous objects and background stuff. Furthermore, we propose a scheme of gated sum to selectively aggregate multi-scale features for each spatial position. The gates in this scheme control the information flow of different scale features. Their values are generated from the testing image by the proposed network learnt from the training data so that they are adaptive not only to the training data, but also to the specific testing image. Without bells and whistles, the proposed approach achieves the state-of-the-arts consistently on the three popular scene segmentation datasets, Pascal Context, SUN-RGBD and COCO Stuff.},
author = {Ding, Henghui and Jiang, Xudong and Shuai, Bing and Liu, Ai Qun and Wang, Gang},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Ding et al. - Unknown - Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation.pdf:pdf},
title = {{Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation}}
}
@techreport{Zhangb,
abstract = {Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Con-volutional Network (FCN) framework by employing Di-lated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmen-tation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7{\%} mIoU on PASCAL-Context, 85.9{\%} mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpasses the winning entry of COCO-Place Challenge 2017. In addition , we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45{\%}, which is comparable with state-of-the-art approaches with over 10× more layers. The source code for the complete system are publicly available 1 .},
archivePrefix = {arXiv},
arxivId = {1803.08904v1},
author = {Zhang, Hang and Dana, Kristin and Shi, Jianping and Zhang, Zhongyue and Wang, Xiaogang and Tyagi, Ambrish and Agrawal, Amit},
eprint = {1803.08904v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Context Encoding for Semantic Segmentation.pdf:pdf},
title = {{Context Encoding for Semantic Segmentation}},
url = {http://hangzh.com/}
}
@techreport{Balloch,
abstract = {Robot perception systems need to perform reliable image segmentation in real-time on noisy, raw perception data. State-of-the-art segmentation approaches use large CNN models and carefully constructed datasets; however, these models focus on accuracy at the cost of real-time inference. Furthermore, the standard semantic segmentation datasets are not large enough for training CNNs without augmentation and are not representative of noisy, uncurated robot perception data. We propose improving the performance of real-time segmentation frameworks on robot perception data by transferring features learned from synthetic seg-mentation data. We show that pretraining real-time seg-mentation architectures with synthetic segmentation data instead of ImageNet improves fine-tuning performance by reducing the bias learned in pretraining and closing the transfer gap as a result. Our experiments show that our real-time robot perception models pretrained on synthetic data outperform those pretrained on ImageNet for every scale of fine-tuning data examined. Moreover, the degree to which synthetic pretraining outperforms ImageNet pretrain-ing increases as the availability of robot data decreases, making our approach attractive for robotics domains where dataset collection is hard and/or expensive.},
archivePrefix = {arXiv},
arxivId = {1809.03676v1},
author = {Balloch, Jonathan C and Agrawal, Varun and Essa, Irfan and Chernova, Sonia},
eprint = {1809.03676v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Balloch et al. - Unknown - Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer(2).pdf:pdf},
title = {{Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer}}
}
@techreport{Goyal,
abstract = {Although Deep Convolutional Neural Networks trained with strong pixel-level annotations have significantly pushed the performance in semantic segmentation, annotation efforts required for the creation of training data remains a roadblock for further improvements. We show that augmentation of the weakly annotated training dataset with synthetic images minimizes both the annotation efforts and also the cost of capturing images with sufficient variety. Evaluation on the PASCAL 2012 validation dataset shows an increase in mean IOU from 52.80{\%} to 55.47{\%} by adding just 100 synthetic images per object class. Our approach is thus a promising solution to the problems of annotation and dataset collection.},
archivePrefix = {arXiv},
arxivId = {1709.00849v3},
author = {Goyal, Manik and Rajpura, Param and Bojinov, Hristo and Hegde, Ravi},
eprint = {1709.00849v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Goyal et al. - Unknown - Dataset Augmentation with Synthetic Images Improves Semantic Segmentation.pdf:pdf},
title = {{Dataset Augmentation with Synthetic Images Improves Semantic Segmentation}}
}
@techreport{Lyu,
abstract = {Video semantic segmentation has been one of the research focus in computer vision recently. It serves as a perception foundation for many fields such as robotics and autonomous driving. The fast development of semantic segmentation attributes enormously to the large scale datasets, especially for the deep learning related methods. Currently, there already exist several semantic segmentation datasets for complex urban scenes, such as the Cityscapes and CamVid datasets. They have been the standard datasets for comparison among semantic segmentation methods. In this paper, we introduce a new high resolution UAV video semantic segmentation dataset as complement, UAVid. Our UAV dataset consists of 30 video sequences capturing high resolution images. In total, 300 images have been densely labelled with 8 classes for urban scene understanding task. Our dataset brings out new challenges. We provide several deep learning baseline methods, among which the proposed novel Multi-Scale-Dilation net performs the best via multi-scale feature extraction. We have also explored the usability of sequence data by leveraging on CRF model in both spatial and temporal domain.},
archivePrefix = {arXiv},
arxivId = {1810.10438v1},
author = {Lyu, Ye and Vosselman, George and Xia, Guisong and Yilmaz, Alper and Yang, Michael Ying},
eprint = {1810.10438v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Lyu et al. - Unknown - The UAVid Dataset for Video Semantic Segmentation.pdf:pdf},
title = {{The UAVid Dataset for Video Semantic Segmentation}}
}
@article{Kemker2018b,
abstract = {Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work.},
archivePrefix = {arXiv},
arxivId = {1703.06452},
author = {Kemker, Ronald and Salvaggio, Carl and Kanan, Christopher},
doi = {10.1016/j.isprsjprs.2018.04.014},
eprint = {1703.06452},
file = {:Users/philipeborba/Downloads/1-s2.0-S0924271618301229-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Convolutional neural network,Deep learning,Multispectral,Semantic segmentation,Synthetic imagery,Unmanned aerial system},
number = {March},
pages = {60--77},
publisher = {Elsevier},
title = {{Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning}},
url = {https://doi.org/10.1016/j.isprsjprs.2018.04.014},
volume = {145},
year = {2018}
}
@techreport{Lin2017b,
abstract = {State-of-the-art semantic image segmentation methods are mostly based on training deep convolutional neural networks (CNNs). In this work, we proffer to improve semantic segmentation with the use of contextual information. In particular, we explore patch-patch context and patch-background context in deep CNNs. We formulate deep structured models by combining CNNs and Conditional Random Fields (CRFs) for learning the patch-patch context between image regions. Specifically, we formulate CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied in order to avoid repeated expensive CRF inference during the course of back propagation.For capturing the patch-background context, we show that a network design with traditional multi-scale image inputs and sliding pyramid pooling is very effective for improving performance. We perform comprehensive evaluation of the proposed method. We achieve new state-of-the-art performance on a number of challenging semantic segmentation datasets.},
archivePrefix = {arXiv},
arxivId = {1603.03183v3},
author = {Lin, Guosheng and Shen, Chunhua and {Van Den Hengel}, Anton and Reid, Ian},
eprint = {1603.03183v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2017 - Exploring Context with Deep Structured models for Semantic Segmentation.pdf:pdf},
keywords = {Conditional Random Fields,Contextual Models !,Convolutional Neural Networks,Index Terms-Semantic Segmentation},
title = {{Exploring Context with Deep Structured models for Semantic Segmentation}},
year = {2017}
}
@techreport{Ballocha,
abstract = {Robot perception systems need to perform reliable image segmentation in real-time on noisy, raw perception data. State-of-the-art segmentation approaches use large CNN models and carefully constructed datasets; however, these models focus on accuracy at the cost of real-time inference. Furthermore, the standard semantic segmentation datasets are not large enough for training CNNs without augmentation and are not representative of noisy, uncurated robot perception data. We propose improving the performance of real-time segmentation frameworks on robot perception data by transferring features learned from synthetic seg-mentation data. We show that pretraining real-time seg-mentation architectures with synthetic segmentation data instead of ImageNet improves fine-tuning performance by reducing the bias learned in pretraining and closing the transfer gap as a result. Our experiments show that our real-time robot perception models pretrained on synthetic data outperform those pretrained on ImageNet for every scale of fine-tuning data examined. Moreover, the degree to which synthetic pretraining outperforms ImageNet pretrain-ing increases as the availability of robot data decreases, making our approach attractive for robotics domains where dataset collection is hard and/or expensive.},
archivePrefix = {arXiv},
arxivId = {1809.03676v1},
author = {Balloch, Jonathan C and Agrawal, Varun and Essa, Irfan and Chernova, Sonia},
eprint = {1809.03676v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Balloch et al. - Unknown - Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer.pdf:pdf},
title = {{Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer}}
}
@techreport{Lin,
abstract = {Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic seg-mentation through the use of contextual information; specifically , we explore 'patch-patch' context between image regions , and 'patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset.},
archivePrefix = {arXiv},
arxivId = {1504.01013v4},
author = {Lin, Guosheng and Shen, Chunhua and {Van Den Hengel}, Anton and Reid, Ian},
eprint = {1504.01013v4},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - Unknown - Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation.pdf:pdf},
title = {{Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation}}
}
@techreport{Mehta,
abstract = {We introduce a fast and efficient convolutional neural network, ES-PNet, for semantic segmentation of high resolution images under resource constraints. ESPNet is based on a new convolutional module, efficient spatial pyramid (ESP), which is efficient in terms of computation, memory, and power. ES-PNet is 22 times faster (on a standard GPU) and 180 times smaller than the state-of-the-art semantic segmentation network PSPNet [1], while its category-wise accuracy is only 8{\%} less. We evaluated ESPNet on a variety of semantic segmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsy whole slide image dataset. Under the same constraints on memory and computation , ESPNet outperforms all the current efficient CNN networks such as Mo-bileNet [16], ShuffleNet [17], and ENet [20] on both standard metrics and our newly introduced performance metrics that measure efficiency on edge devices. Our network can process high resolution images at a rate of 112 and 9 frames per second on a standard GPU and edge device, respectively.},
archivePrefix = {arXiv},
arxivId = {1803.06815v3},
author = {Mehta, Sachin and Rastegari, Mohammad and Caspi, Anat and Shapiro, Linda and Hajishirzi, Hannaneh},
eprint = {1803.06815v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Mehta et al. - Unknown - ESPNet Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation.pdf:pdf},
title = {{ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation}}
}
@techreport{Meletis,
abstract = {We propose a convolutional network with hierarchical classifiers for per-pixel semantic segmentation, which is able to be trained on multiple, heterogeneous datasets and exploit their semantic hierarchy. Our network is the first to be simultaneously trained on three different datasets from the intelligent vehicles domain, i.e. Cityscapes, GTSDB and Mapillary Vistas, and is able to handle different semantic level-of-detail, class imbalances, and different annotation types, i.e. dense per-pixel and sparse bounding-box labels. We assess our hierarchical approach, by comparing against flat, non-hierarchical classifiers and we show improvements in mean pixel accuracy of 13.0{\%} for Cityscapes classes and 2.4{\%} for Vistas classes and 32.3{\%} for GTSDB classes. Our implementation achieves inference rates of 17 fps at a resolution of 520 x 706 for 108 classes running on a GPU.},
archivePrefix = {arXiv},
arxivId = {1803.05675v2},
author = {Meletis, Panagiotis and Dubbelman, Gijs},
eprint = {1803.05675v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Meletis, Dubbelman - Unknown - Training of Convolutional Networks on Multiple Heterogeneous Datasets for Street Scene Semantic Segmentat.pdf:pdf},
title = {{Training of Convolutional Networks on Multiple Heterogeneous Datasets for Street Scene Semantic Segmentation}}
}
@article{Zhao2017a,
abstract = {The deep learning technology has shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. In particular, recent advances of deep learning techniques bring encouraging performance to fine-grained image classification which aims to distinguish subordinate-level categories, such as bird species or dog breeds. This task is extremely challenging due to high intra-class and low inter-class variance. In this paper, we review four types of deep learning based fine-grained image classification approaches, including the general convolutional neural networks (CNNs), part detection based, ensemble of networks based and visual attention based fine-grained image classification approaches. Besides, the deep learning based semantic segmentation approaches are also covered in this paper. The region proposal based and fully convolutional networks based approaches for semantic segmentation are introduced respectively.},
author = {Zhao, Bo and Feng, Jiashi and Wu, Xiao and Yan, Shuicheng},
doi = {10.1007/s11633-017-1053-3},
file = {:Users/philipeborba/Downloads/Zhao2017{\_}Article{\_}ASurveyOnDeepLearning-basedFin.pdf:pdf},
issn = {17518520},
journal = {International Journal of Automation and Computing},
keywords = {Deep learning,convolutional neural network (CNN),fine-grained image classification,recurrent neural network (RNN),semantic segmentation},
number = {2},
pages = {119--135},
title = {{A survey on deep learning-based fine-grained object classification and semantic segmentation}},
volume = {14},
year = {2017}
}
@article{Garcia-Garcia2018,
abstract = {Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we formulate the semantic segmentation problem and define the terminology of this field as well as interesting background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and goals. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. We also devote a part of the paper to review common loss functions and error metrics for this problem. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.},
author = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Martinez-Gonzalez, Pablo and Garcia-Rodriguez, Jose},
doi = {10.1016/j.asoc.2018.05.018},
file = {:Users/philipeborba/Downloads/1-s2.0-S1568494618302813-main.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Deep learning,Scene labeling,Semantic segmentation},
pages = {41--65},
publisher = {Elsevier B.V.},
title = {{A survey on deep learning techniques for image and video semantic segmentation}},
url = {https://doi.org/10.1016/j.asoc.2018.05.018},
volume = {70},
year = {2018}
}
@article{Jiang2018,
abstract = {The image semantic segmentation has been extensively studying. The modern methods rely on the deep convolutional neural networks, which can be trained to address this problem. A few years ago networks require the huge dataset to be trained. However, the recent advances in deep learning allow training networks on the small datasets, which is a critical issue for medical images, since the hospitals and research organizations usually do not provide the huge amount of data. In this paper, we address medical image semantic segmentation problem by applying the modern CNN model. Moreover, the recent achievements in deep learning allow processing the whole image per time by applying concepts of the fully convolutional neural network. Our qualitative and quantitate experiment results demonstrated that modern CNN can successfully tackle the medical image semantic segmentation problem.},
author = {Jiang, Feng and Grigorev, Aleksei and Rho, Seungmin and Tian, Zhihong and Fu, Yun Sheng and Jifara, Worku and Adil, Khan and Liu, Shaohui},
doi = {10.1007/s00521-017-3158-6},
file = {:Users/philipeborba/Downloads/Medical image semantic segmentation based on deep learning.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Medical image,Neural network,Semantic segmentation,X-Ray},
number = {5},
pages = {1257--1265},
title = {{Medical image semantic segmentation based on deep learning}},
volume = {29},
year = {2018}
}
@article{Guo2016a,
abstract = {Deep learning algorithms are a subset of the machine learning algorithms, which aim at discovering multiple levels of distributed representations. Recently, numerous deep learning algorithms have been proposed to solve traditional artificial intelligence problems. This work aims to review the state-of-the-art in deep learning algorithms in computer vision by highlighting the contributions and challenges from over 210 recent research papers. It first gives an overview of various deep learning approaches and their recent developments, and then briefly describes their applications in diverse vision tasks, such as image classification, object detection, image retrieval, semantic segmentation and human pose estimation. Finally, the paper summarizes the future trends and challenges in designing and training deep neural networks.},
author = {Guo, Yanming and Liu, Yu and Oerlemans, Ard and Lao, Songyang and Wu, Song and Lew, Michael S.},
doi = {10.1016/j.neucom.2015.09.116},
file = {:Users/philipeborba/Downloads/1-s2.0-S0925231215017634-main.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Applications,Challenges,Computer vision,Deep learning,Developments,Trends},
pages = {27--48},
title = {{Deep learning for visual understanding: A review}},
volume = {187},
year = {2016}
}
@techreport{Wub,
abstract = {Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract high-resolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Up-sampling (JPU) by formulating the task of extracting high-resolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolu-tions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13{\%}) and ADE20K dataset (final score of 0.5584) while running 3 times faster. Code is available in https://github.com/wuhuikai/FastFCN .},
archivePrefix = {arXiv},
arxivId = {1903.11816v1},
author = {Wu, Huikai and Zhang, Junge and Huang, Kaiqi and Liang, Kongming and Deepwise, Yu and Lab, A I},
eprint = {1903.11816v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - Unknown - FastFCN Rethinking Dilated Convolution in the Backbone for Semantic Segmentation.pdf:pdf},
title = {{FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation}},
url = {https://github.com/wuhuikai/FastFCN}
}
@techreport{Dinga,
abstract = {Context is essential for semantic segmentation. Due to the diverse shapes of objects and their complex layout in various scene images, the spatial scales and shapes of contexts for different objects have very large variation. It is thus ineffective or inefficient to aggregate various context information from a predefined fixed region. In this work, we propose to generate a scale-and shape-variant semantic mask for each pixel to confine its contextual region. To this end, we first propose a novel paired convolution to infer the semantic correlation of the pair and based on that to generate a shape mask. Using the inferred spatial scope of the contextual region, we propose a shape-variant convolution, of which the receptive field is controlled by the shape mask that varies with the appearance of input. In this way, the proposed network aggregates the context information of a pixel from its semantic-correlated region instead of a predefined fixed region. Furthermore, this work also proposes a labeling denoising model to reduce wrong predictions caused by the noisy low-level features. Without bells and whistles, the proposed segmentation network achieves new state-of-the-arts consistently on the six public segmentation datasets.},
archivePrefix = {arXiv},
arxivId = {1909.02651v1},
author = {Ding, Henghui and Jiang, Xudong and Shuai, Bing and Liu, Ai Qun and Wang, Gang},
eprint = {1909.02651v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Ding et al. - Unknown - Semantic Correlation Promoted Shape-Variant Context for Segmentation.pdf:pdf},
title = {{Semantic Correlation Promoted Shape-Variant Context for Segmentation}}
}
@techreport{Hea,
abstract = {Many successful learning targets such as minimizing dice loss and cross-entropy loss have enabled unprecedented breakthroughs in segmentation tasks. Beyond these semantic metrics, this paper aims to introduce location supervision into semantic segmentation. Based on this idea, we present a Location-aware Upsampling (LaU) that adaptively refines the interpolating coordinates with trainable offsets. Then, location-aware losses are established by encouraging pixels to move towards well-classified locations. An LaU is offset prediction coupled with interpolation, which is trained end-to-end to generate confidence score at each position from coarse to fine. Guided by location-aware losses, the new module can replace its plain counterpart (e.g., bilinear upsam-pling) in a plug-and-play manner to further boost the leading encoder-decoder approaches. Extensive experiments validate the consistent improvement over the state-of-the-art methods on benchmark datasets. Our code is available at https://github.com/HolmesShuan/Location-aware-Upsampling-for-Semantic-Segmentation.},
archivePrefix = {arXiv},
arxivId = {1911.05250v2},
author = {He, Xiangyu and Mo, Zitao and Chen, Qiang and Cheng, Anda and Wang, Peisong and Cheng, Jian},
eprint = {1911.05250v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Location-aware Upsampling for Semantic Segmentation.pdf:pdf},
title = {{Location-aware Upsampling for Semantic Segmentation}},
url = {https://github.com/HolmesShuan/Location-aware-}
}
@techreport{Wang,
abstract = {High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions in series (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams in parallel; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at https://github.com/HRNet.},
archivePrefix = {arXiv},
arxivId = {1908.07919v2},
author = {Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and Liu, Wenyu and Xiao, Bin},
eprint = {1908.07919v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, MARCH 2020 Deep High-Resolution Representation L.pdf:pdf},
keywords = {Index Terms-HRNet,high-resolution representations,human pose estimation,low-resolution representations,object detection,semantic segmentation},
title = {{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, MARCH 2020 Deep High-Resolution Representation Learning for Visual Recognition}},
url = {https://github.com/HRNet.}
}
@techreport{Yuana,
abstract = {Automatic building extraction from aerial and satellite imagery is highly challenging due to extremely large variations of building appearances. To attack this problem, we design a convolutional network with a final stage that integrates activations from multiple preceding stages for pixel-wise prediction , and introduce the signed distance function of building boundaries as the output representation, which has an enhanced representation power. We leverage abundant building footprint data available from geographic information systems (GIS) to compile training data. The trained network achieves superior performance on datasets that are significantly larger and more complex than those used in prior work, demonstrating that the proposed method provides a promising and scalable solution for automating this labor-intensive task.},
archivePrefix = {arXiv},
arxivId = {1602.06564v1},
author = {Yuan, Jiangye},
eprint = {1602.06564v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Yuan - Unknown - Automatic Building Extraction in Aerial Scenes Using Convolutional Networks.pdf:pdf},
title = {{Automatic Building Extraction in Aerial Scenes Using Convolutional Networks}},
url = {http://www.openstreetmap.org}
}
@techreport{Zhao,
abstract = {Most semantic segmentation models treat semantic seg-mentation as a pixel-wise classification task and use a pixel-wise classification error as their optimization criterions. However, the pixel-wise error ignores the strong dependencies among the pixels in an image, which limits the performance of the model. Several ways to incorporate the structure information of the objects have been investigated, e.g., conditional random fields (CRF), image structure priors based methods, and generative adversarial network (GAN). Nevertheless, these methods usually require extra model branches or additional memories, and some of them show limited improvements. In contrast, we propose a simple yet effective structural similarity loss (SSL) to encode the structure information of the objects, which only requires a few additional computational resources in the training phase. Inspired by the widely-used structural similarity (SSIM) index in image quality assessment, we use the linear correlation between two images to quantify their structural similarity. And the goal of the proposed SSL is to pay more attention to the positions, whose associated predictions lead to a low degree of linear correlation between two corresponding regions in the ground truth map and the predicted map. Thus the model can achieve a strong structural similarity between the two maps through minimizing the SSL over the whole map. The experimental results demonstrate that our method can achieve substantial and consistent improvements in performance on the PASCAL VOC 2012 and Cityscapes datasets. The code will be released soon.},
archivePrefix = {arXiv},
arxivId = {1910.08711v1},
author = {Zhao, Shuai and Wu, Boxi and Chu, Wenqing and Hu, Yao and Cai, Deng},
eprint = {1910.08711v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Zhao et al. - Unknown - Correlation Maximized Structural Similarity Loss for Semantic Segmentation.pdf:pdf},
title = {{Correlation Maximized Structural Similarity Loss for Semantic Segmentation}}
}
@techreport{Chenb,
abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\%} and 82.1{\%} without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https: //github.com/tensorflow/models/tree/master/research/deeplab.},
archivePrefix = {arXiv},
arxivId = {1802.02611v3},
author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
eprint = {1802.02611v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation.pdf:pdf},
keywords = {Semantic image segmentation,and depthwise separable convolution,encoder-decoder,spatial pyramid pooling},
title = {{Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}},
url = {https://github.com/tensorflow/models/tree/master/}
}
@techreport{Chenc,
abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolu-tional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed 'DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
annote = {Atrous convolution
Deeplab },
archivePrefix = {arXiv},
arxivId = {1706.05587v3},
author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
eprint = {1706.05587v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - Rethinking Atrous Convolution for Semantic Image Segmentation.pdf:pdf},
title = {{Rethinking Atrous Convolution for Semantic Image Segmentation}}
}
@article{Zhang2016a,
author = {Zhang, Liangpei and Xia, Gui Song and Wu, Tianfu and Lin, Liang and Tai, Xue Cheng},
doi = {10.1155/2016/7954154},
file = {:Users/philipeborba/Downloads/07486259.pdf:pdf},
issn = {16877268},
journal = {Journal of Sensors},
number = {june},
title = {{Deep Learning for Remote Sensing Image Understanding}},
volume = {2016},
year = {2016}
}
@article{Lin2018,
abstract = {We propose an approach for exploiting contextual information in semantic image segmentation, and particularly investigate the use of patch-patch context and patch-background context in deep CNNs. We formulate deep structured models by combining CNNs and Conditional Random Fields (CRFs) for learning the patch-patch context between image regions. Specifically, we formulate CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied in order to avoid repeated expensive CRF inference during the course of back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image inputs and sliding pyramid pooling is very effective for improving performance. We perform comprehensive evaluation of the proposed method. We achieve new state-of-the-art performance on a number of challenging semantic segmentation datasets.},
archivePrefix = {arXiv},
arxivId = {1603.03183},
author = {Lin, Guosheng and Shen, Chunhua and {Van Den Hengel}, Anton and Reid, Ian},
doi = {10.1109/TPAMI.2017.2708714},
eprint = {1603.03183},
file = {:Users/philipeborba/Downloads/07934393.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Semantic segmentation,conditional random fields,contextual models,convolutional neural networks},
number = {6},
pages = {1352--1366},
publisher = {IEEE},
title = {{Exploring Context with Deep Structured Models for Semantic Segmentation}},
volume = {40},
year = {2018}
}
@article{Yu2018,
abstract = {Communicated by XIANG Xiang Bai Keywords: Semantic segmentation Convolutional neural network Markov random fields Weakly supervised method 3D point clouds labeling a b s t r a c t Semantic segmentation, also called scene labeling, refers to the process of assigning a semantic label (e.g. car, people, and road) to each pixel of an image. It is an essential data processing step for robots and other unmanned systems to understand the surrounding scene. Despite decades of effort s, semantic segmentation is still a very challenging task due to large variations in natural scenes. In this paper, we provide a systematic review of recent advances in this field. In particular, three categories of methods are reviewed and compared, including those based on hand-engineered features, learned features and weakly supervised learning. In addition, we describe a number of popular datasets aiming for facilitating the development of new segmentation algorithms. In order to demonstrate the advantages and disadvantages of different semantic segmentation models, we conduct a series of comparisons between them. Deep discussions about the comparisons are also provided. Finally, this review is concluded by discussing future directions and challenges in this important field of research.},
author = {Yu, Hongshan and Yang, Zhengeng and Tan, Lei and Wang, Yaonan and Sun, Wei and Sun, Mingui and Tang, Yandong},
doi = {10.1016/j.neucom.2018.03.037},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2018 - Methods and datasets on semantic segmentation A review.pdf:pdf},
journal = {Neurocomputing},
pages = {82--103},
title = {{Methods and datasets on semantic segmentation: A review}},
url = {https://doi.org/10.1016/j.neucom.2018.03.037},
volume = {304},
year = {2018}
}
@techreport{Yuanb,
abstract = {In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second , we compute the object region representation by aggre-gating the representations of the pixels lying in the object region. Last, we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff.},
archivePrefix = {arXiv},
arxivId = {1909.11065v2},
author = {Yuan, Yuhui and Chen, Xilin and Wang, Jingdong},
eprint = {1909.11065v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Yuan, Chen, Wang - Unknown - Object-Contextual Representations for Semantic Segmentation.pdf:pdf},
title = {{Object-Contextual Representations for Semantic Segmentation}}
}
@techreport{Heb,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385v1},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://image-net.org/challenges/LSVRC/2015/}
}
@techreport{Tan,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https: //github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.},
archivePrefix = {arXiv},
arxivId = {1905.11946v3},
author = {Tan, Mingxing and Le, Quoc V},
eprint = {1905.11946v3},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Tan, Le - Unknown - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf:pdf},
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}}
}
@techreport{Tana,
abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neu-ral network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations , we have developed a new family of object detectors , called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single scale, our EfficientDet-D6 achieves state-of-the-art 50.9 mAP on COCO dataset with 52M parameters and 229B FLOPs 1 , being 4x smaller and using 13x fewer FLOPs yet still more accurate (+0.2{\%} mAP) than the best previous detector. Code is available at https://github.com/google/ automl/tree/master/efficientdet.},
archivePrefix = {arXiv},
arxivId = {1911.09070v2},
author = {Tan, Mingxing},
eprint = {1911.09070v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Tan - Unknown - EfficientDet Scalable and Efficient Object Detection.pdf:pdf},
title = {{EfficientDet: Scalable and Efficient Object Detection}},
url = {https://github.com/google/}
}
@article{Shakirov2018,
abstract = {The current state-of-the-art in Deep Learning (DL) based artificial intelligence (AI) is reviewed. A special emphasis is made to compare the level of a concrete AI system with human abilities to show what remains to be done to achieve human level AI. Several estimates are proposed for comparison of the current "intellectual level" of AI systems with the human level. Among them is relation of Shannon's estimate for lower bound on human word perplexity to recent progress in natural language AI modeling. Relations between the operation of DL constructions and principles of live neural information processing are discussed. The problem of AI risks and benefits is also reviewed based on arguments from both sides.},
author = {Shakirov, V V and Solovyeva, K P and Dunin-Barkowski, W L},
doi = {10.3103/S1060992X18020066},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Shakirov, Solovyeva, Dunin-Barkowski - 2018 - Review of State-of-the-Art in Deep Learning Artificial Intelligence.pdf:pdf},
journal = {Optical Memory and Neural Networks},
keywords = {Pavlov Principle,artificial intelligence,deep learning,gen-erative adversarial networks,natural language processing,residual networks},
number = {2},
pages = {65--80},
publisher = {{\textcopyright} Allerton Press, Inc},
title = {{Review of State-of-the-Art in Deep Learning Artificial Intelligence}},
volume = {27},
year = {2018}
}
@article{Griffiths2019,
abstract = {Robust and reliable automatic building detection and segmentation from aerial images/point clouds has been a prominent field of research in remote sensing, computer vision and point cloud processing for a number of decades. One of the largest issues associated with deep learning methods is the high quantity of data required for training. To help address this we present a method to improve public GIS building footprint labels by using Morphological Geodesic Active Contours (MorphGACs). We demonstrate by improving the quality of building footprint labels for detection and semantic segmentation, more robust and reliable models can be obtained. We evaluate these methods over a large UK-based dataset of 24556 images containing 169835 building instances. This is achieved by training several Mask/Faster R-CNN and RetinaNet deep convolutional neural networks. Networks are supplied with both RGB and fused RGB-lidar data. We offer quantitative analysis on the benefits of the inclusion of depth data for building segmentation. By employing both methods we achieve a detection accuracy of 0.92 (mAP@0.5) and segmentation f1 scores of 0.94 over a 4911 test images ranging from urban to rural scenes.},
author = {Griffiths, David and Boehm, Jan},
doi = {10.1016/j.isprsjprs.2019.05.013},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Griffiths, Boehm - 2019 - Improving public data for building segmentation from Convolutional Neural Networks (CNNs) for fused airborne l.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Aerial,Convolutional neural networks,Deep learning,Image processing,Lidar,Segmentation},
number = {May},
pages = {70--83},
publisher = {Elsevier},
title = {{Improving public data for building segmentation from Convolutional Neural Networks (CNNs) for fused airborne lidar and image data using active contours}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.05.013},
volume = {154},
year = {2019}
}
@techreport{HendrikMetzen,
abstract = {While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversar-ial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification , this work proposes attacks against semantic image seg-mentation: we present an approach for generating (univer-sal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the seg-mentation mostly unchanged otherwise.},
author = {{Hendrik Metzen}, Jan and {Chaithanya Kumar}, Mummadi and Brox, Thomas and Fischer, Volker},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Hendrik Metzen et al. - Unknown - Universal Adversarial Perturbations Against Semantic Image Segmentation.pdf:pdf},
title = {{Universal Adversarial Perturbations Against Semantic Image Segmentation}}
}
@techreport{Chenga,
abstract = {This paper proposes an end-to-end trainable network, SegFlow, for simultaneously predicting pixel-wise object segmentation and optical flow in videos. The proposed SegFlow has two branches where useful information of object segmentation and optical flow is propagated bi-directionally in a unified framework. The segmentation branch is based on a fully convolutional network, which has been proved effective in image segmentation task, and the optical flow branch takes advantage of the FlowNet model. The unified framework is trained iteratively offline to learn a generic notion, and fine-tuned online for specific objects. Extensive experiments on both the video object segmenta-tion and optical flow datasets demonstrate that introducing optical flow improves the performance of segmentation and vice versa, against the state-of-the-art algorithms.},
author = {Cheng, Jingchun and Tsai, Yi-Hsuan and Wang, Shengjin and Yang, Ming-Hsuan},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Cheng et al. - Unknown - SegFlow Joint Learning for Video Object Segmentation and Optical Flow.pdf:pdf},
title = {{SegFlow: Joint Learning for Video Object Segmentation and Optical Flow}}
}
@techreport{Luc,
abstract = {The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmenta-tion maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregres-sive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.},
author = {Luc, Pauline and Neverova, Natalia and Couprie, Camille and Verbeek, Jakob and Lecun, Yann and Research, Facebook Ai},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Luc et al. - Unknown - Predicting Deeper into the Future of Semantic Segmentation.pdf:pdf},
title = {{Predicting Deeper into the Future of Semantic Segmentation}}
}
@techreport{Tokmakov,
abstract = {This paper addresses the task of segmenting moving objects in unconstrained videos. We introduce a novel two-stream neural network with an explicit memory module to achieve this. The two streams of the network encode spatial and temporal features in a video sequence respectively , while the memory module captures the evolution of objects over time. The module to build a "visual memory" in video, i.e., a joint representation of all the video frames, is realized with a convolutional recurrent unit learned from a small number of training video sequences. Given a video frame as input, our approach assigns each pixel an object or background label based on the learned spatio-temporal features as well as the "visual memory" specific to the video, acquired automatically without any manually-annotated frames. The visual memory is implemented with convolutional gated recurrent units, which allows to propagate spatial information over time. We evaluate our method extensively on two benchmarks, DAVIS and Freiburg-Berkeley motion segmentation datasets, and show state-of-the-art results. For example, our approach outper-forms the top method on the DAVIS dataset by nearly 6{\%}. We also provide an extensive ablative analysis to investigate the influence of each component in the proposed framework.},
author = {Tokmakov, Pavel and {Alahari Inria}, Karteek and Schmid, Cordelia},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Tokmakov, Alahari Inria, Schmid - Unknown - Learning Video Object Segmentation with Visual Memory.pdf:pdf},
title = {{Learning Video Object Segmentation with Visual Memory}}
}
@techreport{Harley,
abstract = {We introduce an approach to integrate segmentation information within a convolutional neural network (CNN). This counteracts the tendency of CNNs to smooth information across regions and increases their spatial precision. To obtain segmentation information, we set up a CNN to provide an embedding space where region co-membership can be estimated based on Euclidean distance. We use these embeddings to compute a local attention mask relative to every neuron position. We incorporate such masks in CNNs and replace the convolution operation with a "segmentation-aware" variant that allows a neuron to selectively attend to inputs coming from its own region. We call the resulting network a segmentation-aware CNN because it adapts its filters at each image point according to local segmentation cues, while at the same time remaining fully-convolutional. We demonstrate the merit of our method on two widely different dense prediction tasks, that involve classification (semantic segmentation) and regression (optical flow). Our results show that in semantic seg-mentation we can replace DenseCRF inference with a cascade of segmentation-aware filters, and in optical flow we obtain clearly sharper responses than the ones obtained with comparable networks that do not use segmentation. In both cases segmentation-aware convolution yields systematic improvements over strong baselines.},
author = {Harley, Adam W and Derpanis, Konstantinos G and Kokkinos, Iasonas},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Harley, Derpanis, Kokkinos - Unknown - Segmentation-Aware Convolutional Networks Using Local Attention Masks.pdf:pdf},
title = {{Segmentation-Aware Convolutional Networks Using Local Attention Masks}}
}
@techreport{Hec,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Mask R-CNN.pdf:pdf},
title = {{Mask R-CNN}}
}
@techreport{Sultana,
abstract = {From the autonomous car driving to medical diagnosis, the requirement of the task of image segmentation is everywhere. Segmentation of an image is one of the indispensable tasks in computer vision. This task is comparatively complicated than other vision tasks as it needs low-level spatial information. Basically, image segmentation can be of two types: semantic segmentation and instance segmentation. The combined version of these two basic tasks is known as panoptic segmentation. In the recent era, the success of deep convolutional neural network (CNN) has influenced the field of segmentation greatly and gave us various successful models to date. In this survey, we are going to take a glance at the evolution of both semantic and instance segmen-tation work based on CNN. We have also specified comparative architectural details of some state-of-the-art models and discuss their training details to present a lucid understanding of hyper-parameter tuning of those models. Lastly, we have drawn a comparison among the performance of those models on different datasets.},
archivePrefix = {arXiv},
arxivId = {2001.04074v2},
author = {Sultana, Farhana and Sufian, Abu and Dutta, Paramartha},
eprint = {2001.04074v2},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Sultana, Sufian, Dutta - Unknown - Evolution of Image Segmentation using Deep Convolutional Neural Network A Survey.pdf:pdf},
keywords = {Convolutional Neural Network,Deep Learning,Instance Segmentation,Panoptic Segmentation,Semantic Segmentation,Survey},
title = {{Evolution of Image Segmentation using Deep Convolutional Neural Network: A Survey}}
}
@article{Maltezos2017,
abstract = {{\textcopyright} 2017 Society of Photo-Optical Instrumentation Engineers (SPIE). Automatic extraction of buildings from remote sensing data is an attractive research topic, useful for several applications, such as cadastre and urban planning. This is mainly due to the inherent artifacts of the used data and the differences in viewpoint, surrounding environment, and complex shape and size of the buildings. This paper introduces an efficient deep learning framework based on convolutional neural networks (CNNs) toward building extraction from orthoimages. In contrast to conventional deep approaches in which the raw image data are fed as input to the deep neural network, in this paper the height information is exploited as an additional feature being derived from the application of a dense image matching algorithm. As test sites, several complex urban regions of various types of buildings, pixel resolutions and types of data are used, located in Vaihingen in Germany and in Perissa in Greece. Our method is evaluated using the rates of completeness, correctness, and quality and compared with conventional and other "shallow" learning paradigms such as support vector machines. Experimental results indicate that a combination of raw image data with height information, feeding as input to a deep CNN model, provides potentials in building detection in terms of robustness, flexibility, and efficiency.},
author = {Maltezos, Evangelos and Doulamis, Nikolaos and Doulamis, Anastasios and Ioannidis, Charalabos},
doi = {10.1117/1.jrs.11.042620},
file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Maltezos et al. - 2017 - Deep convolutional neural networks for building extraction from orthoimages and dense image matching point clou.pdf:pdf},
issn = {1931-3195},
journal = {Journal of Applied Remote Sensing},
month = {dec},
number = {04},
pages = {1},
publisher = {SPIE-Intl Soc Optical Eng},
title = {{Deep convolutional neural networks for building extraction from orthoimages and dense image matching point clouds}},
volume = {11},
year = {2017}
}
@article{Rodriguez2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.07091v2},
author = {Rodriguez, Andres C and Wegner, Jan D},
eprint = {arXiv:1809.07091v2},
file = {:Users/philipeborba/Downloads/1809.07091.pdf:pdf},
keywords = {computer vision,density estimation,remote sensing},
title = {density estimation from space},
year = {2018}
}
@inproceedings{Audebert2017,
abstract = {In this work, we investigate the use of OpenStreetMap data for semantic labeling of Earth Observation images. Deep neural networks have been used in the past for remote sensing data classification from various sensors, including multispectral, hyperspectral, SAR and LiDAR data. While OpenStreetMap has already been used as ground truth data for training such networks, this abundant data source remains rarely exploited as an input information layer. In this paper, we study different use cases and deep network architectures to leverage OpenStreetMap data for semantic labeling of aerial and satellite images. Especially, we look into fusion based architectures and coarseto- fine segmentation to include the OpenStreetMap layer into multispectral-based deep fully convolutional networks. We illustrate how these methods can be successfully used on two public datasets: ISPRS Potsdam and DFC2017. We show that OpenStreetMap data can efficiently be integrated into the vision-based deep learning models and that it significantly improves both the accuracy performance and the convergence speed of the networks.},
archivePrefix = {arXiv},
arxivId = {1705.06057},
author = {Audebert, Nicolas and Saux, Bertrand Le and Lefevre, Sebastien},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2017.199},
eprint = {1705.06057},
file = {:Users/philipeborba/Downloads/1705.06057.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
pages = {1552--1560},
title = {{Joint Learning from Earth Observation and OpenStreetMap Data to Get Faster Better Semantic Maps}},
volume = {2017-July},
year = {2017}
}
@inproceedings{Hu2020,
abstract = {In recent years, with the development of aerospace technology, we use more and more images captured by satellites to obtain information. But a large number of useless raw images, limited data storage resource and poor transmission capability on satellites hinder our use of valuable images. Therefore, it is necessary to deploy an on-orbit semantic segmentation model to filter out useless images before data transmission. In this paper, we present a detailed comparison on the recent deep learning models. Considering the computing environment of satellites, we compare methods from accuracy, parameters and resource consumption on the same public dataset. And we also analyze the relation between them. Based on experimental results, we further propose a viable on-orbit semantic segmentation strategy. It will be deployed on the TianZhi-2 satellite which supports deep learning methods and will be lunched soon.},
archivePrefix = {arXiv},
arxivId = {1905.10231},
author = {Hu, Junxing and Li, Ling and Lin, Yijun and Wu, Fengge and Zhao, Junsuo},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-030-32456-8_3},
eprint = {1905.10231},
file = {:Users/philipeborba/Downloads/1905.10231.pdf:pdf},
isbn = {9783030324551},
issn = {21945365},
keywords = {Deep learning,Remote sensing images,Semantic segmentation},
pages = {21--29},
title = {{A Comparison and Strategy of Semantic Segmentation on Remote Sensing Images}},
volume = {1074},
year = {2020}
}
@inproceedings{Howe2019,
abstract = {The difficulty in obtaining labeled data relevant to a given task is among the most common and well-known practical obstacles to applying deep learning techniques to new or even slightly modified domains. The data volumes required by the current generation of supervised learning algorithms typically far exceed what a human needs to learn and complete a given task. We investigate ways to expand a given labeled corpus of remote sensed imagery into a larger corpus using Generative Adversarial Networks (GANs). We then measure how these additional synthetic data affect supervised machine learning performance on an object detection task. Our data driven strategy is to train GANs to (1) generate synthetic segmentation masks and (2) generate plausible synthetic remote sensing imagery corresponding to these segmentation masks. Run sequentially, these GANs allow the generation of synthetic remote sensing imagery complete with segmentation labels. We apply this strategy to the data set from ISPRS' 2D Semantic Labeling Contest - Potsdam, with a follow on vehicle detection task. We find that in scenarios with limited training data, augmenting the available data with such synthetically generated data can improve detector performance.},
archivePrefix = {arXiv},
arxivId = {1908.03809},
author = {Howe, Jonathan and Pula, Kyle and Reite, Aaron A},
doi = {10.1117/12.2529586},
eprint = {1908.03809},
file = {:Users/philipeborba/Downloads/1908.03809.pdf:pdf},
isbn = {9781510629714},
issn = {1996756X},
keywords = {deep learning,generative adversarial net-,object detection,remote sensing,synthetic data},
pages = {13},
title = {{Conditional generative adversarial networks for data augmentation and adaptation in remotely sensed imagery}},
year = {2019}
}
@article{Mou,
archivePrefix = {arXiv},
arxivId = {arXiv:1805.02091v1},
author = {Mou, Lichao and Xiang, Xiao},
eprint = {arXiv:1805.02091v1},
file = {:Users/philipeborba/Downloads/1805.02091.pdf:pdf},
title = {{RiFCN : Recurrent Network in Fully Convolutional Network for Semantic Segmentation of High Resolution Remote Sensing Images}}
}
@inproceedings{Liu2018,
abstract = {Semantic mapping of land cover is a key, but challenging, problem in remote sensing. Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have shown outstanding performance in this task. In order to develop refined deep learning pipeline for meeting the rising need for accurate semantic mapping in remote sensing images, this paper study and compare a number of advanced deep learning segmentation architectures, which have obtained state-of-the-art results on computer vision contests like the Pascal VOC. To further analyze and compare the effectiveness of some elaborate layers and underlying structures introduced by these architectures, we evaluate them by re-implementing, train and test them on ISPRS Potsdam dataset. Our results show that a promising performance with overall F1 score above 87{\%} and mIoU of 79{\%} can be obtained by only using the RGB images, without any post-processing such as conditional random field (CRF) smoothing. At last, we propose several possible approaches to further enhance the deep learning architectures to better deal with high-resolution aerial images. We therefore consider this work to be helpful for the remote sensing research community.},
author = {Liu, Qinghui and Salberg, Arnt B{\o}rre and Jenssen, Robert},
booktitle = {International Geoscience and Remote Sensing Symposium (IGARSS)},
doi = {10.1109/IGARSS.2018.8518533},
file = {:Users/philipeborba/Downloads/liu2018-igarss{\_}paper.pdf:pdf},
isbn = {9781538671504},
pages = {6943--6946},
title = {{A comparison of deep learning architectures for semantic mapping of very high resolution images}},
volume = {2018-July},
year = {2018}
}
@inproceedings{Audebert2016,
abstract = {As computer vision before, remote sensing has been radically changed by the introduction of Convolution Neural Networks. Land cover use, object detection and scene understanding in aerial images rely more and more on deep learning to achieve new state-of-the-art results. Recent architectures such as Fully Convolutional Networks (Long et al., 2015) can even produce pixel level annotations for semantic mapping. In this work, we show how to use such deep networks to detect, segment and classify different varieties of wheeled vehicles in aerial images from the ISPRS Potsdam dataset. This allows us to tackle object detection and classification on a complex dataset made up of visually similar classes, and to demonstrate the relevance of such a subclass modeling approach. Especially, we want to show that deep learning is also suitable for object-oriented analysis of Earth Observation data. First, we train a FCN variant on the ISPRS Potsdam dataset and show how the learnt semantic maps can be used to extract precise segmentation of vehicles, which allow us studying the repartition of vehicles in the city. Second, we train a CNN to perform vehicle classification on the VEDAI (Razakarivony and Jurie, 2016) dataset, and transfer its knowledge to classify candidate segmented vehicles on the Potsdam dataset.},
archivePrefix = {arXiv},
arxivId = {1609.06845},
author = {Audebert, Nicolas and {Le Saux}, Bertrand and Lefevre, Sebastien},
doi = {10.3990/2.399},
eprint = {1609.06845},
file = {:Users/philipeborba/Downloads/1609.06845.pdf:pdf},
keywords = {deep learning,object classification,semantic segmentation,vehicle detection},
title = {{On the usability of deep networks for object-based image analysis}},
year = {2016}
}
@inproceedings{Liu2017,
abstract = {Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN)for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the object's class efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods.},
archivePrefix = {arXiv},
arxivId = {1707.06783},
author = {Liu, Fangyu and Li, Shuaipeng and Zhang, Liqiang and Zhou, Chenghu and Ye, Rongtian and Wang, Yuebin and Lu, Jiwen},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.605},
eprint = {1707.06783},
file = {:Users/philipeborba/Downloads/1707.06783.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
pages = {5679--5688},
title = {{3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds}},
volume = {2017-Octob},
year = {2017}
}
@article{Yue2018,
abstract = {For the task of subdecimeter aerial imagery segmentation, fine-grained semantic segmentation results are usually difficult to obtain because of complex remote sensing content and optical conditions. Recently, convolutional neural networks (CNNs) have shown outstanding performance on this task. Although many deep neural network structures and techniques have been applied to improve the accuracy, few have paid attention to better differentiating the easily confused classes. In this paper, we propose TreeSegNet which adopts an adaptive network to increase the classification rate at the pixelwise level. Specifically, based on the infrastructure of DeepUNet, a Tree-CNN block in which each node represents a ResNeXt unit is constructed adaptively according to the confusion matrix and the proposed TreeCutting algorithm. By transporting feature maps through concatenating connections, the Tree-CNN block fuses multiscale features and learns best weights for the model. In experiments on the ISPRS 2D semantic labeling Potsdam dataset, the results obtained by TreeSegNet are better than those of other  Corresponding author. TreeSegNet: Adaptive Tree CNNs for Subdecimeter Aerial Image Segmentation 2 published state-of-the-art methods. Detailed comparison and analysis show that the improvement brought by the adaptive Tree-CNN block is significant.},
author = {Yue, Kai and Yang, Lei and Li, Ruirui and Hu, Wei and Zhang, Fan and Li, Wei},
file = {:Users/philipeborba/Downloads/1804.10879.pdf:pdf},
keywords = {CNN,ISPRS,adaptive network,aerial imagery,semantic segmentation,tree structures},
pages = {1--33},
title = {{TreeSegNet : Automatically Constructed Tree CNNs for Subdecimeter Aerial Image Segmentation}},
url = {https://arxiv.org/pdf/1804.10879v2.pdf},
year = {2018}
}
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:Users/philipeborba/Downloads/1505.04597.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
pages = {234--241},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
volume = {9351},
year = {2015}
}
@article{Liu2018b,
author = {Liu, Xiaolong and Deng, Zhidong and Yang, Yuhan},
doi = {10.1007/s10462-018-9641-3},
file = {:Users/philipeborba/Downloads/1809.10198.pdf:pdf},
issn = {1573-7462},
journal = {Artificial Intelligence Review},
keywords = {Image semantic segmentation,DNN,CNN,FCN,cnn,dnn,fcn,image semantic segmentation},
publisher = {Springer Netherlands},
title = {{Recent progress in semantic image segmentation}},
url = {https://doi.org/10.1007/s10462-018-9641-3},
year = {2018}
}
@article{He2019,
abstract = {Automatic building extraction using a single data type, either 2D remotely-sensed images or light detection and ranging 3D point clouds, remains insufficient to accurately delineate building outlines for automatic mapping, despite active research in this area and the significant progress which has been achieved in the past decade. This paper presents an effective approach to extracting buildings from Unmanned Aerial Vehicle (UAV) images through the incorporation of superpixel segmentation and semantic recognition. A framework for building extraction is constructed by jointly using an improved Simple Linear Iterative Clustering (SLIC) algorithm and Multiscale Siamese Convolutional Networks (MSCNs). The SLIC algorithm, improved by additionally imposing a digital surface model for superpixel segmentation, namely 6D-SLIC, is suited for building boundary detection under building and image backgrounds with similar radiometric signatures. The proposed MSCNs, including a feature learning network and a binary decision network, are used to automatically learn a multiscale hierarchical feature representation and detect building objects under various complex backgrounds. In addition, a gamma-transform green leaf index is proposed to truncate vegetation superpixels for further processing to improve the robustness and efficiency of building detection, the Douglas-Peucker algorithm and iterative optimization are used to eliminate jagged details generated from small structures as a result of superpixel segmentation. In the experiments, the UAV datasets, including many buildings in urban and rural areas with irregular shapes and different heights and that are obscured by trees, are collected to evaluate the proposed method. The experimental results based on the qualitative and quantitative measures confirm the effectiveness and high accuracy of the proposed framework relative to the digitized results. The proposed framework performs better than state-of-the-art building extraction methods, given its higher values of recall, precision, and intersection over Union (IoU).},
author = {He, Haiqing and Zhou, Junchao and Chen, Min and Chen, Ting and Li, Dajun and Cheng, Penggen},
doi = {10.3390/rs11091040},
file = {:Users/philipeborba/Downloads/remotesensing-11-01040.pdf:pdf},
isbn = {8618146625},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Binary decision network,Building extraction,Multiscale Siamese convolutional networks (MSCNs),Simple linear iterative clustering (SLIC),Unmanned aerial vehicle (UAV)},
number = {9},
pages = {1--33},
title = {{Building extraction from UAV images jointly using 6D-SLIC and Multiscale Siamese Convolutional Networks}},
volume = {11},
year = {2019}
}
@article{Pan2019,
abstract = {Segmentation of high-resolution remote sensing images is an important challenge with wide practical applications. The increasing spatial resolution provides fine details for image segmentation but also incurs segmentation ambiguities. In this paper, we propose a generative adversarial network with spatial and channel attention mechanisms (GAN-SCA) for the robust segmentation of buildings in remote sensing images. The segmentation network (generator) of the proposed framework is composed of the well-known semantic segmentation architecture (U-Net) and the spatial and channel attention mechanisms (SCA). The adoption of SCA enables the segmentation network to selectively enhance more useful features in specific positions and channels and enables improved results closer to the ground truth. The discriminator is an adversarial network with channel attention mechanisms that can properly discriminate the outputs of the generator and the ground truth maps. The segmentation network and adversarial network are trained in an alternating fashion on the Inria aerial image labeling dataset and Massachusetts buildings dataset. Experimental results show that the proposed GAN-SCA achieves a higher score (the overall accuracy and intersection over the union of Inria aerial image labeling dataset are 96.61{\%} and 77.75{\%}, respectively, and the F1-measure of the Massachusetts buildings dataset is 96.36{\%}) and outperforms several state-of-the-art approaches.},
author = {Pan, Xuran and Yang, Fan and Gao, Lianru and Chen, Zhengchao and Zhang, Bing and Fan, Hairui and Ren, Jinchang},
doi = {10.3390/rs11080966},
file = {:Users/philipeborba/Downloads/remotesensing-11-00917.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Deep learning,Generative adversarial network,High-resolution aerial images,Inria aerial image labeling dataset,Massachusetts buildings dataset,Semantic segmentation},
number = {8},
pages = {1--18},
title = {{Building extraction from high-resolution aerial imagery using a generative adversarial network with spatial and channel attention mechanisms}},
volume = {11},
year = {2019}
}
@article{Yang2018,
abstract = {Building extraction from very high resolution (VHR) imagery plays an important role in urban planning, disaster management, navigation, updating geographic databases, and several other geospatial applications. Compared with the traditional building extraction approaches, deep learning networks have recently shown outstanding performance in this task by using both high-level and low-level feature maps. However, it is difficult to utilize different level features rationally with the present deep learning networks. To tackle this problem, a novel network based on DenseNets and the attention mechanism was proposed, called the dense-attention network (DAN). The DAN contains an encoder part and a decoder part which are separately composed of lightweight DenseNets and a spatial attention fusion module. The proposed encoder-decoder architecture can strengthen feature propagation and effectively bring higher-level feature information to suppress the low-level feature and noises. Experimental results based on public international society for photogrammetry and remote sensing (ISPRS) datasets with only red-green-blue (RGB) images demonstrated that the proposed DAN achieved a higher score (96.16{\%} overall accuracy (OA), 92.56{\%} F1 score, 90.56{\%} mean intersection over union (MIOU), less training and response time and higher-quality value) when compared with other deep learning methods.},
author = {Yang, Hui and Wu, Penghai and Yao, Xuedong and Wu, Yanlan and Wang, Biao and Xu, Yongyang},
doi = {10.3390/rs10111768},
file = {:Users/philipeborba/Downloads/remotesensing-10-01768.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Attention mechanism,Building extraction,Deep learning,Imagery,Very high resolution},
number = {11},
pages = {1--16},
title = {{Building extraction in very high resolution imagery by dense-attention networks}},
volume = {10},
year = {2018}
}
@article{Audebert2018,
abstract = {In this work, we investigate various methods to deal with semantic labeling of very high resolution multi-modal remote sensing data. Especially, we study how deep fully convolutional networks can be adapted to deal with multi-modal and multi-scale remote sensing data for semantic labeling. Our contributions are threefold: (a) we present an efficient multi-scale approach to leverage both a large spatial context and the high resolution data, (b) we investigate early and late fusion of Lidar and multispectral data, (c) we validate our methods on two public datasets with state-of-the-art results. Our results indicate that late fusion make it possible to recover errors steaming from ambiguous data, while early fusion allows for better joint-feature learning but at the cost of higher sensitivity to missing data.},
archivePrefix = {arXiv},
arxivId = {1711.08681},
author = {Audebert, Nicolas and {Le Saux}, Bertrand and Lef{\`{e}}vre, S{\'{e}}bastien},
doi = {10.1016/j.isprsjprs.2017.11.011},
eprint = {1711.08681},
file = {:Users/philipeborba/Downloads/1711.08681.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Data fusion,Deep learning,Remote sensing,Semantic mapping},
pages = {20--32},
title = {{Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks}},
volume = {140},
year = {2018}
}
@article{Caesar,
author = {Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
file = {:Users/philipeborba/Downloads/caesar16eccv.pdf:pdf},
pages = {20--22},
title = {{Region-based semantic segmentation with end-to-end training}}
}
@inproceedings{Lin2017a,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1612.03144},
author = {Lin, Tsung Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.106},
eprint = {1612.03144},
file = {:Users/philipeborba/Downloads/1612.03144.pdf:pdf},
isbn = {9781538604571},
pages = {936--944},
title = {{Feature pyramid networks for object detection}},
volume = {2017-Janua},
year = {2017}
}
@article{Sun2019,
abstract = {High-resolution representation learning plays an essential role in many vision problems, e.g., pose estimation and semantic segmentation. The high-resolution network (HRNet){\~{}}$\backslash$cite{\{}SunXLW19{\}}, recently developed for human pose estimation, maintains high-resolution representations through the whole process by connecting high-to-low resolution convolutions in $\backslash$emph{\{}parallel{\}} and produces strong high-resolution representations by repeatedly conducting fusions across parallel convolutions. In this paper, we conduct a further study on high-resolution representations by introducing a simple yet effective modification and apply it to a wide range of vision tasks. We augment the high-resolution representation by aggregating the (upsampled) representations from all the parallel convolutions rather than only the representation from the high-resolution convolution as done in{\~{}}$\backslash$cite{\{}SunXLW19{\}}. This simple modification leads to stronger representations, evidenced by superior results. We show top results in semantic segmentation on Cityscapes, LIP, and PASCAL Context, and facial landmark detection on AFLW, COFW, {\$}300{\$}W, and WFLW. In addition, we build a multi-level representation from the high-resolution representation and apply it to the Faster R-CNN object detection framework and the extended frameworks. The proposed approach achieves superior results to existing single-model networks on COCO object detection. The code and models have been publicly available at $\backslash$url{\{}https://github.com/HRNet{\}}.},
archivePrefix = {arXiv},
arxivId = {1904.04514},
author = {Sun, Ke and Zhao, Yang and Jiang, Borui and Cheng, Tianheng and Xiao, Bin and Liu, Dong and Mu, Yadong and Wang, Xinggang and Liu, Wenyu and Wang, Jingdong},
eprint = {1904.04514},
file = {:Users/philipeborba/Downloads/1904.04514.pdf:pdf},
title = {{High-Resolution Representations for Labeling Pixels and Regions}},
url = {http://arxiv.org/abs/1904.04514},
year = {2019}
}
@article{Liu2018a,
abstract = {Efficient and accurate semantic segmentation is the key technique for automatic remote sensing image analysis. While there have been many segmentation methods based on traditional hand-craft feature extractors, it is still challenging to process high-resolution and large-scale remote sensing images. In this work, a novel patch-wise semantic segmentation method with a new training strategy based on fully convolutional networks is presented to segment common land resources. First, to handle the high-resolution image, the images are split as local patches and then a patch-wise network is built. Second, training data is preprocessed in several ways to meet the specific characteristics of remote sensing images, i.e., color imbalance, object rotation variations and lens distortion. Third, a multi-scale training strategy is developed to solve the severe scale variation problem. In addition, the impact of conditional random field (CRF) is studied to improve the precision. The proposed method was evaluated on a dataset collected from a capital city in West China with the Gaofen-2 satellite. The dataset contains ten common land resources (Grassland, Road, etc.). The experimental results show that the proposed algorithm achieves 54.96{\%} in terms of mean intersection over union (MIoU) and outperforms other state-of-the-art methods in remote sensing image segmentation.},
author = {Liu, Yan and Ren, Qirui and Geng, Jiahui and Ding, Meng and Li, Jiangyun},
doi = {10.3390/s18103232},
file = {:Users/philipeborba/Downloads/sensors-18-03232.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Fully convolutional network,Image segmentation,Multi-scale,Patch-wise,Remote sensing},
number = {10},
pages = {1--16},
title = {{Efficient patch-wise semantic segmentation for large-scale remote sensing images}},
volume = {18},
year = {2018}
}
@inproceedings{He2014,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224×224) input image. This requirement is "artificial" and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101. The power of SPP-net is more significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method computes convolutional features 30-170× faster than the recent leading method R-CNN (and 24-64× faster overall), while achieving better or comparable accuracy on Pascal VOC 2007. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10578-9_23},
eprint = {1406.4729},
file = {:Users/philipeborba/Downloads/1406.4729.pdf:pdf},
isbn = {9783319105772},
issn = {16113349},
number = {PART 3},
pages = {346--361},
title = {{Spatial pyramid pooling in deep convolutional networks for visual recognition}},
volume = {8691 LNCS},
year = {2014}
}
@article{Schmitt2019,
abstract = {{\textless}p{\textgreater}{\textless}strong{\textgreater}Abstract.{\textless}/strong{\textgreater} The availability of curated large-scale training data is a crucial factor for the development of well-generalizing deep learning methods for the extraction of geoinformation from multi-sensor remote sensing imagery. While quite some datasets have already been published by the community, most of them suffer from rather strong limitations, e.g. regarding spatial coverage, diversity or simply number of available samples. Exploiting the freely available data acquired by the Sentinel satellites of the Copernicus program implemented by the European Space Agency, as well as the cloud computing facilities of Google Earth Engine, we provide a dataset consisting of 180,662 triplets of dual-pol synthetic aperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches, and MODIS land cover maps. With all patches being fully georeferenced at a 10{\&}thinsp;m ground sampling distance and covering all inhabited continents during all meteorological seasons, we expect the dataset to support the community in developing sophisticated deep learning-based approaches for common tasks such as scene classification or semantic segmentation for land cover mapping.{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {1906.07789},
author = {Schmitt, M. and Hughes, L. H. and Qiu, C. and Zhu, X. X.},
doi = {10.5194/isprs-annals-iv-2-w7-153-2019},
eprint = {1906.07789},
file = {:Users/philipeborba/Downloads/1906.07789.pdf:pdf},
issn = {2194-9050},
journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {data fusion,dataset,deep learning,machine learning,multi-spectral imagery,optical remote sensing,remote sensing,sar,sentinel-1,sentinel-2,synthetic aperture radar},
pages = {153--160},
title = {{SEN12MS {\&}ndash; A CURATED DATASET OF GEOREFERENCED MULTI-SPECTRAL SENTINEL-1/2 IMAGERY FOR DEEP LEARNING AND DATA FUSION}},
volume = {IV-2/W7},
year = {2019}
}
@article{Huang2014,
abstract = {In recent years, it has been widely agreed that spatial features derived from textural, structural, and object-based methods are important information sources to complement spectral properties for accurate urban classification of high-resolution imagery. However, the spatial features always refer to a series of parameters, such as scales, directions, and statistical measures, leading to high-dimensional feature space. The high-dimensional space is almost impractical to deal with considering the huge storage and computational cost while processing high-resolution images. To this aim, we propose a novel multi-index learning (MIL) method, where a set of low-dimensional information indices is used to represent the complex geospatial scenes in high-resolution images. Specifically, two categories of indices are proposed in the study: (1) Primitive indices (PI): High-resolution urban scenes are represented using a group of primitives (e.g., building/shadow/vegetation) that are calculated automatically and rapidly; (2) Variation indices (VI): A couple of spectral and spatial variation indices are proposed based on the 3D wavelet transformation in order to describe the local variation in the joint spectral-spatial domains. In this way, urban landscapes can be decomposed into a set of low-dimensional and semantic indices replacing the high-dimensional but low-level features (e.g., textures). The information indices are then learned via the multi-kernel support vector machines. The proposed MIL method is evaluated using various high-resolution images including GeoEye-1, QuickBird, WorldView-2, and ZY-3, as well as an elaborate comparison to the state-of-the-art image classification algorithms such as object-based analysis, and spectral-spatial approaches based on textural and morphological features. It is revealed that the MIL method is able to achieve promising results with a low-dimensional feature space, and, provide a practical strategy for processing large-scale high-resolution images. {\textcopyright} 2014 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author = {Huang, Xin and Lu, Qikai and Zhang, Liangpei},
doi = {10.1016/j.isprsjprs.2014.01.008},
file = {:Users/philipeborba/Downloads/huang2014.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Classification,Feature extraction,High spatial resolution,Morphological,SVM,Texture},
pages = {36--48},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{A multi-index learning approach for classification of high-resolution remotely sensed images over urban areas}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2014.01.008},
volume = {90},
year = {2014}
}
@article{Kang2018,
abstract = {Land-use classification based on spaceborne or aerial remote sensing images has been extensively studied over the past decades. Such classification is usually a patch-wise or pixel-wise labeling over the whole image. But for many applications, such as urban population density mapping or urban utility planning, a classification map based on individual buildings is much more informative. However, such semantic classification still poses some fundamental challenges, for example, how to retrieve fine boundaries of individual buildings. In this paper, we proposed a general framework for classifying the functionality of individual buildings. The proposed method is based on Convolutional Neural Networks (CNNs) which classify fa{\c{c}}ade structures from street view images, such as Google StreetView, in addition to remote sensing images which usually only show roof structures. Geographic information was utilized to mask out individual buildings, and to associate the corresponding street view images. We created a benchmark dataset which was used for training and evaluating CNNs. In addition, the method was applied to generate building classification maps on both region and city scales of several cities in Canada and the US.},
archivePrefix = {arXiv},
arxivId = {1802.09026},
author = {Kang, Jian and K{\"{o}}rner, Marco and Wang, Yuanyuan and Taubenb{\"{o}}ck, Hannes and Zhu, Xiao Xiang},
doi = {10.1016/j.isprsjprs.2018.02.006},
eprint = {1802.09026},
file = {:Users/philipeborba/Downloads/1-s2.0-S0924271618300352-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building instance classification,CNN,OpenStreetMap,Street view images},
pages = {44--59},
publisher = {The Author(s)},
title = {{Building instance classification using street view images}},
url = {https://doi.org/10.1016/j.isprsjprs.2018.02.006},
volume = {145},
year = {2018}
}
@article{Li2019a,
abstract = {Automatic extraction of building footprints from high-resolution satellite imagery has become an important and challenging research issue receiving greater attention. Many recent studies have explored different deep learning-based semantic segmentation methods for improving the accuracy of building extraction. Although they record substantial land cover and land use information (e.g., buildings, roads, water, etc.), public geographic information system (GIS) map datasets have rarely been utilized to improve building extraction results in existing studies. In this research, we propose a U-Net-based semantic segmentation method for the extraction of building footprints from high-resolution multispectral satellite images using the SpaceNet building dataset provided in the DeepGlobe Satellite Challenge of IEEE Conference on Computer Vision and Pattern Recognition 2018 (CVPR 2018). We explore the potential of multiple public GIS map datasets (OpenStreetMap, Google Maps, and MapWorld) through integration with theWorldView-3 satellite datasets in four cities (Las Vegas, Paris, Shanghai, and Khartoum). Several strategies are designed and combined with the U-Net-based semantic segmentation model, including data augmentation, post-processing, and integration of the GIS map data and satellite images. The proposed method achieves a total F1-score of 0.704, which is an improvement of 1.1{\%} to 12.5{\%} compared with the top three solutions in the SpaceNet Building Detection Competition and 3.0{\%} to 9.2{\%} compared with the standard U-Net-based method. Moreover, the effect of each proposed strategy and the possible reasons for the building footprint extraction results are analyzed substantially considering the actual situation of the four cities.},
author = {Li, Weijia and He, Conghui and Fang, Jiarui and Zheng, Juepeng and Fu, Haohuan and Yu, Le},
doi = {10.3390/rs11040403},
file = {:Users/philipeborba/Downloads/remotesensing-11-00403-v2.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Building extraction,Data fusion,Deep learning,GIS data,High-resolution satellite images,Semantic segmentation},
number = {4},
title = {{Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source GIS data}},
volume = {11},
year = {2019}
}
@article{Shi2020a,
abstract = {Automatic building extraction from optical imagery remains a challenge due to, for example, the complexity of building shapes. Semantic segmentation is an efficient approach for this task. The latest development in deep convolutional neural networks (DCNNs) has made accurate pixel-level classification tasks possible. Yet one central issue remains: the precise delineation of boundaries. Deep architectures generally fail to produce fine-grained segmentation with accurate boundaries due to their progressive down-sampling. Hence, we introduce a generic framework to overcome the issue, integrating the graph convolutional network (GCN) and deep structured feature embedding (DSFE) into an end-to-end workflow. Furthermore, instead of using a classic graph convolutional neural network, we propose a gated graph convolutional network, which enables the refinement of weak and coarse semantic predictions to generate sharp borders and fine-grained pixel-level classification. Taking the semantic segmentation of building footprints as a practical example, we compared different feature embedding architectures and graph neural networks. Our proposed framework with the new GCN architecture outperforms state-of-the-art approaches. Although our main task in this work is building footprint extraction, the proposed method can be generally applied to other binary or multi-label segmentation tasks.},
archivePrefix = {arXiv},
arxivId = {1911.03165},
author = {Shi, Yilei and Li, Qingyu and Zhu, Xiao Xiang},
doi = {10.1016/j.isprsjprs.2019.11.004},
eprint = {1911.03165},
file = {:Users/philipeborba/Downloads/1-s2.0-S092427161930259X-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building extraction,Gated convoluational neural networks,Graph model,Semantic segmentation},
number = {March 2019},
pages = {184--197},
publisher = {Elsevier},
title = {{Building segmentation through a gated graph convolutional neural network with deep structured feature embedding}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.11.004},
volume = {159},
year = {2020}
}
@article{Papadomanolaki2019,
abstract = {Deep learning architectures have received much attention in recent years demonstrating state-of-the-art performance in several segmentation, classification and other computer vision tasks. Most of these deep networks are based on either convolutional or fully convolutional architectures. In this paper, we propose a novel object-based deep-learning framework for semantic segmentation in very high-resolution satellite data. In particular, we exploit object-based priors integrated into a fully convolutional neural network by incorporating an anisotropic diffusion data preprocessing step and an additional loss term during the training process. Under this constrained framework, the goal is to enforce pixels that belong to the same object to be classified at the same semantic category. We compared thoroughly the novel object-based framework with the currently dominating convolutional and fully convolutional deep networks. In particular, numerous experiments were conducted on the publicly available ISPRS WGII/4 benchmark datasets, namely Vaihingen and Potsdam, for validation and inter-comparison based on a variety of metrics. Quantitatively, experimental results indicate that, overall, the proposed object-based framework slightly outperformed the current state-of-the-art fully convolutional networks by more than 1{\%} in terms of overall accuracy, while intersection over union results are improved for all semantic categories. Qualitatively, man-made classes with more strict geometry such as buildings were the ones that benefit most from our method, especially along object boundaries, highlighting the great potential of the developed approach.},
author = {Papadomanolaki, Maria and Vakalopoulou, Maria and Karantzalos, Konstantinos},
doi = {10.3390/rs11060684},
file = {:Users/philipeborba/Downloads/remotesensing-11-00684-v2.pdf:pdf},
issn = {2072-4292},
journal = {Remote Sensing},
keywords = {anisotropic diffusion,convolutional neural networks,earth observation,geobia,machine learning,object-based image analysis,satellite data,superpixels},
number = {6},
pages = {684},
title = {{A Novel Object-Based Deep Learning Framework for Semantic Segmentation of Very High-Resolution Remote Sensing Data: Comparison with Convolutional and Fully Convolutional Networks}},
volume = {11},
year = {2019}
}
@article{Bittner2018,
abstract = {Automatic building extraction and delineation from high-resolution satellite imagery is an important but very challenging task, due to the extremely large diversity of building appearances. Nowadays, it is possible to use multiple high-resolution remote sensing data sources, which allow the integration of different information in order to improve the extraction accuracy of building outlines. Many algorithms are built on spectral-based or appearance-based criteria, from single or fused data sources, to perform the building footprint extraction. But the features for these algorithms are usually manually extracted, which limits their accuracy. Recently developed fully convolutional networks (FCNs), which are similar to normal convolutional neural networks (CNN), but the last fully connected layer is replaced by another convolution layer with a large "receptive field," quickly became the state-of-the-art method for image recognition tasks, as they bring the possibility to perform dense pixelwise classification of input images. Based on these advantages, i.e., the automatic extraction of relevant features, and dense classification of images, we propose an end-to-end FCN, which effectively combines the spectral and height information from different data sources and automatically generates a full resolution binary building mask. Our architecture (Fused-FCN4s) consists of three parallel networks merged at a late stage, which helps propagating fine detailed information from earlier layers to higher levels, in order to produce an output with more accurate building outlines. The inputs to the proposed Fused-FCN4s are three-band (RGB), panchromatic (PAN), and normalized digital surface model (nDSM) images. Experimental results demonstrate that the fusion of several networks is able to achieve excellent results on complex data. Moreover, the developed model was successfully applied to different cities to show its generalization capacity.},
author = {Bittner, Ksenia and Adam, Fathalrahman and Cui, Shiyong and K{\"{o}}rner, Marco and Reinartz, Peter},
doi = {10.1109/JSTARS.2018.2849363},
file = {:Users/philipeborba/Downloads/08447548.pdf:pdf},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Binary classification,building footprint,data fusion,deep learning,fully convolutional networks (FCNs),satellite images},
number = {8},
pages = {2615--2629},
title = {{Building Footprint Extraction From VHR Remote Sensing Images Combined With Normalized DSMs Using Fused Fully Convolutional Networks}},
volume = {11},
year = {2018}
}
@article{Kang2019,
abstract = {Automatic building extraction from high-resolution remote sensing images has many practical applications, such as urban planning and supervision. However, fine details and various scales of building structures in high-resolution images bring new challenges to building extraction. An increasing number of neural network-based models have been proposed to handle these issues, while they are not efficient enough, and still suffer from the error ground truth labels. To this end, we propose an efficient end-to-end model, EU-Net, in this paper. We first design the dense spatial pyramid pooling (DSPP) to extract dense and multi-scale features simultaneously, which facilitate the extraction of buildings at all scales. Then, the focal loss is used in reverse to suppress the impact of the error labels in ground truth, making the training stage more stable. To assess the universality of the proposed model, we tested it on three public aerial remote sensing datasets: WHU aerial imagery dataset, Massachusetts buildings dataset, and Inria aerial image labeling dataset. Experimental results show that the proposed EU-Net is superior to the state-of-the-art models of all three datasets and increases the prediction efficiency by two to four times.},
author = {Kang, Wenchao and Xiang, Yuming and Wang, Feng and You, Hongjian},
doi = {10.3390/rs11232813},
file = {:Users/philipeborba/Downloads/remotesensing-11-02813-v2.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Building extraction,Fully convolutional network,High-resolution aerial imagery,Semantic segmentation},
number = {23},
title = {{EU-Net: An efficient fully convolutional network for building extraction from optical remote sensing images}},
volume = {11},
year = {2019}
}
@article{Li2019b,
abstract = {Automatic extraction of building footprints from high-resolution satellite imagery has become an important and challenging research issue receiving greater attention. Many recent studies have explored different deep learning-based semantic segmentation methods for improving the accuracy of building extraction. Although they record substantial land cover and land use information (e.g., buildings, roads, water, etc.), public geographic information system (GIS) map datasets have rarely been utilized to improve building extraction results in existing studies. In this research, we propose a U-Net-based semantic segmentation method for the extraction of building footprints from high-resolution multispectral satellite images using the SpaceNet building dataset provided in the DeepGlobe Satellite Challenge of IEEE Conference on Computer Vision and Pattern Recognition 2018 (CVPR 2018). We explore the potential of multiple public GIS map datasets (OpenStreetMap, Google Maps, and MapWorld) through integration with theWorldView-3 satellite datasets in four cities (Las Vegas, Paris, Shanghai, and Khartoum). Several strategies are designed and combined with the U-Net-based semantic segmentation model, including data augmentation, post-processing, and integration of the GIS map data and satellite images. The proposed method achieves a total F1-score of 0.704, which is an improvement of 1.1{\%} to 12.5{\%} compared with the top three solutions in the SpaceNet Building Detection Competition and 3.0{\%} to 9.2{\%} compared with the standard U-Net-based method. Moreover, the effect of each proposed strategy and the possible reasons for the building footprint extraction results are analyzed substantially considering the actual situation of the four cities.},
author = {Li, Weijia and He, Conghui and Fang, Jiarui and Zheng, Juepeng and Fu, Haohuan and Yu, Le},
doi = {10.3390/rs11040403},
file = {:Users/philipeborba/Downloads/remotesensing-11-00403-v2-2.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Building extraction,Data fusion,Deep learning,GIS data,High-resolution satellite images,Semantic segmentation},
number = {4},
title = {{Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source GIS data}},
volume = {11},
year = {2019}
}
@article{Gurumurthy2019,
abstract = {With the advancement of remote-sensed imaging large volumes of very high resolution land cover images can now be obtained. Automation of object recognition in these 2D images, however, is still a key issue. High intra-class variance and low inter-class variance in Very High Resolution (VHR) images hamper the accuracy of prediction in object recognition tasks. Most successful techniques in various computer vision tasks recently are based on deep supervised learning. In this work, a deep Convolutional Neural Network (CNN) based on symmetric encoder-decoder architecture with skip connections is employed for the 2D semantic segmentation of most common land cover object classes - impervious surface, buildings, low vegetation, trees and cars. Atrous convolutions are employed to have large receptive field in the proposed CNN model. Further, the CNN outputs are post-processed using Fully Connected Conditional Random Field (FCRF) model to refine the CNN pixel label predictions. The proposed CNN-FCRF model achieves an overall accuracy of 90.5{\%} on the ISPRS Vaihingen Dataset.},
archivePrefix = {arXiv},
arxivId = {1910.06041},
author = {Gurumurthy, Vikas Agaradahalli},
eprint = {1910.06041},
file = {:Users/philipeborba/Downloads/1910.06041.pdf:pdf},
pages = {1--5},
title = {{Encoder-Decoder based CNN and Fully Connected CRFs for Remote Sensed Image Segmentation}},
url = {http://arxiv.org/abs/1910.06041},
year = {2019}
}
@article{Nyasaka2020,
author = {Nyasaka, Divinah and Wang, Jing and Tinega, Haron},
file = {:Users/philipeborba/Downloads/2002.02585.pdf:pdf},
title = {{Learning Hyperspectral Feature Extraction and Classification with ResNeXt Network}},
year = {2020}
}
@article{Lunga2019,
abstract = {The shear volumes of data generated from earth observation and remote sensing technologies continue to make major impact; leaping key geospatial applications into the dual data and compute intensive era. As a consequence, this rapid advancement poses new computational and data processing challenges. We implement a novel remote sensing data flow (RESFlow) for advanced machine learning and computing with massive amounts of remotely sensed imagery. The core contribution is partitioning massive amount of data based on the spectral and semantic characteristics for distributed imagery analysis. RESFlow takes advantage of both a unified analytics engine for large-scale data processing and the availability of modern computing hardware to harness the acceleration of deep learning inference on expansive remote sensing imagery. The framework incorporates a strategy to optimize resource utilization across multiple executors assigned to a single worker. We showcase its deployment across computationally and data-intensive on pixel-level labeling workloads. The pipeline invokes deep learning inference at three stages; during deep feature extraction, deep metric mapping, and deep semantic segmentation. The tasks impose compute intensive and GPU resource sharing challenges motivating for a parallelized pipeline for all execution steps. By taking advantage of Apache Spark, Nvidia DGX1, and DGX2 computing platforms, we demonstrate unprecedented compute speed-ups for deep learning inference on pixel labeling workloads; processing 21,028{\~{}}Terrabytes of imagery data and delivering an output maps at area rate of 5.245sq.km/sec, amounting to 453,168 sq.km/day - reducing a 28 day workload to 21{\~{}}hours.},
archivePrefix = {arXiv},
arxivId = {1908.04383},
author = {Lunga, Dalton and Gerrand, Jonathan and Yang, Hsiuhan Lexie and Layton, Christopher and Stewart, Robert},
doi = {10.1109/jstars.2019.2959707},
eprint = {1908.04383},
file = {:Users/philipeborba/Downloads/1908.04383.pdf:pdf},
title = {{Apache Spark Accelerated Deep Learning Inference for Large Scale Satellite Image Analytics}},
url = {http://arxiv.org/abs/1908.04383},
year = {2019}
}
@article{Chatterjee2019,
abstract = {In this paper we address three different aspects of semantic segmentation from remote sensor data using deep neural networks. Firstly, we focus on the semantic segmentation of buildings from remote sensor data and propose ICT-Net. The proposed network has been tested on the INRIA and AIRS benchmark datasets and is shown to outperform all other state of the art by more than 1.5{\%} and 1.8{\%} on the Jaccard index, respectively. Secondly, as the building classification is typically the first step of the reconstruction process, we investigate the relationship of the classification accuracy to the reconstruction accuracy. Finally, we present the simple yet compelling concept of latent learning and the implications it carries within the context of deep learning. We posit that a network trained on a primary task (i.e. building classification) is unintentionally learning about auxiliary tasks (e.g. the classification of road, tree, etc) which are complementary to the primary task. We extensively tested the proposed technique on the ISPRS benchmark dataset which contains multi-label ground truth, and report an average classification accuracy (F1 score) of 54.29{\%} (SD=17.03) for roads, 10.15{\%} (SD=2.54) for cars, 24.11{\%} (SD=5.25) for trees, 42.74{\%} (SD=6.62) for low vegetation, and 18.30{\%} (SD=16.08) for clutter. The source code and supplemental material is publicly available at http://www.theICTlab.org/lp/2019ICT-Net/.},
archivePrefix = {arXiv},
arxivId = {1912.09216},
author = {Chatterjee, Bodhiswatta and Poullis, Charalambos},
eprint = {1912.09216},
file = {:Users/philipeborba/Downloads/manuscript.pdf:pdf},
pages = {1--17},
title = {{Semantic Segmentation from Remote Sensor Data and the Exploitation of Latent Learning for Classification of Auxiliary Tasks}},
url = {http://arxiv.org/abs/1912.09216},
year = {2019}
}
@article{Liu2019,
abstract = {In recent years, building change detection methods have made great progress by introducing deep learning, but they still suffer from the problem of the extracted features not being discriminative enough, resulting in incomplete regions and irregular boundaries. To tackle this problem, we propose a dual task constrained deep Siamese convolutional network (DTCDSCN) model, which contains three sub-networks: a change detection network and two semantic segmentation networks. DTCDSCN can accomplish both change detection and semantic segmentation at the same time, which can help to learn more discriminative object-level features and obtain a complete change detection map. Furthermore, we introduce a dual attention module (DAM) to exploit the interdependencies between channels and spatial positions, which improves the feature representation. We also improve the focal loss function to suppress the sample imbalance problem. The experimental results obtained with the WHU building dataset show that the proposed method is effective for building change detection and achieves a state-of-the-art performance in terms of four metrics: precision, recall, F1-score, and intersection over union.},
archivePrefix = {arXiv},
arxivId = {1909.07726},
author = {Liu, Yi and Pang, Chao and Zhan, Zongqian and Zhang, Xiaomeng and Yang, Xue},
eprint = {1909.07726},
file = {:Users/philipeborba/Downloads/1909.07726.pdf:pdf},
title = {{Building Change Detection for Remote Sensing Images Using a Dual Task Constrained Deep Siamese Convolutional Network Model}},
url = {http://arxiv.org/abs/1909.07726},
year = {2019}
}
@article{Su2019,
abstract = {Object detection in very high-resolution (VHR) remote sensing images is a fundamental and challenging problem due to the complex environments. In this paper, a precise mask region convolutional neural network (precise Mask R-CNN) is presented for object detection and instance segmentation in VHR remote sensing images. This method generates bounding boxes and segmentation masks for each instance of an object in the image. Contrary to regions of interest (RoI) Align whose sample points is pre-defined and not adaptive the size of the bin, the proposed precise RoI pooling can directly compute the two-order integral based on the continuous feature map to avoid loss of precision. The experiments on NWPU VHR-10 dataset show that the presented precise Mask R-CNN improves the accuracy of object detection and instance segmentation for VHR remote sensing images. Furthermore, it promotes the application of instance segmentation in VHR remote sensing.},
author = {Su, Hao and Wei, Shunjun and Yan, Min and Wang, Chen and Shi, Jun and Zhang, Xiaoling},
doi = {10.1109/IGARSS.2019.8898573},
file = {:Users/philipeborba/Downloads/su2019.pdf:pdf},
isbn = {9781538691540},
journal = {International Geoscience and Remote Sensing Symposium (IGARSS)},
keywords = {Mask R-CNN,Object detection,instance segmentation,precise RoI Pooling,remote sensing images},
pages = {1454--1457},
publisher = {IEEE},
title = {{Object Detection and Instance Segmentation in Remote Sensing Imagery Based on Precise Mask R-CNN}},
year = {2019}
}
@article{Bittner2017,
abstract = {Building detection and footprint extraction are highly demanded for many remote sensing applications. Though most previous works have shown promising results, the automatic extraction of building footprints still remains a nontrivial topic, especially in complex urban areas. Recently developed extensions of the CNN framework made it possible to perform dense pixel-wise classification of input images. Based on these abilities we propose a methodology, which automatically generates a full resolution binary building mask out of a Digital Surface Model (DSM) using a Fully Convolution Network (FCN) architecture. The advantage of using the depth information is that it provides geometrical silhouettes and allows a better separation of buildings from background as well as through its invariance to illumination and color variations. The proposed framework has mainly two steps. Firstly, the FCN is trained on a large set of patches consisting of normalized DSM (nDSM) as inputs and available ground truth building mask as target outputs. Secondly, the generated predictions from FCN are viewed as unary terms for a Fully connected Conditional Random Fields (FCRF), which enables us to create a final binary building mask. A series of experiments demonstrate that our methodology is able to extract accurate building footprints which are close to the buildings original shapes to a high degree. The quantitative and qualitative analysis show the significant improvements of the results in contrast to the multy-layer fully connected network from our previous work.},
author = {Bittner, K. and Cui, S. and Reinartz, P.},
doi = {10.5194/isprs-archives-XLII-1-W1-481-2017},
file = {:Users/philipeborba/Downloads/c1a6669f26c6a2b517603eeb7897160335ca.pdf:pdf},
issn = {16821750},
journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
keywords = {Binary classification,Building footprint,DSM,Deep learning,Fully connected CRF,Fully convolutional networks},
number = {1W1},
pages = {481--486},
title = {{Building extraction from remote sensing data using fully convolutional networks}},
volume = {42},
year = {2017}
}
@article{Kulkarni2020,
abstract = {Change detection (CD) mainly focuses on the extraction of change information from multispectral remote sensing images of the same geographical location for environmental monitoring, natural disaster evaluation, urban studies, and deforestation monitoring. While capturing the Landsat imagery, there may occur data missing issues such as occlusion of cloud, camera sensor, and aperture artifacts. The existing machine learning approaches do not provide significant results. This paper proposes a DeepLab Dilated convolutional neural network (DL-DCNN) for semantic segmentation with the goal to occur the change map for earth observation applications. Experimental results reveal that the accuracy of the proposed change detection results provides improved results as compared with the existing algorithms and maps the semantic objects within the predefined class as change or no change.},
annote = {ler, parece interessante},
author = {Kulkarni, Tejashree and Venugopal, N.},
doi = {10.1007/978-981-10-8569-7_34},
file = {:Users/philipeborba/Downloads/10.1007@s11063-019-10174-x.pdf:pdf},
isbn = {9789811085680},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Change detection,Deep learning,Multispectral,Remote sensing},
number = {0123456789},
pages = {337--344},
publisher = {Springer US},
title = {{Automatic semantic segmentation for change detection in remote sensing images}},
url = {https://doi.org/10.1007/s11063-019-10174-x},
volume = {705},
year = {2020}
}
@article{Wurm2019,
abstract = {Unprecedented urbanization in particular in countries of the global south result in informal urban development processes, especially in mega cities. With an estimated 1 billion slum dwellers globally, the United Nations have made the fight against poverty the number one sustainable development goal. To provide better infrastructure and thus a better life to slum dwellers, detailed information on the spatial location and size of slums is of crucial importance. In the past, remote sensing has proven to be an extremely valuable and effective tool for mapping slums. The nature of used mapping approaches by machine learning, however, made it necessary to invest a lot of effort in training the models. Recent advances in deep learning allow for transferring trained fully convolutional networks (FCN) from one data set to another. Thus, in our study we aim at analyzing transfer learning capabilities of FCNs to slum mapping in various satellite images. A model trained on very high resolution optical satellite imagery from QuickBird is transferred to Sentinel-2 and TerraSAR-X data. While free-of-charge Sentinel-2 data is widely available, its comparably lower resolution makes slum mapping a challenging task. TerraSAR-X data on the other hand, has a higher resolution and is considered a powerful data source for intra-urban structure analysis. Due to the different image characteristics of SAR compared to optical data, however, transferring the model could not improve the performance of semantic segmentation but we observe very high accuracies for mapped slums in the optical data: QuickBird image obtains 86–88{\%} (positive prediction value and sensitivity) and a significant increase for Sentinel-2 applying transfer learning can be observed (from 38 to 55{\%} and from 79 to 85{\%} for PPV and sensitivity, respectively). Using transfer learning proofs extremely valuable in retrieving information on small-scaled urban structures such as slum patches even in satellite images of decametric resolution.},
author = {Wurm, Michael and Stark, Thomas and Zhu, Xiao Xiang and Weigand, Matthias and Taubenb{\"{o}}ck, Hannes},
doi = {10.1016/j.isprsjprs.2019.02.006},
file = {:Users/philipeborba/Downloads/1-s2.0-S0924271619300383-main-3.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Convolutional neural networks,Deep learning,FCN,Slums,Transfer learning},
number = {December 2018},
pages = {59--69},
publisher = {Elsevier},
title = {{Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.02.006},
volume = {150},
year = {2019}
}
@article{Shendryk2019,
abstract = {With the increasing availability of high-resolution satellite imagery it is important to improve the efficiency and accuracy of satellite image indexing, retrieval and classification. Furthermore, there is a need for utilizing all available satellite imagery in identifying general land cover types and monitoring their changes through time irrespective of their spatial, spectral, temporal and radiometric resolutions. Therefore, in this study, we developed deep learning models able to efficiently and accurately classify cloud, shadow and land cover scenes in different high-resolution ({\textless}10 m) satellite imagery. Specifically, we trained deep convolutional neural network (CNN) models to perform multi-label classification of multi-modal, high-resolution satellite imagery at the scene level. Multi-label classification at the scene level (a.k.a. image indexing), as opposed to the pixel level, allows for faster performance, higher accuracy (although at the cost of detail) and higher generalizability. We investigated the generalization ability (i.e. cross-dataset and geographic independence) of individual and ensemble CNN models trained on multi-modal satellite imagery (i.e. PlanetScope and Sentinel-2). The models trained on PlanetScope imagery collected over the Amazon performed well when applied to PlanetScope and Sentinel-2 imagery collected over the Wet Tropics of Australia with an F2 score of 0.72 and 0.69, respectively. Similarly, PlanetScope-based CNN models trained on imagery collected over the Wet Tropics of Australia performed well when applied to Sentinel-2 imagery with an F2 score of 0.76, and the reverse scenario resulted in the same F2 score of 0.76. This suggests that our CNN models have high cross-dataset generalization ability and are suitable for classifying cloud, shadow and land cover classes in satellite imagery with resolutions from 3 m (PlanetScope) to 10 m (Sentinel-2). The performance of our CNN models was also comparable to the state-of-the-art methods (i.e. Sen2Cor and MACCS) developed specifically for classifying cloud and shadow classes in Sentinel-2 imagery. Finally, we show the potential of our CNN models to mask cloud and shadow contaminated areas from PlanetScope- and Sentinel-2-derived NDVI time-series.},
author = {Shendryk, Yuri and Rist, Yannik and Ticehurst, Catherine and Thorburn, Peter},
doi = {10.1016/j.isprsjprs.2019.08.018},
file = {:Users/philipeborba/Downloads/1-s2.0-S0924271619302023-main-3.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {CNN,Deep learning,Ensemble,Indexing,MACCS,Multi-label,Multi-modal,PlanetScope,Remote sensing,Scene classification,Sen2Cor,Sentinel-2},
number = {June},
pages = {124--136},
publisher = {Elsevier},
title = {{Deep learning for multi-modal classification of cloud, shadow and land cover scenes in PlanetScope and Sentinel-2 imagery}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.08.018},
volume = {157},
year = {2019}
}
@article{Zhu2019,
author = {Zhu, Qingtian and Zheng, Yumin and Jiang, Yulai and Yang, Junli},
doi = {10.1109/igarss.2019.8900281},
file = {:Users/philipeborba/Downloads/zhu2019.pdf:pdf},
isbn = {9781538691540},
journal = {IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium},
pages = {1065--1068},
publisher = {IEEE},
title = {{Efficient Multi-Class Semantic Segmentation of High Resolution Aerial Imagery with Dilated LinkNet}},
year = {2019}
}
@article{Zhu2019a,
abstract = {Accurately and efficiently extracting building footprints from a wide range of remote sensed imagery remains a challenge due to their complex structure, variety of scales and diverse appearances. Existing convolutional neural network (CNN)-based building extraction methods are complained that they cannot detect the tiny buildings because the spatial information of CNN feature maps are lost during repeated pooling operations of the CNN, and the large buildings still have inaccurate segmentation edges. Moreover, features extracted by a CNN are always partial which restricted by the size of the respective field, and large-scale buildings with low texture are always discontinuous and holey when extracted. This paper proposes a novel multi attending path neural network (MAP-Net) for accurately extracting multiscale building footprints and precise boundaries. MAP-Net learns spatial localization-preserved multiscale features through a multi-parallel path in which each stage is gradually generated to extract high-level semantic features with fixed resolution. Then, an attention module adaptively squeezes channel-wise features from each path for optimization, and a pyramid spatial pooling module captures global dependency for refining discontinuous building footprints. Experimental results show that MAP-Net outperforms state-of-the-art (SOTA) algorithms in boundary localization accuracy as well as continuity of large buildings. Specifically, our method achieved 0.68$\backslash${\%}, 1.74$\backslash${\%}, 1.46$\backslash${\%} precision, and 1.50$\backslash${\%}, 1.53$\backslash${\%}, 0.82$\backslash${\%} IoU score improvement without increasing computational complexity compared with the latest HRNetv2 on the Urban 3D, Deep Globe and WHU datasets, respectively. The TensorFlow implementation is available at https://github.com/lehaifeng/MAPNet.},
archivePrefix = {arXiv},
arxivId = {1910.12060},
author = {Zhu, Qing and Liao, Cheng and Hu, Han and Mei, Xiaoming and Li, Haifeng},
eprint = {1910.12060},
file = {:Users/philipeborba/Downloads/1910.12060.pdf:pdf},
pages = {1--11},
title = {{MAP-Net: Multi Attending Path Neural Network for Building Footprint Extraction from Remote Sensed Imagery}},
url = {http://arxiv.org/abs/1910.12060},
year = {2019}
}
@article{Shi2020,
abstract = {Automatic building extraction from optical imagery remains a challenge due to, for example, the complexity of building shapes. Semantic segmentation is an efficient approach for this task. The latest development in deep convolutional neural networks (DCNNs) has made accurate pixel-level classification tasks possible. Yet one central issue remains: the precise delineation of boundaries. Deep architectures generally fail to produce fine-grained segmentation with accurate boundaries due to their progressive down-sampling. Hence, we introduce a generic framework to overcome the issue, integrating the graph convolutional network (GCN) and deep structured feature embedding (DSFE) into an end-to-end workflow. Furthermore, instead of using a classic graph convolutional neural network, we propose a gated graph convolutional network, which enables the refinement of weak and coarse semantic predictions to generate sharp borders and fine-grained pixel-level classification. Taking the semantic segmentation of building footprints as a practical example, we compared different feature embedding architectures and graph neural networks. Our proposed framework with the new GCN architecture outperforms state-of-the-art approaches. Although our main task in this work is building footprint extraction, the proposed method can be generally applied to other binary or multi-label segmentation tasks.},
archivePrefix = {arXiv},
arxivId = {1911.03165},
author = {Shi, Yilei and Li, Qingyu and Zhu, Xiao Xiang},
doi = {10.1016/j.isprsjprs.2019.11.004},
eprint = {1911.03165},
file = {:Users/philipeborba/Downloads/1-s2.0-S092427161930259X-main-2.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building extraction,GATED GRAPH CNN,Gated convoluational neural networks,Graph model,Semantic segmentation},
mendeley-tags = {GATED GRAPH CNN},
number = {March 2019},
pages = {184--197},
publisher = {Elsevier},
title = {{Building segmentation through a gated graph convolutional neural network with deep structured feature embedding}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.11.004},
volume = {159},
year = {2020}
}
@article{Chen2018a,
abstract = {In this paper, we address the deep semantic segmentation of aerial imagery based on multi-modal data. Given multi-modal data composed of true orthophotos and the corresponding Digital Surface Models (DSMs), we extract a variety of hand-crafted radiometric and geometric features which are provided separately and in different combinations as input to a modern deep learning framework. The latter is represented by a Residual Shuffling Convolutional Neural Network (RSCNN) combining the characteristics of a Residual Network with the advantages of atrous convolution and a shuffling operator to achieve a dense semantic labeling. Via performance evaluation on a benchmark dataset, we analyze the value of different feature sets for the semantic segmentation task. The derived results reveal that the use of radiometric features yields better classification results than the use of geometric features for the considered dataset. Furthermore, the consideration of data on both modalities leads to an improvement of the classification results. However, the derived results also indicate that the use of all defined features is less favorable than the use of selected features. Consequently, data representations derived via feature extraction and feature selection techniques still provide a gain if used as the basis for deep semantic segmentation.},
author = {Chen, K. and Weinmann, M. and Gao, X. and Yan, M. and Hinz, S. and Jutzi, B. and Weinmann, M.},
doi = {10.5194/isprs-annals-IV-2-65-2018},
file = {:Users/philipeborba/Downloads/isprs-annals-IV-2-65-2018.pdf:pdf},
issn = {21949050},
journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {Aerial Imagery,CNN,Deep Learning,Multi-Modal Data,Residual Network,Semantic Segmentation},
number = {2},
pages = {65--72},
title = {{RESIDUAL SHUFFLING CONVOLUTIONAL NEURAL NETWORKS for DEEP SEMANTIC IMAGE SEGMENTATION USING MULTI-MODAL DATA}},
volume = {4},
year = {2018}
}
@article{Hurt2019,
author = {Hurt, J Alex and Scott, Grant J and Davis, Curt H},
file = {:Users/philipeborba/Downloads/hurt2019.pdf:pdf},
isbn = {9781538691540},
journal = {IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium},
pages = {1326--1329},
publisher = {IEEE},
title = {{META-DATASET TRAINING VERSUS DEEP NEURAL ENSEMBLES J . Alex Hurt , Grant J . Scott , Curt H . Davis}},
year = {2019}
}
@article{Jadhav2018,
author = {Jadhav, Jagannath K. and Singh, R. P.},
doi = {10.21595/mme.2018.19840},
file = {:Users/philipeborba/Downloads/MME-4-2-19840.pdf:pdf},
issn = {2351-5279},
journal = {Mathematical Models in Engineering},
keywords = {cd,change detection,cnns,crop classification,deep learning,dl,landsat-8 remote sensing,rs,self-organizing map,soms},
number = {2},
pages = {112--137},
title = {{Automatic semantic segmentation and classification of remote sensing data for agriculture}},
volume = {4},
year = {2018}
}
@article{Cheng2017,
abstract = {Sea-land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e.g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features from the segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected from Google Earth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.},
author = {Cheng, Dongcai and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
doi = {10.1109/JSTARS.2017.2747599},
file = {:Users/philipeborba/Downloads/FusionNet.pdf:pdf},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Edge aware regularization,harbor images,multitask learning,semantic segmentation},
number = {12},
pages = {5769--5783},
title = {{FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images}},
volume = {10},
year = {2017}
}
@article{Kattenborn2019,
abstract = {Recent technological advances in remote sensing sensors and platforms, such as high-resolution satellite imagers or unmanned aerial vehicles (UAV), facilitate the availability of fine-grained earth observation data. Such data reveal vegetation canopies in high spatial detail. Efficient methods are needed to fully harness this unpreceded source of information for vegetation mapping. Deep learning algorithms such as Convolutional Neural Networks (CNN) are currently paving new avenues in the field of image analysis and computer vision. Using multiple datasets, we test a CNN-based segmentation approach (U-net) in combination with training data directly derived from visual interpretation of UAV-based high-resolution RGB imagery for fine-grained mapping of vegetation species and communities. We demonstrate that this approach indeed accurately segments and maps vegetation species and communities (at least 84{\%} accuracy). The fact that we only used RGB imagery suggests that plant identification at very high spatial resolutions is facilitated through spatial patterns rather than spectral information. Accordingly, the presented approach is compatible with low-cost UAV systems that are easy to operate and thus applicable to a wide range of users.},
author = {Kattenborn, Teja and Eichel, Jana and Fassnacht, Fabian Ewald},
doi = {10.1038/s41598-019-53797-9},
file = {:Users/philipeborba/Downloads/s41598-019-53797-9.pdf:pdf},
isbn = {4159801953797},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--9},
publisher = {Springer US},
title = {{Convolutional Neural Networks enable efficient, accurate and fine-grained segmentation of plant species and communities from high-resolution UAV imagery}},
url = {http://dx.doi.org/10.1038/s41598-019-53797-9},
volume = {9},
year = {2019}
}
@article{Marmanis2016,
abstract = {This paper describes a deep learning approach to semantic segmentation of very high resolution (aerial) images. Deep neural architectures hold the promise of end-to-end learning from raw images, making heuristic feature design obsolete. Over the last decade this idea has seen a revival, and in recent years deep convolutional neural networks (CNNs) have emerged as the method of choice for a range of image interpretation tasks like visual recognition and object detection. Still, standard CNNs do not lend themselves to per-pixel semantic segmentation, mainly because one of their fundamental principles is to gradually aggregate information over larger and larger image regions, making it hard to disentangle contributions from different pixels. Very recently two extensions of the CNN framework have made it possible to trace the semantic information back to a precise pixel position: deconvolutional network layers undo the spatial downsampling, and Fully Convolution Networks (FCNs) modify the fully connected classification layers of the network in such a way that the location of individual activations remains explicit. We design a FCN which takes as input intensity and range data and, with the help of aggressive deconvolution and recycling of early network layers, converts them into a pixelwise classification at full resolution. We discuss design choices and intricacies of such a network, and demonstrate that an ensemble of several networks achieves excellent results on challenging data such as the ISPRS semantic labeling benchmark, using only the raw data as input.},
author = {Marmanis, D. and Wegner, J. D. and Galliani, S. and Schindler, K. and Datcu, M. and Stilla, U.},
doi = {10.5194/isprs-annals-III-3-473-2016},
file = {:Users/philipeborba/Downloads/marmanis-isprs16.pdf:pdf},
issn = {21949050},
journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
pages = {473--480},
title = {{SEMANTIC SEGMENTATION of AERIAL IMAGES with AN ENSEMBLE of CNNS}},
volume = {3},
year = {2016}
}
@article{Yao2019,
abstract = {Land use classification is a fundamental task of information extraction from remote sensing imagery. Semantic segmentation based on deep convolutional neural networks (DCNNs) has shown outstanding performance in this task. However, these methods are still affected by the loss of spatial features. In this study, we proposed a new network, called the dense-coordconv network (DCCN), to reduce the loss of spatial features and strengthen object boundaries. In this network, the coordconv module is introduced into the improved DenseNet architecture to improve spatial information by putting coordinate information into feature maps. The proposed DCCN achieved an obvious performance in terms of the public ISPRS (International Society for Photogrammetry and Remote Sensing) 2D semantic labeling benchmark dataset. Compared with the results of other deep convolutional neural networks (U-net, SegNet, Deeplab-V3), the results of the DCCN method improved a lot and the OA (overall accuracy) and mean F1 score reached 89.48{\%} and 86.89{\%}, respectively. This indicates that the DCCN method can effectively reduce the loss of spatial features and improve the accuracy of semantic segmentation in high resolution remote sensing imagery.},
author = {Yao, Xuedong and Yang, Hui and Wu, Yanlan and Wu, Penghai and Wang, Biao and Zhou, Xinxin and Wang, Shuai},
doi = {10.3390/s19122792},
file = {:Users/philipeborba/Downloads/sensors-19-02792.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Coordconv,DCNNs,High resolution,ISPRS,Remote sensing,Semantic segmentation},
number = {12},
title = {{Land use classification of the deep convolutional neural network method reducing the loss of spatial features}},
volume = {19},
year = {2019}
}
@article{Amirkolaee2019,
abstract = {Extracting 3D information from aerial images is an important and still challenging topic in photogrammetry and remote sensing. Height estimation from only a single aerial image is an ambiguous and ill-posed problem. To address this challenging problem, in this paper, an architecture based on a deep convolutional neural network (CNN) is proposed in order to estimate the height values from a single aerial image. Methodologies for data preprocessing, selection of training data as well as data augmentation are presented. Subsequently, a deep CNN architecture is proposed consisting of encoding and decoding steps. In the encoding part, a deep residual learning is employed for extracting the local and global features. An up-sampling approach is proposed in the decoding part for increasing the output resolution and skip connections are employed in each scale to modify the estimated height values at the object boundaries. Finally, a post-processing approach is proposed to merge the predicted height image patches and generate a seamless continuous height map. The quantitative evaluation of the proposed approaches on the ISPRS datasets indicates relative and root mean square errors of approximately 0.9 m and 3.2 m, respectively.},
author = {Amirkolaee, Hamed Amini and Arefi, Hossein},
doi = {10.1016/j.isprsjprs.2019.01.013},
file = {:Users/philipeborba/Downloads/amirkolaee2019.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Convolutional neural network,Decoder,Digital aerial image,Encoder,Height image},
number = {July 2018},
pages = {50--66},
publisher = {Elsevier},
title = {{Height estimation from single aerial images using a deep convolutional encoder-decoder network}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.01.013},
volume = {149},
year = {2019}
}
@article{Chai2020,
abstract = {This paper addresses the challenge of learning spatial context for the semantic segmentation of high-resolution aerial images using Deep Convolutional Neural Networks (DCNNs). The proposed solution involves deriving a signed distance map for each semantic class from a ground truth label map and training a DCNN to predict this distance map instead of a score map for each class. Since the distance between a target pixel and its nearest object boundary measures how far the pixel penetrates an object, the distance maps encode spatial context, particularly spatial smoothness. Positive pixel values in the distance maps correspond to the correct class and negative values correspond to the incorrect class. A final label map is derived from the predicted distance maps by selecting the class with the maximum distance. Since neighboring pixels in the distance maps have similar values, the segmentation results are smoother than current approaches. The results are shown to be even better than performing post-processing using fully connected Conditional Random Fields (CRFs), a common approach to smoothing the segmentations produced DCNNs. Experimental results on the semantic labeling challenge dataset show the proposed approach outperforms most state-of-the-art methods. Our main contribution, though, is the novel idea of replacing the pixel-wise class score maps of DCNNs with distance maps. This is therefore orthogonal and complementary to other techniques employed by the state-of-the-art methods and could therefore be used to improve upon them.},
author = {Chai, Dengfeng and Newsam, Shawn and Huang, Jingfeng},
doi = {10.1016/j.isprsjprs.2020.01.023},
file = {:Users/philipeborba/Downloads/10.1016@j.isprsjprs.2020.01.023.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {DCNNs,Deep learning,Distance maps,Distance transform,Semantic segmentation},
number = {January},
pages = {309--322},
publisher = {Elsevier},
title = {{Aerial image semantic segmentation using DCNN predicted distance maps}},
url = {https://doi.org/10.1016/j.isprsjprs.2020.01.023},
volume = {161},
year = {2020}
}
@article{Huang2019,
abstract = {Automated extraction of buildings from remotely sensed data is important for a wide range of applications but challenging due to difficulties in extracting semantic features from complex scenes like urban areas. The recently developed fully convolutional neural networks (FCNs) have shown to perform well on urban object extraction because of the outstanding feature learning and end-to-end pixel labeling abilities. The commonly used feature fusion or skip-connection refine modules of FCNs often overlook the problem of feature selection and could reduce the learning efficiency of the networks. In this paper, we develop an end-to-end trainable gated residual refinement network (GRRNet) that fuses high-resolution aerial images and LiDAR point clouds for building extraction. The modified residual learning network is applied as the encoder part of GRRNet to learn multi-level features from the fusion data and a gated feature labeling (GFL) unit is introduced to reduce unnecessary feature transmission and refine classification results. The proposed model - GRRNet is tested in a publicly available dataset with urban and suburban scenes. Comparison results illustrated that GRRNet has competitive building extraction performance in comparison with other approaches. The source code of the developed GRRNet is made publicly available for studies.},
author = {Huang, Jianfeng and Zhang, Xinchang and Xin, Qinchuan and Sun, Ying and Zhang, Pengcheng},
doi = {10.1016/j.isprsjprs.2019.02.019},
file = {:Users/philipeborba/Downloads/huang2019.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building extraction,Convolutional neural networks,Deep learning,Image classification,Semantic segmentation},
number = {January},
pages = {91--105},
publisher = {Elsevier},
title = {{Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.02.019},
volume = {151},
year = {2019}
}
@article{Ding2018,
abstract = {Detection of objects from satellite optical remote sensing images is very important for many commercial and governmental applications. With the development of deep convolutional neural networks (deep CNNs), the field of object detection has seen tremendous advances. Currently, objects in satellite remote sensing images can be detected using deep CNNs. In general, optical remote sensing images contain many dense and small objects, and the use of the original Faster Regional CNN framework does not yield a suitably high precision. Therefore, after careful analysis we adopt dense convoluted networks, a multi-scale representation and various combinations of improvement schemes to enhance the structure of the base VGG16-Net for improving the precision. We propose an approach to reduce the test-time (detection time) and memory requirements. To validate the effectiveness of our approach, we perform experiments using satellite remote sensing image datasets of aircraft and automobiles. The results show that the improved network structure can detect objects in satellite optical remote sensing images more accurately and efficiently.},
author = {Ding, Peng and Zhang, Ye and Deng, Wei Jian and Jia, Ping and Kuijper, Arjan},
doi = {10.1016/j.isprsjprs.2018.05.005},
file = {:Users/philipeborba/Downloads/ding2018.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Deep convolution neural network,Deep learning (DL),Object detection,Remote sensing images},
number = {March},
pages = {208--218},
publisher = {Elsevier},
title = {{A light and faster regional convolutional neural network for object detection in optical remote sensing images}},
url = {https://doi.org/10.1016/j.isprsjprs.2018.05.005},
volume = {141},
year = {2018}
}
@article{Kemker2018a,
abstract = {Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work.},
archivePrefix = {arXiv},
arxivId = {1703.06452},
author = {Kemker, Ronald and Salvaggio, Carl and Kanan, Christopher},
doi = {10.1016/j.isprsjprs.2018.04.014},
eprint = {1703.06452},
file = {:Users/philipeborba/Downloads/f2782d6c8c3066d4e7d7e3afb995d79fa3dd.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Convolutional neural network,Deep learning,Multispectral,Semantic segmentation,Synthetic imagery,Unmanned aerial system},
pages = {60--77},
title = {{Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning}},
volume = {145},
year = {2018}
}
@article{Mateo-Garcia2020,
abstract = {Accurate cloud detection algorithms are mandatory to analyze the large streams of data coming from the different optical Earth observation satellites. Deep learning (DL) based cloud detection schemes provide very accurate cloud detection models. However, training these models for a given sensor requires large datasets of manually labeled samples, which are very costly or even impossible to create when the satellite has not been launched yet. In this work, we present an approach that exploits manually labeled datasets from one satellite to train deep learning models for cloud detection that can be applied (or transferred) to other satellites. We take into account the physical properties of the acquired signals and propose a simple transfer learning approach using Landsat-8 and Proba-V sensors, whose images have different but similar spatial and spectral characteristics. Three types of experiments are conducted to demonstrate that transfer learning can work in both directions: (a) from Landsat-8 to Proba-V, where we show that models trained only with Landsat-8 data produce cloud masks 5 points more accurate than the current operational Proba-V cloud masking method, (b) from Proba-V to Landsat-8, where models that use only Proba-V data for training have an accuracy similar to the operational FMask in the publicly available Biome dataset (87.79–89.77{\%} vs 88.48{\%}), and (c) jointly from Proba-V and Landsat-8 to Proba-V, where we demonstrate that using jointly both data sources the accuracy increases 1–10 points when few Proba-V labeled images are available. These results highlight that, taking advantage of existing publicly available cloud masking labeled datasets, we can create accurate deep learning based cloud detection models for new satellites, but without the burden of collecting and labeling a large dataset of images.},
author = {Mateo-Garc{\'{i}}a, Gonzalo and Laparra, Valero and L{\'{o}}pez-Puigdollers, Dan and G{\'{o}}mez-Chova, Luis},
doi = {10.1016/j.isprsjprs.2019.11.024},
file = {:Users/philipeborba/Downloads/10.1016@j.isprsjprs.2019.11.024.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Cloud masking,Convolutional neural networks,Deep learning,Domain adaptation,Multispectral sensors,Transfer learning},
number = {July 2019},
pages = {1--17},
publisher = {Elsevier},
title = {{Transferring deep learning models for cloud detection between Landsat-8 and Proba-V}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.11.024},
volume = {160},
year = {2020}
}
@article{Zhao2019,
abstract = {Urban scenes refer to city blocks which are basic units of megacities, they play an important role in citizens' welfare and city management. Remote sensing imagery with largescale coverage and accurate target descriptions, has been regarded as an ideal solution for monitoring the urban environment. However, due to the heterogeneity of remote sensing images, it is difficult to access their geographical content at the object level, let alone understanding urban scenes at the block level. Recently, deep learning-based strategies have been applied to interpret urban scenes with remarkable accuracies. However, the deep neural networks require a substantial number of training samples which are hard to satisfy, especially for high-resolution images. Meanwhile, the crowed-sourced Open Street Map (OSM) data provides rich annotation information about the urban targets but may encounter the problem of insufficient sampling (limited by the places where people can go). As a result, the combination of OSM and remote sensing images for efficient urban scene recognition is urgently needed. In this paper, we present a novel strategy to transfer existing OSM data to high-resolution images for semantic element determination and urban scene understanding. To be specific, the object-based convolutional neural network (OCNN) can be utilized for geographical object detection by feeding it rich semantic elements derived from OSM data. Then, geographical objects are further delineated into their functional labels by integrating points of interest (POIs), which contain rich semantic terms, such as commercial or educational labels. Lastly, the categories of urban scenes are easily acquired from the semantic objects inside. Experimental results indicate that the proposed method has an ability to classify complex urban scenes. The classification accuracies of the Beijing dataset are as high as 91{\%} at the object-level and 88{\%} at the scene level. Additionally, we are probably the first to investigate the object level semantic mapping by incorporating high-resolution images and OSM data of urban areas. Consequently, the method presented is effective in delineating urban scenes that could further boost urban environment monitoring and planning with high-resolution images.},
author = {Zhao, Wenzhi and Bo, Yanchen and Chen, Jiage and Tiede, Dirk and Thomas, Blaschke and Emery, William J.},
doi = {10.1016/j.isprsjprs.2019.03.019},
file = {:Users/philipeborba/Downloads/zhao2019.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Data fusion,Deep learning,High-resolution imagery,OpenStreetMap (OSM),Semantic classification,Urban scene recognition},
number = {March},
pages = {237--250},
publisher = {Elsevier},
title = {{Exploring semantic elements for urban scene recognition: Deep integration of high-resolution imagery and OpenStreetMap (OSM)}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.03.019},
volume = {151},
year = {2019}
}
@article{Jozdani2020,
abstract = {Geographic Object-Based Image Analysis (GEOBIA) is a popular classification approach for high spatial resolution (HSR) images in the remote sensing (RS) community. It has been extensively reported that the final classification accuracy of GEOBIA is highly dependent on the quality of image segmentation. Different supervised, unsupervised, and post-classification quality assessment approaches have been proposed to evaluate whether a segmentation result has acceptable quality. Although there have recently been some theoretical review papers on supervised approaches in the context of RS, there have not been any comprehensively experimental tests on comparing these approaches for nearly a decade. Considering this gap, we experimentally compared different recently proposed and popular segmentation evaluation metrics to help the users narrow down the metrics that can be reliably used to measure the quality of a segmentation in a supervised manner. We assessed 21 distinguishable under-segmentation (US), over-segmentation (OS), and combined (UO) metrics in a two-step experimental procedure. The experiments showed that the OS metrics (including Recall, OSeg, NSR, and OSE) and US metrics (Precision, USeg, PSE, USE, and PI) each suffered from some problems including, but not limited to, unsuitable weighting (e.g. NSR), not enough sensitivity to segmentation variations (e.g. OSE/USE), uncertainty/unreliability in some cases (e.g. USeg/PSE), etc. Although none of the UO metrics was able to find the most optimal segmentation found with visual inspection, the UO metrics including F{\_}measure, QR, and SEI could identify an optimal segmentation that was still of high quality and was very similar to the one visually found. It was also found that these three UO metrics performed very similar to each other, and that OI2 and AFI also led to similar results although AFI's unbounded range of values is a source of confusion about what is the most reliable way to choose optimal segmentations in this approach.},
author = {Jozdani, Shahab and Chen, Dongmei},
doi = {10.1016/j.isprsjprs.2020.01.002},
file = {:Users/philipeborba/Downloads/10.1016@j.isprsjprs.2020.01.002.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Geographic object-based image analysis (GOBIA),Image segmentation,OBIA,Segmentation quality,Supervised evaluation},
number = {January},
pages = {275--290},
publisher = {Elsevier},
title = {{On the versatility of popular and recently proposed supervised evaluation metrics for segmentation quality of remotely sensed images: An experimental case study of building extraction}},
url = {https://doi.org/10.1016/j.isprsjprs.2020.01.002},
volume = {160},
year = {2020}
}
@article{Marmanis2018,
abstract = {We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large receptive fields. However, this success comes at a cost, since the associated loss of effective spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class boundaries explicit in the model. First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the SEGNET encoder-decoder architecture. Second, we also include boundary detection in FCN-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves semantic segmentation with CNNs in an end-to-end training scheme. Our best model achieves {\textgreater}90{\%} overall accuracy on the ISPRS Vaihingen benchmark.},
archivePrefix = {arXiv},
arxivId = {1612.01337},
author = {Marmanis, D. and Schindler, K. and Wegner, J. D. and Galliani, S. and Datcu, M. and Stilla, U.},
doi = {10.1016/j.isprsjprs.2017.11.009},
eprint = {1612.01337},
file = {:Users/philipeborba/Downloads/marmanis2018.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
pages = {158--172},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Classification with an edge: Improving semantic image segmentation with boundary detection}},
url = {https://doi.org/10.1016/j.isprsjprs.2017.11.009},
volume = {135},
year = {2018}
}
@article{Yan2019,
abstract = {Machine learning methods, specifically, convolutional neural networks (CNNs), have emerged as an integral part of scientific research in many disciplines. However, these powerful methods often fail to perform pattern analysis and knowledge mining with spatial vector data because in most cases, such data are not underlying grid-like or array structures but can only be modeled as graph structures. The present study introduces a novel graph convolution by converting it from the vertex domain into a point-wise product in the Fourier domain using the graph Fourier transform and convolution theorem. In addition, the graph convolutional neural network (GCNN) architecture is proposed to analyze graph-structured spatial vector data. The focus of this study is the classical task of building pattern classification, which remains limited by the use of design rules and manually extracted features for specific patterns. The spatial vector data representing grouped buildings are modeled as graphs, and indices for the characteristics of individual buildings are investigated to collect the input variables. The pattern features of these graphs are directly extracted by training labeled data. Experiments confirmed that the GCNN produces satisfactory results in terms of identifying regular and irregular patterns, and thus achieves a significant improvement over existing methods. In summary, the GCNN has considerable potential for the analysis of graph-structured spatial vector data as well as scope for further improvement.},
author = {Yan, Xiongfeng and Ai, Tinghua and Yang, Min and Yin, Hongmei},
doi = {10.1016/j.isprsjprs.2019.02.010},
file = {:Users/philipeborba/Downloads/1-s2.0-S0924271619300437-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building pattern classification,Deep learning,Graph Fourier transform,Graph convolutional neural network,Machine learning,Spatial vector data},
number = {March},
pages = {259--273},
publisher = {Elsevier},
title = {{A graph convolutional neural network for classification of building patterns using spatial vector data}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.02.010},
volume = {150},
year = {2019}
}
@article{Marcos2018,
abstract = {In remote sensing images, the absolute orientation of objects is arbitrary. Depending on an object's orientation and on a sensor's flight path, objects of the same semantic class can be observed in different orientations in the same image. Equivariance to rotation, in this context understood as responding with a rotated semantic label map when subject to a rotation of the input image, is therefore a very desirable feature, in particular for high capacity models, such as Convolutional Neural Networks (CNNs). If rotation equivariance is encoded in the network, the model is confronted with a simpler task and does not need to learn specific (and redundant) weights to address rotated versions of the same object class. In this work we propose a CNN architecture called Rotation Equivariant Vector Field Network (RotEqNet) to encode rotation equivariance in the network itself. By using rotating convolutions as building blocks and passing only the values corresponding to the maximally activating orientation throughout the network in the form of orientation encoding vector fields, RotEqNet treats rotated versions of the same object with the same filter bank and therefore achieves state-of-the-art performances even when using very small architectures trained from scratch. We test RotEqNet in two challenging sub-decimeter resolution semantic labeling problems, and show that we can perform better than a standard CNN while requiring one order of magnitude less parameters.},
author = {Marcos, Diego and Volpi, Michele and Kellenberger, Benjamin and Tuia, Devis},
doi = {10.1016/j.isprsjprs.2018.01.021},
file = {:Users/philipeborba/Downloads/marcos2018.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Deep learning,Rotation invariance,Semantic labeling,Sub-decimeter resolution},
pages = {96--107},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models}},
url = {https://doi.org/10.1016/j.isprsjprs.2018.01.021},
volume = {145},
year = {2018}
}
@article{Zhao2017,
abstract = {Contextual information, revealing relationships and dependencies between image objects, is one of the most important information for the successful interpretation of very-high-resolution (VHR) remote sensing imagery. Over the last decade, geographic object-based image analysis (GEOBIA) technique has been widely used to first divide images into homogeneous parts, and then to assign semantic labels according to the properties of image segments. However, due to the complexity and heterogeneity of VHR images, segments without semantic labels (i.e., semantic-free segments) generated with low-level features often fail to represent geographic entities (such as building roofs usually be partitioned into chimney/antenna/shadow parts). As a result, it is hard to capture contextual information across geographic entities when using semantic-free segments. In contrast to low-level features, “deep” features can be used to build robust segments with accurate labels (i.e., semantic segments) in order to represent geographic entities at higher levels. Based on these semantic segments, semantic graphs can be constructed to capture contextual information in VHR images. In this paper, semantic segments were first explored with convolutional neural networks (CNN) and a conditional random field (CRF) model was then applied to model the contextual information between semantic segments. Experimental results on two challenging VHR datasets (i.e., the Vaihingen and Beijing scenes) indicate that the proposed method is an improvement over existing image classification techniques in classification performance (overall accuracy ranges from 82{\%} to 96{\%}).},
author = {Zhao, Wenzhi and Du, Shihong and Wang, Qiao and Emery, William J.},
doi = {10.1016/j.isprsjprs.2017.08.011},
file = {:Users/philipeborba/Downloads/zhao2017.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {CRF model,Contextual information,Deep learning,Semantic segmentation,VHR images},
pages = {48--60},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Contextually guided very-high-resolution imagery classification with semantic segments}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2017.08.011},
volume = {132},
year = {2017}
}
@article{Liu2018c,
abstract = {Semantic labeling for very high resolution (VHR) images in urban areas, is of significant importance in a wide range of remote sensing applications. However, many confusing manmade objects and intricate fine-structured objects make it very difficult to obtain both coherent and accurate labeling results. For this challenging task, we propose a novel deep model with convolutional neural networks (CNNs), i.e., an end-to-end self-cascaded network (ScasNet). Specifically, for confusing manmade objects, ScasNet improves the labeling coherence with sequential global-to-local contexts aggregation. Technically, multi-scale contexts are captured on the output of a CNN encoder, and then they are successively aggregated in a self-cascaded manner. Meanwhile, for fine-structured objects, ScasNet boosts the labeling accuracy with a coarse-to-fine refinement strategy. It progressively refines the target objects using the low-level features learned by CNN's shallow layers. In addition, to correct the latent fitting residual caused by multi-feature fusion inside ScasNet, a dedicated residual correction scheme is proposed. It greatly improves the effectiveness of ScasNet. Extensive experimental results on three public datasets, including two challenging benchmarks, show that ScasNet achieves the state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1807.11236},
author = {Liu, Yongcheng and Fan, Bin and Wang, Lingfeng and Bai, Jun and Xiang, Shiming and Pan, Chunhong},
doi = {10.1016/j.isprsjprs.2017.12.007},
eprint = {1807.11236},
file = {:Users/philipeborba/Downloads/liu2017.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Convolutional neural networks (CNNs),End-to-end,Multi-scale contexts,Semantic labeling},
pages = {78--95},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Semantic labeling in very high resolution images via a self-cascaded convolutional neural network}},
url = {https://doi.org/10.1016/j.isprsjprs.2017.12.007},
volume = {145},
year = {2018}
}
@article{Tsagkatakis2019,
abstract = {Deep Learning, and Deep Neural Networks in particular, have established themselves as the new norm in signal and data processing, achieving state-of-the-art performance in image, audio, and natural language understanding. In remote sensing, a large body of research has been devoted to the application of deep learning for typical supervised learning tasks such as classification. Less yet equally important effort has also been allocated to addressing the challenges associated with the enhancement of low-quality observations from remote sensing platforms. Addressing such channels is of paramount importance, both in itself, since high-altitude imaging, environmental conditions, and imaging systems trade-offs lead to low-quality observation, as well as to facilitate subsequent analysis, such as classification and detection. In this paper, we provide a comprehensive review of deep-learning methods for the enhancement of remote sensing observations, focusing on critical tasks including single and multi-band super-resolution, denoising, restoration, pan-sharpening, and fusion, among others. In addition to the detailed analysis and comparison of recently presented approaches, different research avenues which could be explored in the future are also discussed.},
author = {Tsagkatakis, Grigorios and Aidini, Anastasia and Fotiadou, Konstantina and Giannopoulos, Michalis and Pentari, Anastasia and Tsakalides, Panagiotis},
doi = {10.3390/s19183929},
file = {:Users/philipeborba/Downloads/sensors-19-03929.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Convolutional neural networks,Deep learning,Denoising,Earth observations,Fusion,Generative adversarial networks,Pan-sharpening,Satellite imaging,Super-resolution},
number = {18},
pages = {1--39},
title = {{Survey of deep-learning approaches for remote sensing observation enhancement}},
volume = {19},
year = {2019}
}
@article{Ma2019,
abstract = {Deep learning (DL)algorithms have seen a massive rise in popularity for remote-sensing image analysis over the past few years. In this study, the major DL concepts pertinent to remote-sensing are introduced, and more than 200 publications in this field, most of which were published during the last two years, are reviewed and analyzed. Initially, a meta-analysis was conducted to analyze the status of remote sensing DL studies in terms of the study targets, DL model(s)used, image spatial resolution(s), type of study area, and level of classification accuracy achieved. Subsequently, a detailed review is conducted to describe/discuss how DL has been applied for remote sensing image analysis tasks including image fusion, image registration, scene classification, object detection, land use and land cover (LULC)classification, segmentation, and object-based image analysis (OBIA). This review covers nearly every application and technology in the field of remote sensing, ranging from preprocessing to mapping. Finally, a conclusion regarding the current state-of-the art methods, a critical conclusion on open challenges, and directions for future research are presented.},
author = {Ma, Lei and Liu, Yu and Zhang, Xueliang and Ye, Yuanxin and Yin, Gaofei and Johnson, Brian Alan},
doi = {10.1016/j.isprsjprs.2019.04.015},
file = {:Users/philipeborba/Downloads/1-s2.0-S0924271619301108-main-2.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Deep learning (DL),LULC classification,Object detection,Remote sensing,Scene classification},
number = {March},
pages = {166--177},
publisher = {Elsevier},
title = {{Deep learning in remote sensing applications: A meta-analysis and review}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.04.015},
volume = {152},
year = {2019}
}
@article{Li2019,
abstract = {Cloud detection is an important preprocessing step for the precise application of optical satellite imagery. In this paper, we propose a deep learning based cloud detection method named multi-scale convolutional feature fusion (MSCFF) for remote sensing images of different sensors. In the network architecture of MSCFF, the symmetric encoder-decoder module, which provides both local and global context by densifying feature maps with trainable convolutional filter banks, is utilized to extract multi-scale and high-level spatial features. The feature maps of multiple scales are then up-sampled and concatenated, and a novel multi-scale feature fusion module is designed to fuse the features of different scales for the output. The two output feature maps of the network are cloud and cloud shadow maps, which are in turn fed to binary classifiers outside the model to obtain the final cloud and cloud shadow mask. The MSCFF method was validated on hundreds of globally distributed optical satellite images, with spatial resolutions ranging from 0.5 to 50 m, including Landsat-5/7/8, Gaofen-1/2/4, Sentinel-2, Ziyuan-3, CBERS-04, Huanjing-1, and collected high-resolution images exported from Google Earth. The experimental results show that MSCFF achieves a higher accuracy than the traditional rule-based cloud detection methods and the state-of-the-art deep learning models, especially in bright surface covered areas. The effectiveness of MSCFF means that it has great promise for the practical application of cloud detection for multiple types of medium and high-resolution remote sensing images. Our established global high-resolution cloud detection validation dataset has been made available online (http://sendimage.whu.edu.cn/en/mscff/).},
archivePrefix = {arXiv},
arxivId = {1810.05801},
author = {Li, Zhiwei and Shen, Huanfeng and Cheng, Qing and Liu, Yuhao and You, Shucheng and He, Zongyi},
doi = {10.1016/j.isprsjprs.2019.02.017},
eprint = {1810.05801},
file = {:Users/philipeborba/Downloads/li2019.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Cloud detection,Cloud shadow,Convolutional feature fusion,Convolutional neural network,MSCFF,Multi-scale},
number = {January},
pages = {197--212},
publisher = {Elsevier},
title = {{Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.02.017},
volume = {150},
year = {2019}
}
@article{Li2019c,
abstract = {Semantic segmentation is a fundamental means of extracting information from remotely sensed images at the pixel level. Deep learning has enabled considerable improvements in efficiency and accuracy of semantic segmentation of general images. Typical models range from benchmarks such as fully convolutional networks, U-Net, Micro-Net, and dilated residual networks to the more recently developed DeepLab 3+. However, many of these models were originally developed for segmentation of general or medical images and videos, and are not directly relevant to remotely sensed images. The studies of deep learning for semantic segmentation of remotely sensed images are limited. This paper presents a novel flexible autoencoder-based architecture of deep learning that makes extensive use of residual learning and multiscaling for robust semantic segmentation of remotely sensed land-use images. In this architecture, a deep residual autoencoder is generalized to a fully convolutional network in which residual connections are implemented within and between all encoding and decoding layers. Compared with the concatenated shortcuts in U-Net, these residual connections reduce the number of trainable parameters and improve the learning efficiency by enabling extensive backpropagation of errors. In addition, resizing or atrous spatial pyramid pooling (ASPP) can be leveraged to capture multiscale information from the input images to enhance the robustness to scale variations. The residual learning and multiscaling strategies improve the trained model's generalizability, as demonstrated in the semantic segmentation of land-use types in two real-world datasets of remotely sensed images. Compared with U-Net, the proposed method improves the Jaccard index (JI) or the mean intersection over union (MIoU) by 4-11{\%} in the training phase and by 3-9{\%} in the validation and testing phases. With its flexible deep learning architecture, the proposed approach can be easily applied for and transferred to semantic segmentation of land-use variables and other surface variables of remotely sensed images.},
author = {Li, Lianfa},
doi = {10.3390/rs11182142},
file = {:Users/philipeborba/Downloads/remotesensing-11-02142-v2.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Atrous spatial pyramid pooling,Autoencoder,Multiscale,Remotely sensed land-use images,Residual learning,Semantic segmentation},
number = {18},
title = {{Deep residual autoencoder with multiscaling for semantic segmentation of land-use images}},
volume = {11},
year = {2019}
}
@article{Chen2019,
abstract = {As an important branch of deep learning, convolutional neural network has largely improved the performance of building detection. For further accelerating the development of building detection toward automatic mapping, a benchmark dataset bears significance in fair comparisons. However, several problems still remain in the current public datasets that address this task. First, although building detection is generally considered equivalent to extracting roof outlines, most datasets directly provide building footprints as ground truths for testing and evaluation; the challenges of these benchmarks are more complicated than roof segmentation, as relief displacement leads to varying degrees of misalignment between roof outlines and footprints. On the other hand, an image dataset should feature a large quantity and high spatial resolution to effectively train a high-performance deep learning model for accurate mapping of buildings. Unfortunately, the remote sensing community still lacks proper benchmark datasets that can simultaneously satisfy these requirements. In this paper, we present a new large-scale benchmark dataset termed Aerial Imagery for Roof Segmentation (AIRS). This dataset provides a wide coverage of aerial imagery with 7.5 cm resolution and contains over 220,000 buildings. The task posed for AIRS is defined as roof segmentation. We implement several state-of-the-art deep learning methods of semantic segmentation for performance evaluation and analysis of the proposed dataset. The results can serve as the baseline for future work.},
author = {Chen, Qi and Wang, Lei and Wu, Yifan and Wu, Guangming and Guo, Zhiling and Waslander, Steven L.},
doi = {10.1016/j.isprsjprs.2018.11.011},
file = {:Users/philipeborba/Downloads/1807.09532.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Automatic mapping,Building detection,Deep learning,Large-scale dataset,Roof segmentation},
month = {jan},
pages = {42--55},
publisher = {Elsevier B.V.},
title = {{Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings}},
volume = {147},
year = {2019}
}
@article{Cheng2017a,
abstract = {{\textcopyright} 1963-2012 IEEE. Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various data sets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning data sets and methods for scene classification is still lacking. In addition, almost all existing data sets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale data set, termed 'NWPU-RESISC45,' which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This data set contains 31 500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 1) is large-scale on the scene classes and the total image number; 2) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion; and 3) has high within-class diversity and between-class similarity. The creation of this data set will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed data set, and the results are reported as a useful baseline for future research.},
author = {Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
doi = {10.1109/JPROC.2017.2675998},
file = {:Users/philipeborba/Downloads/1703.00121.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Benchmark data set,deep learning,handcrafted features,remote sensing image,scene classification,unsupervised feature learning},
number = {10},
pages = {1865--1883},
title = {{Remote Sensing Image Scene Classification: Benchmark and State of the Art}},
volume = {105},
year = {2017}
}
@article{Yuan2019,
abstract = {In this paper, we address the problem of semantic segmentation and focus on the context aggregation strategy for robust segmentation. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we construct object regions based on a feature map supervised by the ground-truth segmentation, and then compute the object region representations. Second, we compute the representation similarity between each pixel and each object region, and augment the representation of each pixel with an object contextual representation, which is a weighted aggregation of all the object region representations according to their similarities with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on six challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL VOC 2012, PASCAL-Context and COCO-Stuff. Notably, we achieved the $\backslash$nth{\{}2{\}} place on the Cityscapes leader-board with a single model.},
archivePrefix = {arXiv},
arxivId = {1909.11065},
author = {Yuan, Yuhui and Chen, Xilin and Wang, Jingdong},
eprint = {1909.11065},
file = {:Users/philipeborba/Downloads/1909.11065v1.pdf:pdf},
title = {{Object-Contextual Representations for Semantic Segmentation}},
url = {http://arxiv.org/abs/1909.11065},
year = {2019}
}
@article{Zhu2017a,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.03959v1},
author = {Zhu, Xiao Xiang and Tuia, Devis and Mou, Lichao and Xia, Gui-song and Zhang, Liangpei and Fraundorfer, Friedrich},
eprint = {arXiv:1710.03959v1},
file = {:Users/philipeborba/Downloads/Deep{\_}Learning{\_}in{\_}Remote{\_}Sensing{\_}A{\_}Review.pdf:pdf},
number = {October},
title = {{Deep Learning in Remote Sensing : A Review}},
year = {2017}
}
@article{Zhang2018,
abstract = {The recent advances in deep neural networks have convincingly demonstrated high capability in learning vision models on large datasets. Nevertheless, collecting expert labeled datasets especially with pixel-level annotations is an extremely expensive process. An appealing alternative is to render synthetic data (e.g., computer games) and generate ground truth automatically. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. In this paper, we facilitate this issue from the perspectives of both visual appearance-level and representation-level domain adaptation. The former adapts source-domain images to appear as if drawn from the "style" in the target domain and the latter attempts to learn domain-invariant representations. Specifically, we present Fully Convolutional Adaptation Networks (FCAN), a novel deep architecture for semantic segmentation which combines Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN). AAN learns a transformation from one domain to the other in the pixel space and RAN is optimized in an adversarial learning manner to maximally fool the domain discriminator with the learnt source and target representations. Extensive experiments are conducted on the transfer from GTA5 (game videos) to Cityscapes (urban street scenes) on semantic segmentation and our proposal achieves superior results when comparing to state-of-the-art unsupervised adaptation techniques. More remarkably, we obtain a new record: mIoU of 47.5{\%} on BDDS (drive-cam videos) in an unsupervised setting.},
author = {Zhang, Yiheng and Qiu, Zhaofan and Yao, Ting and Liu, Dong and Mei, Tao},
doi = {10.1109/CVPR.2018.00712},
file = {:Users/philipeborba/Downloads/long{\_}shelhamer{\_}fcn-2.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {6810--6818},
title = {{Fully Convolutional Adaptation Networks for Semantic Segmentation}},
year = {2018}
}
@article{Chen2018,
abstract = {{\textcopyright} 2018 Informa UK Limited, trading as Taylor  {\&}  Francis Group Over the last two decades (since ca. 2000), Geographic Object-Based Image Analysis (GEOBIA) has emerged as a new paradigm to analyzing high-spatial resolution remote-sensing imagery. During this time, research interests have demonstrated a shift from the development of GEOBIA theoretical foundations to advanced geo-object-based models and their implementation in a wide variety of real-world applications. We suggest that such a rapid GEOBIA evolution warrants the need for a systematic review that defines the recent developments in this field. Therefore, the main objective of this paper is to elucidate the emerging trends in GEOBIA and discuss potential opportunities for future development. The emerging trends were found in multiple subfields of GEOBIA, including data sources, image segmentation, object-based feature extraction, and geo-object-based modeling frameworks. It is our view that understanding the state-of-the-art in GEOBIA will further facilitate and support the study of geographic entities and phenomena at multiple scales with effective incorporation of semantics, informing high-quality project design, and improving geo-object-based model performance and results.},
author = {Chen, Gang and Weng, Qihao and Hay, Geoffrey J. and He, Yinan},
doi = {10.1080/15481603.2018.1426092},
file = {:Users/philipeborba/Downloads/Chen{\_}2018{\_}GEOBIA-review.pdf:pdf},
issn = {15481603},
journal = {GIScience and Remote Sensing},
keywords = {data sources,emerging trends,geographic object-based image analysis (GEOBIA),image segmentation,object-based feature extraction},
number = {2},
pages = {159--182},
publisher = {Taylor {\&} Francis},
title = {{Geographic object-based image analysis (GEOBIA): emerging trends and future opportunities}},
url = {https://doi.org/10.1080/15481603.2018.1426092},
volume = {55},
year = {2018}
}
@article{Mboga2019,
abstract = {Land cover Classified maps obtained from deep learning methods such as Convolutional neural networks (CNNs) and fully convolutional networks (FCNs) usually have high classification accuracy but with the detailed structures of objects lost or smoothed. In this work, we develop a methodology based on fully convolutional networks (FCN) that is trained in an end-to-end fashion using aerial RGB images only as input. Skip connections are introduced into the FCN architecture to recover high spatial details from the lower convolutional layers. The experiments are conducted on the city of Goma in the Democratic Republic of Congo. We compare the results to a state-of-the art approach based on a semi-automatic Geographic object image-based analysis (GEOBIA) processing chain. State-of-the art classification accuracies are obtained by both methods whereby FCN and the best baseline method have an overall accuracy of 91.3{\%} and 89.5{\%} respectively. The maps have good visual quality and the use of an FCN skip architecture minimizes the rounded edges that is characteristic of FCN maps. Additional experiments are done to refine FCN classified maps using segments obtained from GEOBIA generated at different scale and minimum segment size. High OA of up to 91.5{\%} is achieved accompanied with an improved edge delineation in the FCN maps, and future work will involve explicitly incorporating boundary information from the GEOBIA segmentation into the FCN pipeline in an end-to-end fashion. Finally, we observe that FCN has a lower computational cost than the standard patch-based CNN approach especially at inference.},
annote = {Deep learning melhor que obia para segmenta{\c{c}}{\~{a}}o},
author = {Mboga, Nicholus and Georganos, Stefanos and Grippa, Tais and Lennert, Moritz and Vanhuysse, Sabine and Wolff, El{\'{e}}onore},
doi = {10.3390/rs11050597},
file = {:Users/philipeborba/Downloads/remotesensing-11-00597.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Convolutional neural networks,Fully convolutional networks,Geographical object-based image analysis,Landcover classification,Remote sensing,Very high resolution},
number = {5},
title = {{Fully convolutional networks and geographic object-based image analysis for the classification of VHR imagery}},
volume = {11},
year = {2019}
}
@article{Wang2019,
abstract = {High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions $\backslash$emph{\{}in series{\}} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams $\backslash$emph{\{}in parallel{\}}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at{\~{}}{\{}$\backslash$url{\{}https://github.com/HRNet{\}}{\}}.},
archivePrefix = {arXiv},
arxivId = {1908.07919},
author = {Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and Liu, Wenyu and Xiao, Bin},
eprint = {1908.07919},
file = {:Users/philipeborba/Downloads/1908.07919v1.pdf:pdf},
pages = {1--17},
title = {{Deep High-Resolution Representation Learning for Visual Recognition}},
url = {http://arxiv.org/abs/1908.07919},
year = {2019}
}
@article{Zhang2016,
abstract = {{\textcopyright} 2013 IEEE. Deep-learning (DL) algorithms, which learn the representative and discriminative features in a hierarchical manner from the data, have recently become a hotspot in the machine-learning area and have been introduced into the geoscience and remote sensing (RS) community for RS big data analysis. Considering the low-level features (e.g., spectral and texture) as the bottom level, the output feature representation from the top level of the network can be directly fed into a subsequent classifier for pixel-based classification. As a matter of fact, by carefully addressing the practical demands in RS applications and designing the input"output levels of the whole network, we have found that DL is actually everywhere in RS data analysis: from the traditional topics of image preprocessing, pixel-based classification, and target recognition, to the recent challenging tasks of high-level semantic feature extraction and RS scene understanding.},
author = {Zhang, Liangpei and Zhang, Lefei and Du, Bo},
doi = {10.1109/MGRS.2016.2540798},
file = {:Users/philipeborba/Downloads/zhang{\_}2016.pdf:pdf},
issn = {21686831},
journal = {IEEE Geoscience and Remote Sensing Magazine},
number = {2},
pages = {22--40},
title = {{Deep learning for remote sensing data: A technical tutorial on the state of the art}},
volume = {4},
year = {2016}
}
@inproceedings{Ball2018,
abstract = {Deep learning usually requires big data, with respect to both volume and variety. However, most remote sensing applications only have limited training data, of which a small subset is labeled. Herein, we review three state-of-the-art approaches in deep learning to combat this challenge. The first topic is transfer learning, in which some aspects of one domain, e.g., features, are transferred to another domain. The next is unsupervised learning, e.g., autoencoders, which operate on unlabeled data. The last is generative adversarial networks, which can generate realistic looking data that can fool the likes of both a deep learning network and human. The aim of this article is to raise awareness of this dilemma, to direct the reader to existing work and to highlight current gaps that need solving.},
author = {Ball, John E. and Anderson, Derek T. and Wei, Pan},
booktitle = {International Geoscience and Remote Sensing Symposium (IGARSS)},
doi = {10.1109/IGARSS.2018.8518681},
file = {:Users/philipeborba/Downloads/08518681.pdf:pdf},
isbn = {9781538671504},
keywords = {Deep learning,Generative adversarial networks,Limited training data,Remote sensing,Transfer learning},
title = {{State-of-the-art and gaps for deep learning on limited training data in remote sensing}},
year = {2018}
}
@article{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {arXiv:1512.03385v1},
file = {:Users/philipeborba/Downloads/1512.03385.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
volume = {2016-Decem},
year = {2016}
}
@article{Diakogiannis2019,
abstract = {Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. Here we present a novel deep learning architecture, $\backslash$resuneta, that combines ideas from various state of the art modules used in computer vision for semantic segmentation tasks. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has better convergence properties and behaves well even under the presence of highly imbalanced classes. The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9{\%} over all classes for our best model.},
archivePrefix = {arXiv},
arxivId = {1904.00592},
author = {Diakogiannis, Foivos I. and Waldner, Fran{\c{c}}ois and Caccetta, Peter and Wu, Chen},
eprint = {1904.00592},
file = {:Users/philipeborba/Downloads/1904.00592v2-3.pdf:pdf},
keywords = {architecture,convolutional neural network,data augmentation,loss function,very high spatial resolution},
title = {{ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data}},
url = {http://arxiv.org/abs/1904.00592},
year = {2019}
}
@article{Kemker2018,
abstract = {Deep convolutional neural networks (DCNNs) have been used to achieve state-of-the-art performance on many computer vision tasks (e.g., object recognition, object detection, semantic segmentation) thanks to a large repository of annotated image data. Large labeled datasets for other sensor modalities, e.g., multispectral imagery (MSI), are not available due to the large cost and manpower required. In this paper, we adapt state-of-the-art DCNN frameworks in computer vision for semantic segmentation for MSI imagery. To overcome label scarcity for MSI data, we substitute real MSI for generated synthetic MSI in order to initialize a DCNN framework. We evaluate our network initialization scheme on the new RIT-18 dataset that we present in this paper. This dataset contains very-high resolution MSI collected by an unmanned aircraft system. The models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work.},
annote = {uso de imagens sint{\'{e}}ticas para aumento de acur{\'{a}}cia no treinamento},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.06452v3},
author = {Kemker, Ronald and Salvaggio, Carl and Kanan, Christopher},
doi = {10.1016/j.isprsjprs.2018.04.014},
eprint = {arXiv:1703.06452v3},
file = {:Users/philipeborba/Downloads/1703.06452v3.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Convolutional neural network,Deep learning,Multispectral,Semantic segmentation,Synthetic imagery,Unmanned aerial system},
pages = {60--77},
title = {{Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning}},
volume = {145},
year = {2018}
}
@article{Lin2017,
abstract = {Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.06612v3},
author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
doi = {10.1109/CVPR.2017.549},
eprint = {arXiv:1611.06612v3},
file = {:Users/philipeborba/Pessoal/Mestrado/03-Dissertacao/01-Ref{\_}Bibliograficas/1611.06612-2.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5168--5177},
title = {{RefineNet: Multi-path refinement networks for high-resolution semantic segmentation}},
volume = {2017-Janua},
year = {2017}
}

@inproceedings{Hobel2015,
abstract = {Recent years have witnessed a growing production of Volunteer Geographic Information (VGI). This led to the general availability of semantically rich datasets, allowing for novel ways to understand, analyze or generalize urban areas. This paper presents an approach that exploits this semantic richness to extract urban settings, i.e., conceptually-uniform geographic areas with respect to certain activities. We argue that urban settings are a more accurate way of generalizing cities, since it more closely models human sense-making of urban spaces. To this end, we formalized and implemented a semantic region growing algorithm—a modification of a standard image segmentation procedure. To evaluate our approach, shopping areas of two European capital cities (Vienna and London) were extracted from an OpenStreetMap dataset. Finally, we explored the use of our approach to search for urban settings (e.g., shopping areas) in one city, that are similar to a setting in another.},
author = {Hobel, Heidelinde and Abdalla, Amin and Fogliaroni, Paolo and Frank, Andrew U.},
booktitle = {Lecture Notes in Geoinformation and Cartography},
doi = {10.1007/978-3-319-16787-9_2},
isbn = {9783319167862},
issn = {18632351},
keywords = {Image segmentation,Place affordances,Semantic region growing,Urban settings},
title = {{A semantic region growing algorithm: Extraction of urban settings}},
year = {2015}
}

@article{Khanal2019,
abstract = {During the last few decades, a large number of people have migrated to Kathmandu city from all parts of Nepal, resulting in rapid expansion of the city. The unplanned and accelerated growth is causing many environmental and population management issues. To manage urban growth efficiently, the city authorities need a means to be able to monitor urban expansion regularly. In this study, we introduced a novel approach to automatically detect urban expansion by leveraging state-of-the-art cloud computing technologies using the Google Earth Engine (GEE) platform. We proposed a new index named Normalized Difference and Distance Built-up Index (NDDBI) for identifying built-up areas by combining the LandSat-derived vegetation index with distances from the nearest roads and buildings analysed from OpenStreetMap (OSM). We also focused on logical consistencies of land-cover change to remove unreasonable transitions supported by the repeat photography. Our analysis of the historical urban growth patterns between 2000 and 2018 shows that the settlement areas were increased from 63.68 sq km in 2000 to 148.53 sq km in 2018. The overall accuracy of mapping the newly-built areas of urban expansion was 94.33{\%}. We have demonstrated that the methodology and data generated in the study can be replicated to easily map built-up areas and support quicker and more efficient land management and land-use planning in rapidly growing cities worldwide.},
author = {Khanal, Nishanta and Uddin, Kabir and Matin, Mir A. and Tenneson, Karis},
doi = {10.3390/rs11192296},
file = {:Users/philipeborba/Downloads/remotesensing-11-02296-v2.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Built-up mapping,GEE,Kathmandu,Landsat,Nepal,OSM,Remote sensing},
mendeley-groups = {Pre-Projeto},
number = {19},
title = {{Automatic detection of spatiotemporal urban expansion patterns by fusing OSM and Landsat data in Kathmandu}},
volume = {11},
year = {2019}
}

@article{doi:10.1080/01431160701469016,
author = { Wei   Su  and  Jing   Li  and  Yunhao   Chen  and  Zhigang   Liu  and  Jinshui   Zhang  and  Tsuey Miin   Low  and  Inbaraj   Suppiah  and  Siti Atikah Mohamed   Hashim },
title = {Textural and local spatial statistics for the object‐oriented classification of urban areas using high resolution imagery},
journal = {International Journal of Remote Sensing},
volume = {29},
number = {11},
pages = {3105-3117},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1080/01431160701469016},

URL = { 
        https://doi.org/10.1080/01431160701469016
    
},
eprint = { 
        https://doi.org/10.1080/01431160701469016
    
}
,
    abstract = { Textural and local spatial statistical information is important in the classification of urban areas using very high resolution imagery. This paper describes the utility of textural and local spatial statistics for the improvement of object‐oriented classification for QuickBird imagery. All textural/spatial bands were used as additional bands in the supervised object‐oriented classification. The texture analysis is based on two levels: segmented image objects and moving windows across the whole image. In the texture analysis over image objects, the angular second moment textural feature at a 45° angle showed an improved classification performance with regard to buildings, depicting the patterns of buildings better than any other directions. The texture analysis based on moving windows across the whole image was conducted with various window sizes (from 3×3 to 13×13), and four grey‐level co‐occurrence matrix (GLCM) textural features (homogeneity, contrast, angular second moment, and entropy) were calculated. The contrast feature with the 7×7 window size improved classification up to 6\%. One type of local spatial statistics, Moran's I feature with the vertical neighbourhood rule, improved the classification accuracy even further, up to 7\%. Comparison of results between spectral and spectral+textural/spatial information indicated that textural and spatial information can be used to improve the object‐oriented classification of urban areas using very high resolution imagery. }
}

@article{cheng2001color,
  title={Color image segmentation: advances and prospects},
  author={Cheng, Heng-Da and Jiang, X\_ H\_ and Sun, Ying and Wang, Jingli},
  journal={Pattern recognition},
  volume={34},
  number={12},
  pages={2259--2281},
  year={2001},
  publisher={Elsevier}
}

@article{wiseman2009quantification,
  title={Quantification of shelterbelt characteristics using high-resolution imagery},
  author={Wiseman, G and Kort, J and Walker, D},
  journal={Agriculture, ecosystems \& environment},
  volume={131},
  number={1-2},
  pages={111--117},
  year={2009},
  publisher={Elsevier}
}

@manual{tensorflow2015-whitepaper,
	title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={https://www.tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
	Mart\'{\i}n~Abadi and
	Ashish~Agarwal and
	Paul~Barham and
	Eugene~Brevdo and
	Zhifeng~Chen and
	Craig~Citro and
	Greg~S.~Corrado and
	Andy~Davis and
	Jeffrey~Dean and
	Matthieu~Devin and
	Sanjay~Ghemawat and
	Ian~Goodfellow and
	Andrew~Harp and
	Geoffrey~Irving and
	Michael~Isard and
	Yangqing Jia and
	Rafal~Jozefowicz and
	Lukasz~Kaiser and
	Manjunath~Kudlur and
	Josh~Levenberg and
	Dandelion~Man\'{e} and
	Rajat~Monga and
	Sherry~Moore and
	Derek~Murray and
	Chris~Olah and
	Mike~Schuster and
	Jonathon~Shlens and
	Benoit~Steiner and
	Ilya~Sutskever and
	Kunal~Talwar and
	Paul~Tucker and
	Vincent~Vanhoucke and
	Vijay~Vasudevan and
	Fernanda~Vi\'{e}gas and
	Oriol~Vinyals and
	Pete~Warden and
	Martin~Wattenberg and
	Martin~Wicke and
	Yuan~Yu and
	Xiaoqiang~Zheng},
	year={2015},
}
@book{python3-10.5555/1593511, 
	author = {Van Rossum, Guido and Drake, Fred L.}, 
	title = {Python 3 Reference Manual}, 
	year = {2009}, 
	isbn = {1441412697}, 
	publisher = {CreateSpace}, 
	address = {Scotts Valley, CA} 
}
@Manual{QGIS_software,
	title = {QGIS Geographic Information System},
	author = {{QGIS Development Team}},
	organization = {Open Source Geospatial Foundation},
	year = {2009},
	url = {http://qgis.org},
}
@Manual{PostgreSQL_software,
	title = {PostgreSQL},
	author = {{The PostgreSQL Global Development Group}},
	year = {1996},
	url = {www.postgresql.org},
}
@Manual{PostGIS_software,
	title = {PostGIS},
	author = {{Refractions Research Inc}},
	year = {2005},
	url = {http://postgis.net},
}
@article{merkel2014docker,
	title={Docker: lightweight linux containers for consistent development and deployment},
	author={Merkel, Dirk},
	journal={Linux journal},
	volume={2014},
	number={239},
	pages={2},
	year={2014}
}
@book{cuda-10.5555/2430671,
	author = {Cook, Shane},
	title = {CUDA Programming: A Developer’s Guide to Parallel Computing with GPUs},
	year = {2012}, isbn = {9780124159334}, publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
	edition = {1st}
}
@conference{Kluyver:2016aa,
	Author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing},
	Booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
	Editor = {F. Loizides and B. Schmidt},
	Organization = {IOS Press},
	Pages = {87 - 90},
	Title = {Jupyter Notebooks -- a publishing format for reproducible computational workflows},
	Year = {2016}
}
@book{goodfellow2016deep,
	title={Deep learning},
	author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year={2016},
	publisher={MIT press}
}
@InProceedings{10.1007/978-3-319-46349-0_5,
	author="Soekhoe, Deepak
	and van der Putten, Peter
	and Plaat, Aske",
	editor="Bostr{\"o}m, Henrik
	and Knobbe, Arno
	and Soares, Carlos
	and Papapetrou, Panagiotis",
	title="On the Impact of Data Set Size in Transfer Learning Using Deep Neural Networks",
	booktitle="Advances in Intelligent Data Analysis XV",
	year="2016",
	publisher="Springer International Publishing",
	address="Cham",
	pages="50--60",
	abstract="In this paper we study the effect of target set size on transfer learning in deep learning convolutional neural networks. This is an important problem as labelling is a costly task, or for new or specific classes the number of labelled instances available may simply be too small. We present results for a series of experiments where we either train on a target of classes from scratch, retrain all layers, or subsequently lock more layers in the network, for the Tiny-ImageNet and MiniPlaces2 data sets. Our findings indicate that for smaller target data sets freezing the weights for the initial layers of the network gives better results on the target set classes. We present a simple and easy to implement training heuristic based on these findings.",
	isbn="978-3-319-46349-0"
}
@article{Avbelj2015,
	abstract = {The standardization of evaluation techniques for building extraction is an unresolved issue in the fields of remote sensing, photogrammetry, and computer vision. In this letter, we propose a metric with a working title 'PoLiS metric' to compare two polygons. The PoLiS metric is a positive-definite and symmetric function that satisfies a triangle inequality. It accounts for shape and accuracy differences between the polygons, is straightforward to apply, and requires no thresholds. We show through an example that the PoLiS metric between two polygons changes approximately linearly with respect to small translation, rotation, and scale changes. Furthermore, we compare building polygons extracted from a digital surface model to the reference building polygons by computing PoLiS, Hausdorff, and Chamfer distances. The results show that quantification by the PoLiS distance of the dissimilarity between polygons is consistent with visual perception. Furthermore, Hausdorff and Chamfer distances overrate the dissimilarity when one polygon has more vertices than the other. We propose an approach toward standardizing building extraction evaluation, which may also have broader applications in the field of shape similarity. {\textcopyright} 2014 IEEE.},
	author = {Avbelj, Janja and Muller, Rupert and Bamler, Richard},
	doi = {10.1109/LGRS.2014.2330695},
	file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Avbelj, Muller, Bamler - 2015 - A metric for polygon comparison and building extraction evaluation.pdf:pdf},
	issn = {1545598X},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	keywords = {Building extraction,metric,polygon comparison,quality assessment,shape similarity},
	mendeley-groups = {Dissertacao{\_}Mestrado},
	number = {1},
	pages = {170--174},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {{A metric for polygon comparison and building extraction evaluation}},
	volume = {12},
	year = {2015}
}
@article{Partovi2017,
	abstract = {Efficient and fully automatic building outline extraction and simplification methods are highly demanded for three-dimensional model reconstruction tasks. In spite of the efforts put into developing such methods, the results of the recently proposed methods are still not satisfactory, especially for satellite images, due to object complexities and the presence of noise. Dealing with this problem, in this article, we propose a new approach that detects rough building boundaries (building mask) from Digital Surface Model data and then refines the resulting mask by classifying the geometrical features of the high spatial resolution panchromatic satellite image. The refined mask represents finer details of the building outlines, which are close to the original building edges. These outlines are then simplified through a parameterization phase wherein a tracing algorithm detects the building boundary points from the refined masks and a set of line segments is fitted to them. After that, for each building, the existing main orientations are determined based on the length and arc lengths of the building's line segments. Our method is able to determine the multiple main orientations of complex buildings. Through a regularization process, the line segments are then aligned and adjusted according to the building's main orientations. Finally, the adjusted line segments are intersected and connected to each other in order to form a polygon representing the building's outlines. Experimental results demonstrate that the computed building outlines are highly accurate and simple, even for large and complex buildings with inner yards.},
	author = {Partovi, Tahmineh and Bahmanyar, Reza and Kraus, Thomas and Reinartz, Peter},
	doi = {10.1109/JSTARS.2016.2611861},
	file = {:Users/philipeborba/Downloads/final{\_}manuscript{\%}20{\%}282{\%}29-2.pdf:pdf},
	issn = {21511535},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	keywords = {Building detection,Digital Surface Model (DSM),high-resolution satellite image,outline extraction,outline simplification},
	mendeley-groups = {Dissertacao{\_}Mestrado{\_}Generalizacao{\_}Predios},
	number = {3},
	pages = {933--947},
	title = {{Building Outline Extraction Using a Heuristic Approach Based on Generalization of Line Segments}},
	volume = {10},
	year = {2017}
}
@article{Reina2020,
	abstract = {Convolutional neural network (CNN) models obtain state of the art performance on image classification, localization, and segmentation tasks. Limitations in computer hardware, most notably memory size in deep learning accelerator cards, prevent relatively large images, such as those from medical and satellite imaging, from being processed as a whole in their original resolution. A fully convolutional topology, such as U-Net, is typically trained on down-sampled images and inferred on images of their original size and resolution, by simply dividing the larger image into smaller (typically overlapping) tiles, making predictions on these tiles, and stitching them back together as the prediction for the whole image. In this study, we show that this tiling technique combined with translationally-invariant nature of CNNs causes small, but relevant differences during inference that can be detrimental in the performance of the model. Here we quantify these variations in both medical (i.e., BraTS) and non-medical (i.e., satellite) images and show that training a 2D U-Net model on the whole image substantially improves the overall model performance. Finally, we compare 2D and 3D semantic segmentation models to show that providing CNN models with a wider context of the image in all three dimensions leads to more accurate and consistent predictions. Our results suggest that tiling the input to CNN models-while perhaps necessary to overcome the memory limitations in computer hardware-may lead to undesirable and unpredictable errors in the model's output that can only be adequately mitigated by increasing the input of the model to the largest possible tile size.},
	author = {Reina, G A and Panchumarthy, R and Thakur, S P and Bastidas, A and Bakas, S},
	doi = {10.3389/fnins.2020.00065},
	file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Reina et al. - 2020 - Systematic Evaluation of Image Tiling Adverse Effects on Deep Learning Semantic Segmentation(3).pdf:pdf},
	journal = {Front. Neurosci},
	keywords = {BraTS,CNN,brain tumor,deep learning,glioma,satellite imaging,segmentation,tiling},
	mendeley-groups = {Artigo{\_}BCG},
	pages = {65},
	title = {{Systematic Evaluation of Image Tiling Adverse Effects on Deep Learning Semantic Segmentation}},
	url = {www.frontiersin.org},
	volume = {14},
	year = {2020}
}
@article{Guo2020-2,
	abstract = {{\textless}p{\textgreater}Semantic segmentation of high-resolution remote sensing images plays an important role in applications for building extraction. However, the current algorithms have some semantic information extraction limitations, and these can lead to poor segmentation results. To extract buildings with high accuracy, we propose a multiloss neural network based on attention. The designed network, based on U-Net, can improve the sensitivity of the model by the attention block and suppress the background influence of irrelevant feature areas. To improve the ability of the model, a multiloss approach is proposed during training the network. The experimental results show that the proposed model offers great improvement over other state-of-the-art methods. For the public Inria Aerial Image Labeling dataset, the F1 score reached 76.96{\%} and showed good performance on the Aerial Imagery for Roof Segmentation dataset.{\textless}/p{\textgreater}},
	author = {Guo, Mingqiang and Liu, Heng and Xu, Yongyang and Huang, Ying},
	doi = {10.3390/rs12091400},
	file = {:Users/philipeborba/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - 2020 - Building Extraction Based on U-Net with an Attention Block and Multiple Losses(2).pdf:pdf},
	issn = {2072-4292},
	journal = {Remote Sensing},
	keywords = {attention block,building extraction,multiple losses,remote sensing images,semantic segmentation},
	mendeley-groups = {Artigo{\_}BCG},
	month = {apr},
	number = {9},
	pages = {1400},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {{Building Extraction Based on U-Net with an Attention Block and Multiple Losses}},
	url = {https://www.mdpi.com/2072-4292/12/9/1400},
	volume = {12},
	year = {2020}
}
